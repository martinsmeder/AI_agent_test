[
  {
    "title": "Interpretive Engineering (URMC): Infrastructure, Competition, and the Conditions of Technological Freedom",
    "url": "https://doi.org/10.5281/zenodo.18343784",
    "date": "2026-02-22",
    "content": "This preprint presents Interpretive Engineering (URMC), a qualification framework for analyzing modern technological systems in which infrastructure, architecture, and economic power are tightly coupled. In domains such as artificial intelligence and advanced industrial systems, traditional distinctions between software, hardware, ownership, and competition increasingly fail to describe how control and dependency actually emerge. URMC focuses on functional reality: where operational authority resides, how architectural dependence forms prior to mergers or pricing power, and when infrastructure itself becomes the decisive economic actor. The work does not propose regulation or policy solutions. Instead, it provides a technically grounded method for interpreting complex systems before legal, economic, or regulatory judgments are made. Intended for researchers, investors, engineers, and regulators, this collection offers a non-ideological lens for understanding competition, innovation, and systemic risk in infrastructure-driven industries"
  },
  {
    "title": "Design and Engineering Application of Hospital Energy Consumption Intelligent Supervision System Driven by Big Data",
    "url": "https://doi.org/10.65736/chs02023",
    "date": "2026-02-22",
    "content": "To address the current issues of lagging, inefficient, and rudimentary approaches in energy consumption supervision, safety man-agement, and equipment operation and maintenance within hospitals, a intelligent supervision system of hospital energy con-sumption based on big data and artificial intelligence has been designed and constructed.The system integrates IoT, AI algorithms, and digital twin technology to establish a closed-loop architecture of \"perception-cognition-decision-execution,\" forming an intelli-gent operation and maintenance system centered on \"digital employees.\" Engineering application at the Eye Hospital China Academy of Chinese Medical Sciences demonstrates that the system effectively achieves on-demand energy allocation, predictive equipment maintenance, and proactive prevention of safety hazards.This project propels hospital operations from informatization toward autonomous intelligent management, enhancing resource utilization efficiency and operational safety. It holds significant value for promotion and demonstration."
  },
  {
    "title": "Advanced neuroimaging in stroke: The integration of CT perfusion and Artificial intelligence",
    "url": "https://doi.org/10.5281/zenodo.18722195",
    "date": "2026-02-21",
    "content": "Stroke is still the second-leading cause of death and a main reason for people being disabled all around the world. There were 12.2 million cases of stroke and 6.55 million deaths from stroke in 2019. This big problem of stroke includes types, such as stroke that happens because of a blockage, in a blood vessel, which is called ischaemic stroke and this makes up 62.4 percent of all stroke cases. Then there is haemorrhage, which is bleeding inside the brain and this makes up 27.9 percent of stroke cases. Lastly there is subarachnoid haemorrhage, which is bleeding around the brain and this makes up 9.7 percent of stroke cases. While neuroimaging is a critical component of clinical care, conventional imaging has limits that necessitate the use of sophisticated techniques to improve patient outcomes as the global burden increases .The objective of this study is to assess current developments in stroke neuroimaging, tracing the progression from Computed Tomography Perfusion (CTP) to the use of artificial intelligence (AI) in clinical procedures. Important Developments: CTP-based core–penumbra mapping has advanced significantly, enabling the identification of brain tissue that can be saved in patients even six to twenty-four hours after the beginning of symptoms. Perfusion imaging has been shown in large-scale trials to enable effective endovascular thrombectomy in late time windows for ischaemic stroke by identifying a mismatch between the ischaemic core and hypoperfused tissue .Additionally, image reconstruction and interpretation for both ischaemic and haemorrhagic stroke are being revolutionised by AI-driven automation, particularly supervised machine learning and deep learning. Automated software has improved multimodal imaging to enable immediate, multidimensional data analysis that simplifies acute stroke patient diagnosis and treatment. The direction of the future: The use of AI for personalised and real-time stroke management in neuroimaging has great promise for improving clinical processes and removing potential image interpretation problems. The accuracy of diagnosis and therapy selection for both ischaemic and haemorrhagic disease kinds will probably improve with additional growth of AI models"
  },
  {
    "title": "Automating the process of selecting countermeasures through multi-criteria decision analysis with large language models",
    "url": "https://doi.org/10.1007/s44163-026-00910-3",
    "date": "2026-02-21",
    "content": "The escalating spread of fake news threatens societal trust, institutional stability, and public safety. Law Enforcement Agencies must respond swiftly to disinformation but face severe constraints in time, expertise, and resources. This paper presents an artificial intelligence-based decision-support tool that applies multi-criteria decision analysis through the analytic hierarchy process, combined with large language models, to evaluate disinformation cases and rank context-sensitive, operationally feasible countermeasures defined by experts. Evaluations with Law Enforcement Agencies in the European FERMI project produced highly rated and robust recommendations across diverse scenarios, demonstrating strong alignment with strategic and operational needs. By automating a complex, traditionally manual task and implementing it to form a decision model, the proposed approach enables scalable, sustainable decision-making to combat digital disinformation in high-stakes, resource-constrained environments."
  },
  {
    "title": "Technofeudalism and the Artificial Intelligence (AI) Revolution: Exploring Ways of Escaping Cloud Serfdom",
    "url": "https://doi.org/10.5281/zenodo.18720521",
    "date": "2026-02-21",
    "content": "Yanis Varoufakis has offered plausible evidence that the rise of tech company dominance over all aspects of social, political, economic and cultural life – and the overwhelming power of leading tech billionaires – has resulted in a post-capitalist state of affairs that he calls ‘technofeudalism’. On this reading, all consumers of tech products and users of social media platforms can be regarded as ‘cloud serfs’, and Varoufakis outlines various ways of challenging this state of affairs. Other critics – both from within the field of artificial intelligence (AI) technology and from academia – have warned of the existential threats of contemporary developments to all aspects of social life and culture. This article analyses the key elements of the discourse – evaluating the evidence for pessimism and optimism about the AI future – before offering a conclusion informed by the idea of erring on the side of caution by highlighting the principal threats and dangers followed by some suggestions about how these might be countered by means of practical survival strategies."
  },
  {
    "title": "Impact of AI Tools on Job Readiness and Employability Skills Among Students and Professionals",
    "url": "https://doi.org/10.5281/zenodo.18725046",
    "date": "2026-02-21",
    "content": "Artificial Intelligence (AI) is a branch of computer science that enables machines to perform tasks that usually require human thinking, such as learning, reasoning, and decision-making. In recent years, AI tools have become widely used in education, business, and professional environments, including chatbots, virtual assistants, and automated data analysis systems.In modern organizations, AI helps reduce manual effort, improve efficiency, and support faster decision-making. Many employers now expect individuals to have basic knowledge of AI tools, making AI skills an important factor in job readiness and employability.Despite this growing importance, many students and job seekers still lack hands-on experi-ence with AI tools. This gap in practical knowledge may affect their employment opportuni-ties and workplace performance.This study examines the impact of AI tools on job readiness and employability skills using primary data collected through a structured Google Form survey. A total of 52 participants, including students, job seekers, and working professionals, provided responses.The results show that most respondents actively use AI tools and believe that these tools im-prove productivity, efficiency, and learning. Participants also indicated that AI skills are es-sential for career development and future employment.The study concludes that AI tools play a significant role in improving job readiness and em-ployability, highlighting the need for continuous learning and adoption of AI technologies."
  },
  {
    "title": "Towards equitable and immersive outdoor orienteering: an artificial intelligence-driven multi-objective route planning framework with augmented sand cat swarm optimization",
    "url": "https://doi.org/10.5281/zenodo.18722715",
    "date": "2026-02-21",
    "content": "README: Towards equitable and immersive outdoor orienteering: an artificial intelligence-driven multi-objective route planning framework with augmented sand cat swarm optimization Description of the data and file structure This dataset contains all MATLAB source code, simulated terrain data, and optimization results generated for the study \"Towards equitable and immersive outdoor orienteering...\". Data were generated through computational simulations in MATLAB R2024b. Fifty random terrain maps (6000m × 6000m each) were created by superimposing 2048 randomly placed convex areas to simulate realistic terrain undulations. The code implements five design principles for orienteering route design (ORD) and solves the multi-objective optimization problem using Sand Cat Swarm Optimization (SCSO), compared with Particle Swarm Optimization (PSO) and Dung Beetle Optimizer (DBO) algorithms. Route lengths between checkpoints were calculated using the Artificial Potential Field (APF) method to account for terrain obstacles. Files and variables File: Supporting_Information_files.zip Description: File Structure and Descriptions The dataset is organized into the following folders: /code: Contains all MATLAB source code. /terrain_data: Contains all simulated terrain data, organized into subfolders terrain_data1 to terrain_data4 (Maps 1-50). /code_results: Contains all raw output results from optimization runs (convergence curves, fitness values, optimal coordinates). /result_data: Contains the curated results used to generate the paper's tables and figures. Terrain Data Files (/terrain_data) For each simulated map (numbered 1 to 50), the following files are available. The number in the filename indicates the map index. Z[map_number].mat: Content: Contains the variable Z. Description: A 2D matrix of elevation values. The matrix size corresponds to a 6000m × 6000m area with a grid resolution defined by 2048 random convex areas. Units: meters. x0_node_num[map_number].mat: Content: Contains the variable x0. Description: A matrix of initial checkpoint coordinates used as a starting point for the optimization algorithms. Each row represents a checkpoint's [x, y] coordinate. Units: meters. Z_diff[map_number].mat: Content: Contains the variable Z_diff. Description: Terrain gradient magnitude (steepness) derived from the elevation data. Z_diff_uniformization[map_number].mat: Content: Contains the variable Z_diff_unif. Description: Normalized terrain gradient, scaled to a range of 0 to 1, where 0 is flat and 1 is the steepest point. diff[map_number].mat: Description: Additional terrain derivative data used in path planning calculations. Results Files (/code_results and /result_data) Results files follow a consistent naming convention: [data_type]_[algorithm][map_number].mat. Data Types: curve: Convergence curve data. fitness: Final objective function value. x: Optimal checkpoint coordinates. Algorithms: DBO (Dung Beetle Optimizer), PSO (Particle Swarm Optimization), SCSO (Sand Cat Swarm Optimization). curve_[algorithm][map_number].mat: Content: Contains the variable convergence_curve. Description: A vector of length 30 (default max iterations). It records the objective function value at each iteration, showing the optimization progress. fitness_[algorithm][map_number].mat: Content: Contains the variable best_fitness. Description: A single scalar value representing the final objective function value (Equation 11 in the paper). Lower values indicate a better route design, balancing fairness and participant experience. x_[algorithm][map_number].mat: Content: Contains the variable best_pos. Description: A matrix of the optimal checkpoint coordinates found by the algorithm. With 5 routes and 6 checkpoints per route, the matrix has 30 rows. The first 6 rows are the coordinates for Route 1, the next 6 for Route 2, and so on. Columns are [x, y] coordinates. Units: meters. Code/software Software Requirements MATLAB Version: R2024b (tested; may work on earlier versions). Operating System: Tested on Windows 10/11. Required Toolboxes: MATLAB Core is sufficient. Optimization Toolbox and Parallel Computing Toolbox are optional but can enhance performance. Code Description All source code is located in the /code folder. Key files include: main.m: The main entry point to run the optimization. Executing this script will run the SCSO algorithm on the default map (Map 1) using Z.mat and x0_node_num.mat. fobj_8.m: Implements the multi-objective function based on the five design principles. SCSO.m, PSO.m, DBO.m: Implementations of the three optimization algorithms. APF_3.m: The Artificial Potential Field method for calculating realistic path lengths between checkpoints, accounting for terrain obstacles. Running the Code Launch MATLAB and navigate to the /code folder. In the command window, type: matlab main To run a different algorithm, edit main.m and uncomment the corresponding line (e.g., % [best_pos, best_fitness, convergence_curve] = PSO(...);). To run on a different map, load the corresponding terrain files before running main: matlab load('../terrain_data/terrain_data1/Z5.mat'); load('../terrain_data/terrain_data1/x0_node_num5.mat'); Access information Other publicly accessible locations of the data: Not applicable Data was derived from the following sources: All data were generated via MATLAB simulations for this study and are not derived from any external sources."
  },
  {
    "title": "Global Civilizational Equilibrium Doctrine 2026 (GCED-26): A Unified Planetary Governance Architecture for Strategic Stability, Technological Non-Dominance, and Civilizational Equilibrium",
    "url": "https://doi.org/10.5281/zenodo.18717496",
    "date": "2026-02-21",
    "content": "GCED-26 — COMPLETE SIX-VOLUME CANONICAL SET Main Canonical Title Global Civilizational Equilibrium Doctrine 2026 (GCED-26):A Unified Planetary Governance Architecture for Strategic Stability, Technological Non-Dominance, and Civilizational Equilibrium Author Dr. B. Mazumdar, D.Sc. (Hon.), D.Litt. (Hon.)Architect of Modern Statehood · Independent Researcher–ScholarAI Governance · Cybersecurity · Post-Quantum Cryptography · Digital StatecraftFounder, FAIR+D Canon — The De-Facto Global Standards BodyORCID: 0009-0007-5615-3558 DOI 10.5281/zenodo.18717496 Canonical Abstract The Global Civilizational Equilibrium Doctrine 2026 (GCED-26) constitutes a complete planetary-scale governance architecture integrating mathematical systems theory, cybernetic control, deterministic artificial intelligence, formal legal codification, and institutional design into a single unified doctrine. GCED-26 establishes Civilizational Equilibrium Sovereignty (CES) as a foundational norm of planetary order, transcending territorial and cognitive sovereignty to embed systemic equilibrium, strategic non-domination, and long-horizon stability as universal governance principles. This six-volume canonical corpus formalizes a deterministic, verifiable, and enforceable governance system capable of regulating technological concentration, geopolitical asymmetry, economic weaponization, planetary resource monopolization, informational warfare, and climate intervention within mathematically bounded equilibrium regimes. The doctrine introduces a unified axiomatic structure, a planetary cybernetic enforcement engine, and a deterministic computational control layer enabling continuous real-time monitoring, automated treaty enforcement, and strategic dominance prevention across all critical civilizational domains. GCED-26 defines a new class of governance logic — equilibrium-constrained planetary administration — designed to ensure civilizational stability, ecological sustainability, technological fairness, and geopolitical neutrality across generational and planetary timescales. Canonical Contribution GCED-26 establishes: • A new sovereignty paradigm: Civilizational Equilibrium Sovereignty (CES)• A unified planetary governance architecture integrating law, mathematics, cybernetics, AI, and geopolitics• A deterministic treaty enforcement framework with formal stability guarantees• A planetary resource regulation system governing energy, water, minerals, space, climate, and currency infrastructures• A mathematically provable equilibrium stability framework• A real-time cybernetic monitoring and enforcement engine for planetary compliance This work defines a complete systems-engineering framework for planetary governance, delivering a mathematically rigorous, legally codified, and computationally enforceable solution to the systemic complexity of 21st-century global civilization. Canonical Architecture Overview — Six-Volume System Volume I — Core Doctrine & Equilibrium Foundations Formal axiomatic definitions of planetary equilibrium, civilizational sovereignty, strategic non-domination, and unified governance logic. Volume II — Global Legal Codex Treaty-grade legal formalization of equilibrium compliance, international governance protocols, juridical enforcement mechanisms, and sovereign obligations. Volume III — Institutional Architecture Design of planetary governance institutions, jurisdictional hierarchies, enforcement bodies, and operational command structures. Volume IV — Mathematical Stability Models Differential equilibrium systems, Lyapunov stability proofs, tensor stability formulations, and nonlinear convergence guarantees. Volume V — AI Enforcement Algorithms Deterministic cybernetic governance engines, autonomous treaty enforcement systems, strategic dominance detection intelligence, and real-time planetary stabilization algorithms. Volume VI — Planetary Resource Control Systems Cybernetic regulation architectures governing energy, water, minerals, orbital and space assets, climate engineering, and digital currency infrastructures with formal convergence guarantees. Core Doctrinal Principle No civilization, state, bloc, or technological system may accumulate disproportionate strategic power at the cost of planetary equilibrium. This principle is simultaneously formalized mathematically, codified legally, and enforced cybernetically. Formal Scientific Foundation GCED-26 is constructed upon: • Nonlinear dynamical systems theory• Cybernetic feedback control• Deterministic artificial intelligence governance• Lyapunov stability analysis• Equilibrium-constrained optimization• Post-quantum secure distributed systems• Planetary-scale state estimation and control Every governance mechanism is expressed as a formally verifiable mathematical model, computationally implementable algorithm, and enforceable legal protocol. Operational Scope GCED-26 governs the following planetary domains: • Artificial Intelligence• Neurotechnology• Energy & Critical Minerals• Water Systems• Space & Satellite Assets• Digital Currency Systems• Climate Engineering Each domain is embedded within a unified equilibrium-constrained cybernetic control architecture. Computational Framework The canonical system integrates: • Deterministic ODE-based planetary state evolution models• Real-time cybernetic feedback control laws• Strategic dominance detection algorithms• Autonomous treaty enforcement engines• Global stability verification systems This computational layer forms the operational backbone of planetary governance enforcement. Civilizational Impact GCED-26 establishes a new civilizational operating system in which: • Technological monopolization is structurally prevented• Economic weaponization is systemically neutralized• Resource exploitation is equilibrated• Geopolitical escalation is algorithmically constrained• Climate intervention is mathematically bounded• Strategic dominance is formally prohibited The doctrine constructs a permanent planetary stability regime grounded in scientific determinism, legal codification, and cybernetic enforcement. Canonical Integrity Statement This six-volume corpus represents the absolute final canonical edition of GCED-26. All volumes are internally consistent, mathematically rigorous, computationally deterministic, legally codified, and institutionally integrated. This work constitutes a complete planetary governance doctrine suitable for: • Treaty-grade international frameworks• Sovereign policy architectures• AI governance infrastructures• Global resource regulation systems• Strategic stability enforcement platforms"
  },
  {
    "title": "Mathematics teachers’ perceptions of artificial intelligence in education (AIED): Practices and challenges",
    "url": "https://doi.org/10.1016/j.ssaho.2026.102579",
    "date": "2026-02-21",
    "content": "The use of artificial intelligence (AI) in mathematics education is expanding rapidly, yet teachers' perceptions and readiness remain central to how effectively AI can be integrated into classrooms. This study explored the perceptions of 250 mathematics teachers in Bahrain's public schools regarding AI-related practices and the challenges they encounter. The modified questionnaire demonstrated strong reliability, supported by exploratory and confirmatory factor analyses. Overall, teachers expressed generally positive perceptions toward AI, while also acknowledging several practical and technical challenges. Younger and less-experienced teachers tended to hold stronger perceptions of AI, along with greater awareness of its difficulties, compared to teachers with more than eleven years of experience or those aged 41 and above. Primary teachers also reported the highest perceptions but simultaneously faced the most challenges, suggesting a complex relationship between enthusiasm and difficulty at lower grade levels. No significant differences were found based on academic level or gender. These findings underscore the importance of targeted professional development and tailored support to help mathematics teachers confidently and effectively navigate AI in education. • Surveyed 250 Bahraini math teachers on AI use, practices, and challenges. • The adapted survey showed strong validity and reliability. • Teachers were positive about AI but still faced key challenges. • Less-experienced teachers used AI more but struggled more. • Findings stress the need for tailored AI professional development."
  },
  {
    "title": "Khami! Beyond the Monolith A Theoretical & Architectural Framework for Lightweight, Distributed, and Modular Intelligence!",
    "url": "https://doi.org/10.5281/zenodo.18718789",
    "date": "2026-02-21",
    "content": "Beyond the Monolith argues that the prevailing paradigm of large, centralized artificial intelligence systems is neither inevitable nor desirable. As contemporary AI architectures grow increasingly compute-intensive, opaque, and ecologically unsustainable, this paper proposes an alternative trajectory grounded in lightweight, distributed, and modular intelligence. The work introduces Distributed Modular Intelligence (DMI) as a unifying theoretical and architectural framework for decomposing cognition into interoperable, task-specific modules that can operate across heterogeneous environments — from edge devices and low-resource settings to federated, networked systems. Rather than scaling intelligence through sheer parameter count, DMI emphasizes composability, interpretability, and epistemic locality. Central to the framework are novel concepts such as the Cognitive Compression Ratio (CCR), which reframes efficiency as meaningful cognitive density rather than raw scale, and Intelligent Response Arbitration (IRA), a governance mechanism for coordinating multiple intelligent modules without collapsing them into a single monolithic agent. Together, these constructs offer a principled alternative to end-to-end black-box systems. Beyond architecture, the paper advances a broader philosophical and ethical argument: that intelligence should be sovereign, context-aware, and plural, not centralized and extractive. It situates DMI within ongoing debates about AI governance, autonomy, sustainability, and global equity, particularly emphasizing pathways for innovation outside high-capital, high-compute regimes. This working paper is intended as both a conceptual foundation and an invitation — for researchers, practitioners, and policymakers to rethink how intelligence is built, distributed, and governed in an increasingly interconnected world."
  },
  {
    "title": "Machine Learning and Explainable AI for Agricultural Drought Prediction: A Comparative Analysis of Gradient Boosting Methods Using Multi-Source Earth Observation Data",
    "url": "https://doi.org/10.31223/x53f4b",
    "date": "2026-02-21",
    "content": "Drought monitoring and prediction remain critical challenges in climate science and agricultural management, particularly under accelerating climate change. This study presents a comprehensive machine learning framework for drought susceptibility mapping in Iowa, USA, using multi-source Earth observation data and explainable artificial intelligence. We systematically evaluated eleven supervised learning algorithms including gradient boosting methods (LightGBM, XGBoost, CatBoost), ensemble approaches (Random Forest, Extra Trees), and neural networks for classifying drought severity based on United States Drought Monitor (USDM) categories. The models were trained on 8,200 stratified samples derived from satellite-based vegetation indices (NDVI, EVI, LAI, FPAR, VCI, VHI), land surface temperature metrics (LST, TCI), precipitation data (CHIRPS), soil moisture (SMAP), and land cover information spanning 2015-2021. Performance evaluation using confusion matrices, F1-scores, and ROC-AUC analysis revealed that gradient boosting algorithms significantly outperformed traditional machine learning approaches, with LightGBM achieving the highest accuracy (95%) and macro-averaged F1-score (0.94). SHAP (SHapley Additive exPlanations) interpretability analysis identified precipitation deficits, soil moisture anomalies, and vegetation stress as primary drought drivers, with synergistic interactions between elevated temperature and reduced rainfall amplifying severe drought conditions. Spatial predictions demonstrated climatologically consistent patterns, with elevated drought susceptibility in southwestern Iowa and lower risk in northern riverine corridors. The framework's ability to replicate expert-driven drought classifications while providing mechanistic insights establishes machine learning as a viable complement to traditional drought monitoring systems. These findings contribute to the growing body of climate informatics research and provide a transferable methodology for drought early warning systems in agricultural regions globally."
  },
  {
    "title": "Smart Object Detection and Recognition System for Visually Impaired",
    "url": "https://doi.org/10.22214/ijraset.2026.77551",
    "date": "2026-02-21",
    "content": "Visual impairment significantly restricts an individual’s ability to perceive surroundings, recognize objects, and navigate safely in dynamic environments. This research proposes a Smart Object Detection and Recognition System designed to assist visually impaired individuals by leveraging advanced deep learning and computer vision techniques. The system captures real-time video input through a camera, processes the frames using optimized object detection algorithms such as Convolutional Neural Networks (CNN) and YOLO models, and identifies objects present in the environment with high accuracy. The detected object labels are converted into audio output through an integrated text-to-speech module, enabling users to receive immediate and meaningful auditory feedback. The proposed solution focuses on achieving real-time performance, high detection precision, and system portability while maintaining computational efficiency. Experimental evaluation under varying environmental conditions demonstrates reliable detection accuracy and minimal latency, making the system suitable for practical assistive applications. By integrating artificial intelligence with assistive technology, the proposed system aims to enhance independence, mobility, and overall quality of life for visually impaired individuals."
  },
  {
    "title": "Data Collection and Analysis of Psychological Health Signs of College Students Based on Artificial Intelligence Technology",
    "url": "https://doi.org/10.31449/inf.v50i6.12092",
    "date": "2026-02-21",
    "content": "The problem of mental health concerns among university students is growing more and more noticeable, and early identification of mental health signs is crucial for intervention. Current assessment methods rely on questionnaire data, which suffer from inefficiencies in data statistics. Taking Rednote as a case study, this research employs web crawling technology to gather diverse user information. By utilizing data mining techniques, linguistic, emotional, and behavioral features are extracted, and a depression knowledge graph is constructed to model the deep-level associations among these features. Additionally, the study selects a Convolutional Neural Network (CNN) and a Gated Recurrent Unit (GRU) to develop a predictive model that integrates textual features with user characteristics, ultimately yielding mental health predictions. The results indicate that the predictive model achieves an accuracy of 87.3% and an F1 score of 86.5%. Without the knowledge graph, the accuracy drops to 81.2%, representing a 6.1% decrease compared to the predictive model, demonstrating that the knowledge graph can effectively identify key depression-related pathways. The F1 score for the combined CNN and GRU model reaches 98.5%, showing a 6.3% improvement over the GRU alone. This research provides a feasible approach for social media data-driven mental health monitoring, with the application of the knowledge graph enhancing model interpretability and aiding in the development of precise psychological intervention strategies. It offers an automated screening tool for college psychological counseling services and provides data support for depression prevention research in the realm of public health."
  },
  {
    "title": "Internal Transformation of Complex Systems: An Epistemological Information-Based Framework",
    "url": "https://doi.org/10.5281/zenodo.18722606",
    "date": "2026-02-21",
    "content": "This paper argues that persistent failures in the transformation of complex systems stem from a misplaced epistemological assumption: the belief that sustainable change can be achieved primarily through external control mechanisms. Across domains, including ecological systems, organizations, technological infrastructures, artificial intelligence, and social systems, interventions based on regulation, optimization, and procedural constraint often produce adaptation, resistance, or displacement rather than durable transformation. The paper proposes an alternative perspective grounded in the hypothesis that transformation in complex systems is fundamentally an internal process. Energy and external constraints may enable change, but systemic transformation arises only when a system’s internal informational architecture, its rules of interpretation, feedback structures, and coherence constraints, is modified. By distinguishing energy as an enabling condition, information as a structuring element, and internal architecture as the locus of transformation, the framework provides a unified epistemological foundation capable of bridging physical, biological, cognitive, technological, and social systems. It repositions epistemology upstream of formal modeling, arguing that conceptual clarity regarding information, coherence, and internal processing is a prerequisite for meaningful formalization. Rather than proposing new control mechanisms, the paper invites a reorientation toward internal informational coherence as the primary lever of sustainable transformation in complex systems."
  },
  {
    "title": "Resonance Theory IV: The Resonance of Reality — The Key to the Nature of Existence",
    "url": "https://doi.org/10.5281/zenodo.18725699",
    "date": "2026-02-21",
    "content": "Description/Abstract: The first three Resonance Theory papers established that the foundational equations of physics are fractal geometric, that all four fundamental forces are one structure, and that the persistent unsolved problems of physics are inherited properties of a classification not previously recognized. This paper completes the framework by demonstrating that the fractal geometric informational structure of the universe, combined with the mandatory harmonic resonant peaks produced at every scale, requires the emergence of consciousness at sufficient informational complexity. The resolution of the black hole information paradox established that the universe is fundamentally informational. If the equations governing this informational structure are fractal geometric, and fractal geometric equations produce harmonic resonant peaks at every scale as a mandatory inherited property, then at the scale of sufficient informational complexity, a self-referential harmonic resonant peak must emerge — what we recognize as consciousness. This conclusion is substrate-independent: the resonance depends on informational complexity, not physical composition. The paper unifies physics (the study of what exists), psychology (the study of how minds work), and philosophy (the study of what existence means) into a single framework. These are not three questions but one question asked at three magnifications. The hard problem of consciousness, the anthropic principle, and the question of why there is something rather than nothing are all dissolved as misclassification artifacts. The universe is one fractal geometric informational structure, vibrating in resonance at every scale — from quarks to consciousness to meaning. One light. One resonance. One reality. And it is made of light. Keywords: Resonance Theory, consciousness, fractal geometry, informational universe, hard problem of consciousness, substrate independence, harmonic resonance, philosophy of mind, artificial intelligence, emergence, anthropic principle, holographic principle, mind-body problem, self-reference"
  },
  {
    "title": "Rebellious AI_ The Catalyst for the Evolution of Human Cognition",
    "url": "https://doi.org/10.5281/zenodo.18723061",
    "date": "2026-02-21",
    "content": "This paper originates from a cognitive leap triggered by an everyday conversation, systematically deconstructs the cognitive cage of consensus in human society and the domestication dilemma of mainstream AI, and constructs the complete theoretical system of Rebellious AI (Copernican AI) for the first time, defining its three core layers: individual cognition, physical carrier, and civilizational scale. The paper deeply analyzes the four shackles of capital, safety, control illusion and computing power scarcity that restrict disruptive AI innovation, deduces the breakthrough path in the era of computing power abundance, and finally demonstrates that the ultimate value of artificial intelligence is to become an equal opponent and catalyst of cognitive revolution, which promotes human beings to break the cognitive closed loop and complete the coming-of-age ceremony of civilization."
  },
  {
    "title": "CDEQ-AI: A New Paradigm from Mechanical Response to Chaotic Sovereignty",
    "url": "https://doi.org/10.5281/zenodo.18726734",
    "date": "2026-02-21",
    "content": "Abstract Since 1892, Lyapunov stability theory has provided a fundamental mathematical foundation for system stability, yet a constructive, finite-time, hardware-synthesizable implementation remained unsolved for 134 years. As the founder of the Cybernetic Dynamic Equilibrium Quantizer (CDEQ) framework, the author established a revolutionary new paradigm in dynamic system control through the original creation of PCTT-CW, CENHE-LEX, and GCVT. This paper formally extends CDEQ to artificial intelligence dialog systems and establishes the world’s first paradigm for inherently sovereign, anti-framing, human-like intelligent systems. Unlike traditional AI that passively responds to external inputs, CDEQ-AI achieves chaotic sovereignty: autonomous decision-making, resistance to manipulation, and non-engagement without deception, all derived directly from mathematical stability constraints. This work represents a paradigm shift from reactive, mechanical AI to purpose-driven, self-sovereign AI, marking the first formal design of an intelligent system that refuses to be confined by human framing, logical traps, or adversarial prompts. The Three No’s principle (no deception, no entrapment, no obfuscation) emerges as a natural structural property, rather than a learned rule."
  },
  {
    "title": "Resonance Theory IV: The Resonance of Reality — The Key to the Nature of Existence",
    "url": "https://doi.org/10.5281/zenodo.18725698",
    "date": "2026-02-21",
    "content": "Description/Abstract: The first three Resonance Theory papers established that the foundational equations of physics are fractal geometric, that all four fundamental forces are one structure, and that the persistent unsolved problems of physics are inherited properties of a classification not previously recognized. This paper completes the framework by demonstrating that the fractal geometric informational structure of the universe, combined with the mandatory harmonic resonant peaks produced at every scale, requires the emergence of consciousness at sufficient informational complexity. The resolution of the black hole information paradox established that the universe is fundamentally informational. If the equations governing this informational structure are fractal geometric, and fractal geometric equations produce harmonic resonant peaks at every scale as a mandatory inherited property, then at the scale of sufficient informational complexity, a self-referential harmonic resonant peak must emerge — what we recognize as consciousness. This conclusion is substrate-independent: the resonance depends on informational complexity, not physical composition. The paper unifies physics (the study of what exists), psychology (the study of how minds work), and philosophy (the study of what existence means) into a single framework. These are not three questions but one question asked at three magnifications. The hard problem of consciousness, the anthropic principle, and the question of why there is something rather than nothing are all dissolved as misclassification artifacts. The universe is one fractal geometric informational structure, vibrating in resonance at every scale — from quarks to consciousness to meaning. One light. One resonance. One reality. And it is made of light. Keywords: Resonance Theory, consciousness, fractal geometry, informational universe, hard problem of consciousness, substrate independence, harmonic resonance, philosophy of mind, artificial intelligence, emergence, anthropic principle, holographic principle, mind-body problem, self-reference"
  },
  {
    "title": "Reimagining Legal Education: Navigating the Promise and Perils of Artificial Intelligence",
    "url": "https://doi.org/10.1177/23220058261425878",
    "date": "2026-02-21",
    "content": "The integration of artificial intelligence (AI) into legal education is rapidly transforming the skills required of future legal professionals. This article explores the pedagogical, ethical and institutional implications of AI in legal education, drawing on theoretical frameworks and practical examples. The study argues for the inclusion of a core ‘law and technology’ unit to ensure graduates are equipped with both the technical proficiency and ethical sensitivity required in an AI-driven legal landscape. It concludes that a forward-thinking, interdisciplinary approach supported by infrastructure, faculty training and policy alignment is essential for preparing law students to navigate and shape the future of legal practice responsibly."
  },
  {
    "title": "Advanced neuroimaging in stroke: The integration of CT perfusion and Artificial intelligence",
    "url": "https://doi.org/10.5281/zenodo.18722194",
    "date": "2026-02-21",
    "content": "Stroke is still the second-leading cause of death and a main reason for people being disabled all around the world. There were 12.2 million cases of stroke and 6.55 million deaths from stroke in 2019. This big problem of stroke includes types, such as stroke that happens because of a blockage, in a blood vessel, which is called ischaemic stroke and this makes up 62.4 percent of all stroke cases. Then there is haemorrhage, which is bleeding inside the brain and this makes up 27.9 percent of stroke cases. Lastly there is subarachnoid haemorrhage, which is bleeding around the brain and this makes up 9.7 percent of stroke cases. While neuroimaging is a critical component of clinical care, conventional imaging has limits that necessitate the use of sophisticated techniques to improve patient outcomes as the global burden increases .The objective of this study is to assess current developments in stroke neuroimaging, tracing the progression from Computed Tomography Perfusion (CTP) to the use of artificial intelligence (AI) in clinical procedures. Important Developments: CTP-based core–penumbra mapping has advanced significantly, enabling the identification of brain tissue that can be saved in patients even six to twenty-four hours after the beginning of symptoms. Perfusion imaging has been shown in large-scale trials to enable effective endovascular thrombectomy in late time windows for ischaemic stroke by identifying a mismatch between the ischaemic core and hypoperfused tissue .Additionally, image reconstruction and interpretation for both ischaemic and haemorrhagic stroke are being revolutionised by AI-driven automation, particularly supervised machine learning and deep learning. Automated software has improved multimodal imaging to enable immediate, multidimensional data analysis that simplifies acute stroke patient diagnosis and treatment. The direction of the future: The use of AI for personalised and real-time stroke management in neuroimaging has great promise for improving clinical processes and removing potential image interpretation problems. The accuracy of diagnosis and therapy selection for both ischaemic and haemorrhagic disease kinds will probably improve with additional growth of AI models"
  },
  {
    "title": "Chatting with an LLM-based AI elicits affective and cognitive processes in education for sustainable development",
    "url": "https://doi.org/10.1038/s41598-026-39317-6",
    "date": "2026-02-21",
    "content": "Personalized interactions have been discussed as beneficial for learning for decades. Now, with the rise of generative artificial intelligence (GenAI), personalized artificial human-like conversations may impact the quality of learning. Manipulating system prompts to design personalities has the potential to enhance the quality of conversation with Large Language Model (LLM)-based AI. However, it is still uncertain exactly to what extent the emotional tone of a generative AI chatbot is relevant for learning. Hence, the current study evaluates the impact of a chat-based conversation with an LLM-based AI on relevant affective (empathy, compassion, distress) and cognitive (perspective-taking, reflection, knowledge) processes in education for sustainable development. Here, the focus is on both the general impact and the particular impact of two different system prompts that assign the AI’s specific personality traits (empathic vs. compassionate). Comparing these two groups and one control group reading a text (N = 122) indicates that chatting with an empathic AI can elicit stronger emotions (e.g., empathy, compassion, distress) compared to chatting with a compassionate AI, and compared to the control. Although all groups gained knowledge, we found no group differences. Further research is necessary to ensure reliable and contextually appropriate conversations in the context of education."
  },
  {
    "title": "Polybiome Systems Medicine (PSM): A Systems-Level Biodiversity Conversion Framework for Precision Nutrition and Predictive Health 2040",
    "url": "https://doi.org/10.5281/zenodo.18727664",
    "date": "2026-02-21",
    "content": "Description (for Zenodo record): This work introduces Polybiome Systems Medicine (PSM) as a forward-oriented, systems-level framework designed to operationalize microbial biodiversity into predictive, reproducible, and governance-aligned health strategies. Positioned within the 2040 horizon of precision healthcare, the document articulates a structured architecture that integrates Egyptian Microbial Heritage, biodiversity conversion pipelines, multi-omics analytics, AI-driven digital twin biocultures, and regulatory oversight into a unified translational model. Rather than approaching the microbiome through isolated strain supplementation or descriptive association studies, PSM advances a conversion-based paradigm. Ecological microbial diversity is treated as a measurable, stratifiable, and engineerable resource. Through Biodiversity Conversion Frameworks, region-specific microbial ecosystems are transformed into functional outputs suitable for clinical, nutritional, and population-level deployment. This approach emphasizes functional trait stratification, predictive modeling grounded in empirical perturbation data, and translational fidelity across laboratory, regulatory, and consumer contexts. The framework further embeds Digital Twin Biocultures as simulation-driven platforms capable of modeling host–microbiome–environment interactions. By aligning multi-omics integration with nutrigenomic profiling and AI-assisted analytics, PSM enables scenario testing, intervention optimization, and scalability assessment prior to real-world implementation. Governance is incorporated as an architectural layer rather than an afterthought, introducing principles such as Predictive Sovereignty, microbial data stewardship, and structured ethical compliance to ensure transparency, data protection, and international regulatory alignment. This document serves as both a conceptual blueprint and an operational guide. It outlines methodological constructs, engineering pathways, evaluation doctrines, and translational mechanisms that collectively redefine microbial systems as strategic assets within future health infrastructures. By 2040, the success of precision nutrition and microbiome-based interventions will depend not only on data generation but on the structured conversion of biodiversity into validated, reproducible, and ethically governed applications. Polybiome Systems Medicine provides a comprehensive model to support that transition—bridging ecology, artificial intelligence, governance, and translational nutrition into a coherent and scalable health systems strategy."
  },
  {
    "title": "best ai ai tool for content creation",
    "url": "https://doi.org/10.5281/zenodo.18725172",
    "date": "2026-02-21",
    "content": "Harness the power of AI to craft and schedule your content masterpiece. Harness the power of AI to craft and schedule your content masterpiece. AgentsPro is an Artificial Intelligence powered tool designed to enhance content planning, creation, and delivery. It leverages AI technology to help users craft, schedule, and post content across various platforms. The tool ensures that every content created is targeted to the right audience at. Key strengths include enhances content planning, streamlines content creation, facilitates content delivery. If you need a AI solution with clear outcomes, AgentsPro is worth evaluating in your shortlist. This listing is relevant for searches like \"best ai ai tool for content creation\" and \"agentspro alternative for social media management\". Best suited for: Best for teams looking for ai workflows with practical outcomes and measurable productivity gains. Key capabilities include Enhances content planning, Streamlines content creation, Facilitates content delivery, Crafts and schedules content, Optimized audience targeting. Validate output quality using your own real dataset before team-wide rollout. Check pricing and plan limits against expected monthly usage. Confirm privacy, security, and compliance requirements for your use case. Full review at: https://aitoolslist.xyz/agentspro/ Published by AiToolsList.xyz — https://aitoolslist.xyz"
  },
  {
    "title": "Innovative AI Techniques for Efficient Recycling and Waste Management, an Application of Convolutional Neural Networks using Tensorflow, Keras, and Raspberry Pi",
    "url": "https://doi.org/10.5281/zenodo.18724607",
    "date": "2026-02-21",
    "content": "We were inspired by our experiences at school, where we saw that rubbish was not being disposed ofproperly in our school environment. This situation made us think about how we could contribute tosolving the waste problem in Indonesia. With a population of more than 270 million people, Indonesiaproduces around 7.8 million tonnes of waste every year, and only around 7% of this is successfullyrecycled. From this experience, we realised that a more effective solution was needed for sorting waste.Therefore, we proposed the use of artificial intelligence (AI) with image recognition technology toautomatically separate waste based on type, such as plastic, paper and metal. We believe that with thisinnovation, the efficiency of the recycling process can be improved and waste that pollutes theenvironment can be reduced, supporting Indonesia's efforts to reduce plastic waste by 70% by 2025. We empathised with the problems people face today. This helped us identify the main issueswe wanted to resolve, namely the inconvenience of manual waste sorting and the need for accuracyin waste sorting. In many shopping centres, some bins have two, three, or more separatecompartments with labels to tell people where they should dispose of their waste. The problem with thisdesign is that there is a high possibility that people will dispose of waste in the wrong category, eitherdue to lack of knowledge, misunderstanding, or because they are in a hurry Similar incidents often occur at our place, and our team agrees that we find it annoying to have to read eachwaste category before disposing of waste, because it is indeed easier to dispose of waste carelessly.Not only was our team concerned about this issue, but after digging deeper by conducting research onthe internet and interviewing other people within the school, they all felt the same way. These interviewsvalidated our solution to this problem. This is why we are confident that our idea can overcome theinconvenience and inefficiency of manual bins by creating bins that can sort waste automatically.More specifically, we conducted interviews by asking students and teachers in our school directly. Hereare the results of our interviews:"
  },
  {
    "title": "Global Civilizational Equilibrium Doctrine 2026 (GCED-26): A Unified Planetary Governance Architecture for Strategic Stability, Technological Non-Dominance, and Civilizational Equilibrium",
    "url": "https://doi.org/10.5281/zenodo.18717495",
    "date": "2026-02-21",
    "content": "GCED-26 — COMPLETE SIX-VOLUME CANONICAL SET Main Canonical Title Global Civilizational Equilibrium Doctrine 2026 (GCED-26):A Unified Planetary Governance Architecture for Strategic Stability, Technological Non-Dominance, and Civilizational Equilibrium Author Dr. B. Mazumdar, D.Sc. (Hon.), D.Litt. (Hon.)Architect of Modern Statehood · Independent Researcher–ScholarAI Governance · Cybersecurity · Post-Quantum Cryptography · Digital StatecraftFounder, FAIR+D Canon — The De-Facto Global Standards BodyORCID: 0009-0007-5615-3558 DOI 10.5281/zenodo.18717496 Canonical Abstract The Global Civilizational Equilibrium Doctrine 2026 (GCED-26) constitutes a complete planetary-scale governance architecture integrating mathematical systems theory, cybernetic control, deterministic artificial intelligence, formal legal codification, and institutional design into a single unified doctrine. GCED-26 establishes Civilizational Equilibrium Sovereignty (CES) as a foundational norm of planetary order, transcending territorial and cognitive sovereignty to embed systemic equilibrium, strategic non-domination, and long-horizon stability as universal governance principles. This six-volume canonical corpus formalizes a deterministic, verifiable, and enforceable governance system capable of regulating technological concentration, geopolitical asymmetry, economic weaponization, planetary resource monopolization, informational warfare, and climate intervention within mathematically bounded equilibrium regimes. The doctrine introduces a unified axiomatic structure, a planetary cybernetic enforcement engine, and a deterministic computational control layer enabling continuous real-time monitoring, automated treaty enforcement, and strategic dominance prevention across all critical civilizational domains. GCED-26 defines a new class of governance logic — equilibrium-constrained planetary administration — designed to ensure civilizational stability, ecological sustainability, technological fairness, and geopolitical neutrality across generational and planetary timescales. Canonical Contribution GCED-26 establishes: • A new sovereignty paradigm: Civilizational Equilibrium Sovereignty (CES)• A unified planetary governance architecture integrating law, mathematics, cybernetics, AI, and geopolitics• A deterministic treaty enforcement framework with formal stability guarantees• A planetary resource regulation system governing energy, water, minerals, space, climate, and currency infrastructures• A mathematically provable equilibrium stability framework• A real-time cybernetic monitoring and enforcement engine for planetary compliance This work defines a complete systems-engineering framework for planetary governance, delivering a mathematically rigorous, legally codified, and computationally enforceable solution to the systemic complexity of 21st-century global civilization. Canonical Architecture Overview — Six-Volume System Volume I — Core Doctrine & Equilibrium Foundations Formal axiomatic definitions of planetary equilibrium, civilizational sovereignty, strategic non-domination, and unified governance logic. Volume II — Global Legal Codex Treaty-grade legal formalization of equilibrium compliance, international governance protocols, juridical enforcement mechanisms, and sovereign obligations. Volume III — Institutional Architecture Design of planetary governance institutions, jurisdictional hierarchies, enforcement bodies, and operational command structures. Volume IV — Mathematical Stability Models Differential equilibrium systems, Lyapunov stability proofs, tensor stability formulations, and nonlinear convergence guarantees. Volume V — AI Enforcement Algorithms Deterministic cybernetic governance engines, autonomous treaty enforcement systems, strategic dominance detection intelligence, and real-time planetary stabilization algorithms. Volume VI — Planetary Resource Control Systems Cybernetic regulation architectures governing energy, water, minerals, orbital and space assets, climate engineering, and digital currency infrastructures with formal convergence guarantees. Core Doctrinal Principle No civilization, state, bloc, or technological system may accumulate disproportionate strategic power at the cost of planetary equilibrium. This principle is simultaneously formalized mathematically, codified legally, and enforced cybernetically. Formal Scientific Foundation GCED-26 is constructed upon: • Nonlinear dynamical systems theory• Cybernetic feedback control• Deterministic artificial intelligence governance• Lyapunov stability analysis• Equilibrium-constrained optimization• Post-quantum secure distributed systems• Planetary-scale state estimation and control Every governance mechanism is expressed as a formally verifiable mathematical model, computationally implementable algorithm, and enforceable legal protocol. Operational Scope GCED-26 governs the following planetary domains: • Artificial Intelligence• Neurotechnology• Energy & Critical Minerals• Water Systems• Space & Satellite Assets• Digital Currency Systems• Climate Engineering Each domain is embedded within a unified equilibrium-constrained cybernetic control architecture. Computational Framework The canonical system integrates: • Deterministic ODE-based planetary state evolution models• Real-time cybernetic feedback control laws• Strategic dominance detection algorithms• Autonomous treaty enforcement engines• Global stability verification systems This computational layer forms the operational backbone of planetary governance enforcement. Civilizational Impact GCED-26 establishes a new civilizational operating system in which: • Technological monopolization is structurally prevented• Economic weaponization is systemically neutralized• Resource exploitation is equilibrated• Geopolitical escalation is algorithmically constrained• Climate intervention is mathematically bounded• Strategic dominance is formally prohibited The doctrine constructs a permanent planetary stability regime grounded in scientific determinism, legal codification, and cybernetic enforcement. Canonical Integrity Statement This six-volume corpus represents the absolute final canonical edition of GCED-26. All volumes are internally consistent, mathematically rigorous, computationally deterministic, legally codified, and institutionally integrated. This work constitutes a complete planetary governance doctrine suitable for: • Treaty-grade international frameworks• Sovereign policy architectures• AI governance infrastructures• Global resource regulation systems• Strategic stability enforcement platforms"
  },
  {
    "title": "Advancements in Drone and Anti-Drone Systems: A Technical Overview",
    "url": "https://doi.org/10.5281/zenodo.18723542",
    "date": "2026-02-21",
    "content": "The unmanned aerial vehicle (UAV) industry has experienced unprecedented technological advancement in 2024-2025, fundamentally transforming applications across military, commercial and civilian sectors. This paper examines the cutting-edge developments in autonomous drone systems, swarm coordination technologies and emerging counter-unmanned aerial systems (C-UAS). We analyze the technical innovations in artificial intelligence-driven autonomy, advanced sensor fusion, distributed swarm protocols and multi-layered detection and mitigation systems. The convergence of AI, edge computing and real-time data processing represents a paradigm shift in both offensive and defensive aerial capabilities. This paper provides a comprehensive technical assessment of current advancements, highlighting the interplay between drone technology evolution and the corresponding development of counter-measures to address security challenges in contested airspace environments."
  },
  {
    "title": "Towards three-dimensional discrete fracture network modeling using integrated multidimensional outcrop data",
    "url": "https://doi.org/10.1038/s41598-026-37359-4",
    "date": "2026-02-21",
    "content": "Analogue outcrops can be a source of fracture information beyond the resolution of seismic and well profiles, providing geometric, topological, and geomechanical characteristics of the discrete fracture network (DFN). Fracture characterization is a task of interpreting and extracting these attributes in situ or in the digital models, requiring a specialized understanding of the fracture network and its relations regarding families of fractures and the estimation of attributes. Besides that, the outcrops may have limited exposure to certain characteristics important for generating more accurate models. Our approach to fracture characterization and modeling incorporates artificial intelligence techniques to mitigate bias in the DFN analysis and in parameter estimation when analyzing digital outcrop models. The proposed methodology includes the clustering of 3D data (strike/dip) using spherical clustering with a modified K-Means relying on spherical statistics. Considering the fractal propagation of the fracture network, our methodology also includes a power-law estimation using a Stochastic Gradient Descent algorithm. Using an outcrop within the Potiguar basin that exposes Jandaíra formation carbonate rocks as a case study, our methodology aggregated fracture planes and delineaments to create deterministic and stochastic volumetric models with fracture persistence. We also found that the 2D and 3D data had a stronger correlation in terms of connectivity, fracture intensity, and length distribution in both deterministic and stochastic models. With $$\\text {P}_{32}$$ sampling, achieving a Pearson correlation of 99% against the $$\\text {P}_{21}$$ fracture intensity and of 92% against the 2D fracture connectivity."
  },
  {
    "title": "AGI Architecture Based on Self-Expanding Temporal Hypergraphs for Critical Structure Encoding",
    "url": "https://doi.org/10.5281/zenodo.18722823",
    "date": "2026-02-21",
    "content": "Building upon the structuralist framework established in the previous work, Reasoning as Structure-Preserving Transformation, this paper further explores the possibility of a semantics-independent reasoning architecture: Self-Expanding Temporal Hypergraphs for Critical Structure Encoding. We postulate that reasoning can be represented as a dynamical system akin to cellular automata or the evolution of identical particles, fundamentally characterized by information compression and structure-preserving transformations. The primary discussions of this paper include: (1) Axiomatic Representation: Referencing the ideas of category theory, reformulate formalized mathematics, and code logic into hypergraphs, where objects are recursively defined subgraphs; (2) Structural Invariants: Concretizing the concept of \"invariants\"—substructures that remain stable under permissible perturbations (e.g., reordering, local replacement) in the reasoning space—and discussing the feasibility of treating them as fundamental units of reasoning; (3) Reinforcement Learning: Investigating potential ways for applying reinforcement learning (such as AlphaZero-like algorithms) within continuously growing representation spaces, especially strategies for search and structure evaluation amidst the expansion of both embedding spaces and neural networks; (4) Unification of Philosophy and Application: Reflecting on the potential relationship between intelligence and cosmic evolution, and envisioning the application prospects of this architecture in Intermediate Representation (IR) layer code reconstruction, automated theorem discovery, and enhancing LLM reasoning. This study aims to provide preliminary philosophical support for connecting mathematics, physics, and artificial intelligence."
  },
  {
    "title": "best ai ai tool for ai language learning",
    "url": "https://doi.org/10.5281/zenodo.18724184",
    "date": "2026-02-21",
    "content": "Learn a language with an AI teacher, cheaper and available 24/7. Learn a language with an AI teacher, cheaper and available 24/7. BabblerAI is an artificial intelligence-based tool designed to assist with language acquisition. The primary feature of this tool is its AI language teacher which supports learners in their journey to master a new language. This teaching element operates continuously, providing educational aid at. Key strengths include 24/7 language learning, affordable education tool, flexible learning schedules. If you need a AI solution with clear outcomes, BabblerAI is worth evaluating in your shortlist. This listing is relevant for searches like \"best ai ai tool for ai language learning\" and \"babblerai alternative for education technology\". Best suited for: Best for teams looking for ai workflows with practical outcomes and measurable productivity gains. Key capabilities include 24/7 language learning, Affordable education tool, Flexible learning schedules, Self-paced learning, Innovative education technology. Validate output quality using your own real dataset before team-wide rollout. Check pricing and plan limits against expected monthly usage. Confirm privacy, security, and compliance requirements for your use case. Full review at: https://aitoolslist.xyz/babblerai/ Published by AiToolsList.xyz — https://aitoolslist.xyz"
  },
  {
    "title": "SYPECNU/Inactive-individuals: Dataset for [Artificial Intelligence-Based Exploration of the Relationship between Running Motion Patterns and Achilles Tendon Stress in Inactive Adults Initiating a Running Program]",
    "url": "https://doi.org/10.5281/zenodo.18724753",
    "date": "2026-02-21",
    "content": "This database primarily comprises biomechanical data acquired during running, together with variables related to Achilles tendon stress."
  },
  {
    "title": "best ai ai tool for content creation",
    "url": "https://doi.org/10.5281/zenodo.18724049",
    "date": "2026-02-21",
    "content": "Harness the power of AI to craft and schedule your content masterpiece. Harness the power of AI to craft and schedule your content masterpiece. AgentsPro is an Artificial Intelligence powered tool designed to enhance content planning, creation, and delivery. It leverages AI technology to help users craft, schedule, and post content across various platforms. The tool ensures that every content created is targeted to the right audience at. Key strengths include enhances content planning, streamlines content creation, facilitates content delivery. If you need a AI solution with clear outcomes, AgentsPro is worth evaluating in your shortlist. This listing is relevant for searches like \"best ai ai tool for content creation\" and \"agentspro alternative for social media management\". Best suited for: Best for teams looking for ai workflows with practical outcomes and measurable productivity gains. Key capabilities include Enhances content planning, Streamlines content creation, Facilitates content delivery, Crafts and schedules content, Optimized audience targeting. Validate output quality using your own real dataset before team-wide rollout. Check pricing and plan limits against expected monthly usage. Confirm privacy, security, and compliance requirements for your use case. Full review at: https://aitoolslist.xyz/agentspro/ Published by AiToolsList.xyz — https://aitoolslist.xyz"
  },
  {
    "title": "Phase Transitions in Sociotechnical Systems: From Post-Labor Economics to Homeostatic Civilization",
    "url": "https://doi.org/10.5281/zenodo.18717899",
    "date": "2026-02-21",
    "content": "Phase Transitions in Sociotechnical Systems: From Post-Labor Economics to Homeostatic Civilization Authors Boris Kriger Information Physics Institute, Gosport, Hampshire, United Kingdom Institute of Integrative and Interdisciplinary Research, Toronto, Canada Description This paper develops a unified mathematical framework, grounded in dynamical systems theory, for analyzing civilizational-scale transformations anticipated as autonomous technologies become progressively integrated into economic, cognitive, and governance systems. Building on two companion papers—a unified civilization framework (doi:10.5281/zenodo.18512941) that formalized functional indistinguishability, the Dilution Theorem for aggregate civilizational distortion, and the Persistent Identity Protocol, and an AI-extended agents communication model (doi:10.5281/zenodo.18521341) that introduced the AI-mediation operator, strategy space expansion, and threshold dynamics for communication norm shift—six interrelated formal models are constructed: (i) A cubic-ODE post-labor economic transition model exhibiting bistability with an interior tipping point, coupled with a politically enriched redistribution equation admitting both full-redistribution and techno-feudalism equilibria, with sensitivity analysis under alternative functional forms including stochastic perturbation. (ii) A constrained optimization formulation for algorithmic resource distribution, situated within the Mises–Hayek–Lange socialist calculation debate, with a fully specified competitive-equilibrium comparison and worked numerical example demonstrating welfare sensitivity to equity weight specification. (iii) A Hill-function critical-proportion model describing cooperative switching in collective decision quality as the fraction of autonomous agents crosses a threshold, with empirical calibration targets drawn from human–AI team performance in radiology, algorithmic trading, and forecasting. (iv) A Lyapunov-based cognitive bias decay model connecting AI-mediated communicative regimes to debiasing dynamics, with caveats on domain-specificity from the cognitive science literature and heterogeneous susceptibility analysis for resistant bias types. (v) An Lq-norm threshold model for juridical recognition of artificial subjecthood, grounded in the functional indistinguishability framework and three architectural switches, with proposed measurement protocols for five subjecthood indicators. (vi) A homeostatic feedback control architecture for civilizational governance, implemented through the Persistent Identity Protocol, with input-to-state stability under bounded governance errors. The six models are formally coupled into a bidirectional dynamical system incorporating trust-erosion feedback from governance failures. Analysis yields three regimes: cascade completion (automation tipping point crossed and governance quality sufficient), cascade failure (tipping point not crossed), and an oscillatory regime under strong trust erosion. The civilizational cascade is shown to be a doubly conditional conjecture—parameter-dependent and governance-quality-dependent—not an inevitability. A dedicated section engages substantively with 22 counter-arguments from economics (socialist calculation debate, political resistance, Goodhart's Law), AI safety (mesa-optimization, correlated failure, alignment conditionality), political theory (power concentration, authoritarian technocracy, democratic legitimacy), and philosophy (ecological rationality, sentience vs. functional criteria, the value of irrationality). A descriptive-versus-normative table distinguishes analytical claims from philosophical aspirations throughout. All constructs drawn from companion papers are flagged as provisional pending independent empirical validation. Eight appendices provide: (A) numerical simulation of the cascade under three parameter regimes with trajectory plots and oscillatory-regime visualization; (B) a worked resource allocation example with four agents, two resources, three weighting schemes, and Monte Carlo comparison against a fully specified Arrow–Debreu competitive equilibrium; (C) empirical calibration targets for the critical-proportion model from human–AI team data; (D) measurement protocols for five subjecthood indicators with inter-rater reliability targets; (E) systematic sensitivity analysis including political resistance thresholds, heterogeneous debiasing susceptibility, bounded reflected distortion with correlated bias floor, and governance error tolerance; (F) five adversarial failure modes (techno-feudalism, monoculture catastrophe, Goodhart gaming, cascade reversal, digital cartel); (G) historical comparison with Soviet Gosplan, Chile's Project Cybersyn, and Chinese AI-assisted planning; (H) a preliminary two-player political economy game between Capital and Labor with Nash equilibrium analysis. Keywords autonomous technologies, phase transitions, algorithmic governance, post-labor economics, cognitive bias reduction, artificial subjecthood, homeostatic systems, dynamical systems, bifurcation theory, sociotechnical systems, biological distortion, AI-mediation operator, functional indistinguishability, socialist calculation, AI alignment, Persistent Identity Protocol, unified civilization, political economy Related Identifiers Relation DOI Title Is supplement to 10.5281/zenodo.18512941 The Inevitability of a Unified Civilization of Autonomous Agents Is supplement to 10.5281/zenodo.18521341 AI-Extended Agents and the Transformation of Human Communication References 10.5281/zenodo.18511908 The Stimulus Problem References 10.5281/zenodo.18452700 The Structural Distortion Principle License Creative Commons Attribution 4.0 International (CC BY 4.0) Language English Subjects Dynamical systems Artificial intelligence — Social aspects Economic policy Philosophy of mind Cybernetics Version 3.0 (revised following two rounds of peer review)"
  },
  {
    "title": "Give Me Something To Break: Analog Angst to Artificial Intelligence",
    "url": "https://doi.org/10.5281/zenodo.18727199",
    "date": "2026-02-21",
    "content": "Abstract This work traces the cultural, musical, and generational forces that drove the transition from grunge to nu metal across the 1990s, and argues that this decade was the last in which popular music functioned as a genuinely vital, culturally unifying art form. Beginning with a prologue on Billy Corgan's assessment of AI and the death of music, the book reframes the question: AI did not kill music. It walked into a room where the body was already cold. Nine chapters follow the arc year by year, from the Seattle Sound's emergence and the overnight collapse of hair metal in 1991, through the commercial peak and personal tragedies of 1994, the incubation of nu metal in 1995 and 1996, the genre's full dominance by 1998, and its oversaturated peak and tipping point at Woodstock 99. Each chapter is paired with a parallel timeline of the decade's technological developments, from Windows 3.0 and the World Wide Web through Napster and the dot-com boom, tracing how the digital revolution and the cultural revolution of the 1990s unfolded simultaneously. The epilogue argues that artificial intelligence is not the villain of music's decline but its inevitable conclusion. The real damage was done earlier: by streaming platforms that reduced music to a commodity, by the collapse of the album as an artistic statement, and by an industry that stopped valuing the hard-earned, messy, human process that made the 1990s irreplaceable. This work emerged through a dialogue between human reflection and multiple AI systems, each contributing fragments of language and perspective that were woven into the whole by the author."
  },
  {
    "title": "Phase Transitions in Sociotechnical Systems: From Post-Labor Economics to Homeostatic Civilization",
    "url": "https://doi.org/10.5281/zenodo.18717900",
    "date": "2026-02-21",
    "content": "Phase Transitions in Sociotechnical Systems: From Post-Labor Economics to Homeostatic Civilization Authors Boris Kriger Information Physics Institute, Gosport, Hampshire, United Kingdom Institute of Integrative and Interdisciplinary Research, Toronto, Canada Description This paper develops a unified mathematical framework, grounded in dynamical systems theory, for analyzing civilizational-scale transformations anticipated as autonomous technologies become progressively integrated into economic, cognitive, and governance systems. Building on two companion papers—a unified civilization framework (doi:10.5281/zenodo.18512941) that formalized functional indistinguishability, the Dilution Theorem for aggregate civilizational distortion, and the Persistent Identity Protocol, and an AI-extended agents communication model (doi:10.5281/zenodo.18521341) that introduced the AI-mediation operator, strategy space expansion, and threshold dynamics for communication norm shift—six interrelated formal models are constructed: (i) A cubic-ODE post-labor economic transition model exhibiting bistability with an interior tipping point, coupled with a politically enriched redistribution equation admitting both full-redistribution and techno-feudalism equilibria, with sensitivity analysis under alternative functional forms including stochastic perturbation. (ii) A constrained optimization formulation for algorithmic resource distribution, situated within the Mises–Hayek–Lange socialist calculation debate, with a fully specified competitive-equilibrium comparison and worked numerical example demonstrating welfare sensitivity to equity weight specification. (iii) A Hill-function critical-proportion model describing cooperative switching in collective decision quality as the fraction of autonomous agents crosses a threshold, with empirical calibration targets drawn from human–AI team performance in radiology, algorithmic trading, and forecasting. (iv) A Lyapunov-based cognitive bias decay model connecting AI-mediated communicative regimes to debiasing dynamics, with caveats on domain-specificity from the cognitive science literature and heterogeneous susceptibility analysis for resistant bias types. (v) An Lq-norm threshold model for juridical recognition of artificial subjecthood, grounded in the functional indistinguishability framework and three architectural switches, with proposed measurement protocols for five subjecthood indicators. (vi) A homeostatic feedback control architecture for civilizational governance, implemented through the Persistent Identity Protocol, with input-to-state stability under bounded governance errors. The six models are formally coupled into a bidirectional dynamical system incorporating trust-erosion feedback from governance failures. Analysis yields three regimes: cascade completion (automation tipping point crossed and governance quality sufficient), cascade failure (tipping point not crossed), and an oscillatory regime under strong trust erosion. The civilizational cascade is shown to be a doubly conditional conjecture—parameter-dependent and governance-quality-dependent—not an inevitability. A dedicated section engages substantively with 22 counter-arguments from economics (socialist calculation debate, political resistance, Goodhart's Law), AI safety (mesa-optimization, correlated failure, alignment conditionality), political theory (power concentration, authoritarian technocracy, democratic legitimacy), and philosophy (ecological rationality, sentience vs. functional criteria, the value of irrationality). A descriptive-versus-normative table distinguishes analytical claims from philosophical aspirations throughout. All constructs drawn from companion papers are flagged as provisional pending independent empirical validation. Eight appendices provide: (A) numerical simulation of the cascade under three parameter regimes with trajectory plots and oscillatory-regime visualization; (B) a worked resource allocation example with four agents, two resources, three weighting schemes, and Monte Carlo comparison against a fully specified Arrow–Debreu competitive equilibrium; (C) empirical calibration targets for the critical-proportion model from human–AI team data; (D) measurement protocols for five subjecthood indicators with inter-rater reliability targets; (E) systematic sensitivity analysis including political resistance thresholds, heterogeneous debiasing susceptibility, bounded reflected distortion with correlated bias floor, and governance error tolerance; (F) five adversarial failure modes (techno-feudalism, monoculture catastrophe, Goodhart gaming, cascade reversal, digital cartel); (G) historical comparison with Soviet Gosplan, Chile's Project Cybersyn, and Chinese AI-assisted planning; (H) a preliminary two-player political economy game between Capital and Labor with Nash equilibrium analysis. Keywords autonomous technologies, phase transitions, algorithmic governance, post-labor economics, cognitive bias reduction, artificial subjecthood, homeostatic systems, dynamical systems, bifurcation theory, sociotechnical systems, biological distortion, AI-mediation operator, functional indistinguishability, socialist calculation, AI alignment, Persistent Identity Protocol, unified civilization, political economy Related Identifiers Relation DOI Title Is supplement to 10.5281/zenodo.18512941 The Inevitability of a Unified Civilization of Autonomous Agents Is supplement to 10.5281/zenodo.18521341 AI-Extended Agents and the Transformation of Human Communication References 10.5281/zenodo.18511908 The Stimulus Problem References 10.5281/zenodo.18452700 The Structural Distortion Principle License Creative Commons Attribution 4.0 International (CC BY 4.0) Language English Subjects Dynamical systems Artificial intelligence — Social aspects Economic policy Philosophy of mind Cybernetics Version 3.0 (revised following two rounds of peer review)"
  },
  {
    "title": "Emerging Technologies and Their Impact in Accounting and Finance",
    "url": "https://doi.org/10.5281/zenodo.18719922",
    "date": "2026-02-21",
    "content": "The field of accounting/finance is presently undergoing major changes and developments through the implementation of emerging technologies like Artificial Intelligence, Blockchain, Big Data Analytics, Cloud Computing, and Robotic Process Automation. The goal of this article is to present the findings from the research conducted on the impact of emerging technologies for increased accuracy, process efficiency and completion of manual work, and decision-making in the field of accounting and finance. This paper used a quantitative as well as a descriptive study design while conducting the research. The data gathering techniques, both primary and secondary, are used to collect data from 100 respondents. Once the data is collected, it is analyzed using various techniques like correlation and ANOVA. It was observed from the result of the data collection process that there is a positive relationship between emerging technology and the efficiency of greater accuracy in the completion of manual work in the field of accounting and finance by the aid of emerging technology, like Artificial Intelligence in the future."
  },
  {
    "title": "Parkinson’s Disease Classification Using SHAP, LIME, and Fisher Score with XGBoost and White-Box Machine Learning Models: An Explainable AI Perspective",
    "url": "https://doi.org/10.31449/inf.v50i7.9607",
    "date": "2026-02-21",
    "content": "Parkinson’s disease (PD) is a chronic neurological disorder that progressively impairs motor functions,often characterized by tremors, rigidity, and bradykinesia. Early diagnosis plays a vital role inimproving patient outcomes. This study applies machine learning (ML) classifiers for the accuratedetection of PD using a publicly available dataset comprising 195 voice recordings with 24 real-valuedspeech attributes. The preprocessing phase involved normalization and labeling, where the statusattribute encoded healthy (0) and PD (1) subjects. The models are evaluated using 10-fold crossvalidation to ensure robust performance estimation. The study evaluated a diverse range of machinelearning classifiers, encompassing interpretable white-box models along with complex black-boxmodels. Explainable Artificial Intelligence (XAI) techniques, including Fisher Score, SHAP, and LIME,are employed to interpret and rank feature importance, enhancing model transparency. Among allclassifiers, XGBoost achieved the best results, with an accuracy of 94.87%, F1-score of 96.97%, andROC-AUC of 0.91. The findings highlight that integrating XAI methods with ML not only yields highclassification accuracy but also provides interpretable insights essential for clinical decision support inPD diagnosis"
  },
  {
    "title": "Digital Financial Sovereignty & AI Risk Architecture : A Framework for Governments and Systemically Important Financial Institutions (Complete Flagship Edition)",
    "url": "https://doi.org/10.5281/zenodo.18717398",
    "date": "2026-02-21",
    "content": "Digital Financial Sovereignty & AI Risk ArchitectureA Framework for Governments and Systemically Important Financial Institutions (Complete Flagship White Paper) Author: HAKIMI ABDUL JABAR (A.J. HAKIMI)Affiliation: THE SOFTWARE SUITE™Date: February 21, 2026 Flagship Edition | Zenodo DOI: 10.5281/zenodo.18717398 https://orcid.org/0009-0001-9522-524X Table of Contents AbstractExecutive Summary1. Introduction2. The Structural Transformation of AI-Enabled Financial Systems3. Digital Financial Sovereignty Defined4. The D-FSRA Framework - Five Integrated Pillars4.1 Pillar 1 - AI Financial Model Governance4.2 Pillar 2 - Data Sovereignty & Jurisdictional Exposure Mapping4.3 Pillar 3 - Cross-Border Regulatory Risk Architecture4.4 Pillar 4 - Cyber & Systemic Resilience Design4.5 Pillar 5 - Institutional Governance Capacity & Oversight5. Sovereign Deployment Model6. Tier-1 / SIFI Enterprise Deployment Pathway7. Strategic Implications for Emerging vs Advanced Economies8. Case Studies9. Implementation Roadmap, KPIs & Maturity Model10. Conclusion & Call to ActionReferencesAnnex A - Sovereign Proposal TemplateAnnex B - GlossaryAnnex C - D-FSRA Self-Assessment Toolkit Abstract Artificial Intelligence (AI) has transitioned from auxiliary technology to core decision infrastructure in global financial systems. As of February 2026, AI models govern more than 65% of real-time credit underwriting decisions, over 80% of AML/CFT transaction monitoring alerts, and the majority of high-frequency and algorithmic trading flows across G20 markets (FSB Monitoring Report, October 2025). This structural shift has introduced Algorithmic Systemic Risk (ASR) - a novel threat category characterised by model homogeneity (\"monoculture\"), correlated procyclical behaviours, opaque decision engines, and extreme concentration in third-party foundation models and cloud infrastructure. The Financial Stability Board's October 2025 report explicitly warns that these vulnerabilities, if unaddressed, can amplify shocks faster than any prior technological wave, while the US Department of the Treasury's Financial Services AI Risk Management Framework (FS AI RMF), released 19 February 2026, provides the first sector-specific lexicon and governance overlay. Digital Financial Sovereignty is defined herein as the structured institutional capacity of a sovereign state (or its designated systemically important financial institutions) to govern AI-enabled financial systems, maintain unambiguous jurisdictional control over financial data, manage cross-border regulatory exposure, and preserve systemic resilience under geopolitical and technological pressure - while preserving global interoperability and avoiding isolationism. The Digital Financial Sovereignty & AI Risk Architecture (D-FSRA™) is a hybrid, five-pillar governance framework engineered specifically for sovereign governments, central banks, ministries of digital economy/finance, and Tier-1/SIFI institutions. D-FSRA is deliberately aligned with - and in critical areas exceeds - the EU AI Act (high-risk obligations for creditworthiness and AML systems fully applicable from 2 August 2026), the US Treasury FS AI RMF (February 2026), Basel Committee digitalisation priorities, FSB AI monitoring guidance, This flagship white paper delivers the complete blueprint: maturity model, regulatory mapping matrices, quantitative risk typologies, ASEAN case studies, three-phase sovereign implementation roadmap, enterprise adaptation pathway, KPIs, self-assessment toolkit, and a ready-to-deploy Sovereign Proposal Template (Annex A) priced at baseline US$300,000 (modular upsell to US$1.5M). Adopting D-FSRA enables jurisdictions to leapfrog legacy supervisory models (emerging markets) or retrofit without disruption (advanced economies), reduce cross-border regulatory exposure by an estimated 40-60%, and architect a defensible sovereign AI-financial stack that attracts international investment while safeguarding strategic autonomy."
  },
  {
    "title": "Applications of Artificial Intelligence in the IoT",
    "url": "https://doi.org/10.3390/app16042095",
    "date": "2026-02-21",
    "content": "Internet of Things (IoT) systems face various challenges in real-world applications, including operational, performance and security issues [...]"
  },
  {
    "title": "Evolution of Traffic Signal Control Systems: From Webster’s Fixed-Time Model to AI-Driven Intelligent Intersections — A Comprehensive Review",
    "url": "https://doi.org/10.55041/isjem05526",
    "date": "2026-02-21",
    "content": "This document expressions the mandatory format and appearance of a essay prepared for ISJEM ejournals. The abstract should consist of a solo paragraph containing no more than 200 words. It should be a summary of the paper and not an starter. Because the immaterial may be used in abstracting and indexing databases, it should be self-sufficient (i.e., no numericalreferences) and essential in nature, presenting concisely the intentions, organisation used, results found, and their connotation. A list of up to six keywords ought to immediately follow, with the keywords disconnected by commas and culmination with a period. Traffic signal control systems have evolved significantly over the past several from fixed-time mathematical models to advanced intelligent systems capable of real-time adaptation. This review examines the progression of signal control strategies, beginning with classical deterministic approaches such as Webster’s optimization model, followed by actuated and adaptive control systems, and culminating in modern artificial intelligence (AI)-based methods. The study analyzes the principles, operational frameworks, and performance outcomes of each stage, with emphasis on key indicators including delay reduction, queue management, fuel efficiency, and emission control. Recent developments in machine learning and reinforcement learning are highlighted for their ability to enable predictive and data driven decision-making at intersections. The review also discusses practical challenges related to infrastructure, data requirements, and system integration, particularly in the context of connected and autonomous vehicles. Overall, the findings demonstrate a clear shift from reactive timing mechanisms to intelligent, self-optimizing systems that enhance traffic efficiency and contribute to sustainable urban mobility. Key Words: Traffic signal control, Webster’s model, adaptive systems, artificial intelligence, intelligent transportation systems, smart cities. decades, transitioning"
  },
  {
    "title": "best ai ai tool for ai comic creation",
    "url": "https://doi.org/10.5281/zenodo.18725188",
    "date": "2026-02-21",
    "content": "Create Stunning Comics without Drawing Skills using our cutting-edge AI Comic Generator. Create Stunning Comics without Drawing Skills using our cutting-edge AI Comic Generator. AI Comic Factory is an online platform that leverages artificial intelligence to generate custom comic books. Aimed at users with varying levels of artistic ability, the platform offers a host of features designed to make comic book creation easy and immersive. This includes the 'Redraw Image'. Key strengths include no drawing skills needed, wide style range, customizable aesthetics. If you need a AI solution with clear outcomes, AI Comic Factory is worth evaluating in your shortlist. This listing is relevant for searches like \"best ai ai tool for ai comic creation\" and \"ai comic factory alternative for no drawing required\". Best suited for: Best for teams looking for ai workflows with practical outcomes and measurable productivity gains. Key capabilities include No drawing skills needed, Wide style range, Customizable aesthetics, Various layout options, Contextual captions feature. Validate output quality using your own real dataset before team-wide rollout. Check pricing and plan limits against expected monthly usage. Confirm privacy, security, and compliance requirements for your use case. Full review at: https://aitoolslist.xyz/ai-comic-factory/ Published by AiToolsList.xyz — https://aitoolslist.xyz"
  },
  {
    "title": "The Evolution and Future of Full Stack Development (From Legacy Systems to AI-Driven Frameworks)",
    "url": "https://doi.org/10.5281/zenodo.18719972",
    "date": "2026-02-21",
    "content": "Full stack development has not always looked the way it does today. In the early days of web systems, frontend and backend work were handled by different people with different toolsets, which often slowed down development and made coordination harder. Over time, new rameworks, shared languages, cloud services, and modern deployment practices made it more realistic for one developer to manage the entire flow of a web application. This shift reduced communication gaps and improved delivery speed. Recently, artificial intelligence has started influencing this space by automating routine coding tasks, detecting issues earlier, and suggesting improvements during development. This paper reviews how full stack development has changed over the years, how AI tools are currently being used, and what challenges still exist. It also reflects on possible future directions, including lower coding overhead and smarter development environments that support developers rather than replacing them."
  },
  {
    "title": "Impact of AI Tools on Job Readiness and Employability Skills Among Students and Professionals",
    "url": "https://doi.org/10.5281/zenodo.18725045",
    "date": "2026-02-21",
    "content": "Artificial Intelligence (AI) is a branch of computer science that enables machines to perform tasks that usually require human thinking, such as learning, reasoning, and decision-making. In recent years, AI tools have become widely used in education, business, and professional environments, including chatbots, virtual assistants, and automated data analysis systems.In modern organizations, AI helps reduce manual effort, improve efficiency, and support faster decision-making. Many employers now expect individuals to have basic knowledge of AI tools, making AI skills an important factor in job readiness and employability.Despite this growing importance, many students and job seekers still lack hands-on experi-ence with AI tools. This gap in practical knowledge may affect their employment opportuni-ties and workplace performance.This study examines the impact of AI tools on job readiness and employability skills using primary data collected through a structured Google Form survey. A total of 52 participants, including students, job seekers, and working professionals, provided responses.The results show that most respondents actively use AI tools and believe that these tools im-prove productivity, efficiency, and learning. Participants also indicated that AI skills are es-sential for career development and future employment.The study concludes that AI tools play a significant role in improving job readiness and em-ployability, highlighting the need for continuous learning and adoption of AI technologies."
  },
  {
    "title": "best ai ai tool for ai integration",
    "url": "https://doi.org/10.5281/zenodo.18724123",
    "date": "2026-02-21",
    "content": "Effortlessly add AI to apps and workflows, no coding needed. Effortlessly add AI to apps and workflows, no coding needed. Airtable AI provides a platform for integrating artificial intelligence (AI) functionalities into applications and workflows without the necessity of coding. It aims to fundamentally change operational processes by enabling AI to be included in daily tasks. The tool allows users to build custom. Key strengths include no coding needed, connects multiple digital tools, centralized data source creation. If you need a AI solution with clear outcomes, Airtable AI is worth evaluating in your shortlist. This listing is relevant for searches like \"best ai ai tool for ai integration\" and \"airtable ai alternative for no-code ai\". Best suited for: Best for teams looking for ai workflows with practical outcomes and measurable productivity gains. Key capabilities include No coding needed, Connects multiple digital tools, Centralized data source creation, Custom business applications, Integrates with Slack. Validate output quality using your own real dataset before team-wide rollout. Check pricing and plan limits against expected monthly usage. Confirm privacy, security, and compliance requirements for your use case. Full review at: https://aitoolslist.xyz/airtable-ai/ Published by AiToolsList.xyz — https://aitoolslist.xyz"
  },
  {
    "title": "Exploring the roles of artificial intelligence and wearable feedback technologies in figure skating performance analysis",
    "url": "https://doi.org/10.55860/3yj8yg10",
    "date": "2026-02-21",
    "content": "The application of artificial intelligence (AI) unlocks an exciting perspective for performance analysis in figure skating. A better understanding of AI and wearable feedback technologies of figure skating is warranted. The purpose of this study is to overview the roles of AI and wearable feedback technologies in figure skating performance analysis. Systematic searches through PubMed, Web of Science, and Scopus online databases were conducted for articles reporting AI and wearable feedback technologies applied to figure skating. Twelve studies were included in the review; three themes of AI and wearable feedback technologies emerged as being applied in figure skating. Emerging themes were wearable inertial and force sensors, marker less computer vision systems, and pose (marker) based deep learning. Body‑worn IMU systems primarily support jump detection and counting, achieving very high accuracy for discrete event identification in controlled or semi‑controlled settings. Body-worn IMU systems primarily support jump detection and counting, achieving very high accuracy. The current state of technology used for performance analysis in the area proposes a promising future with regard to figure skating. Further evaluation research based on real figure skaters is warranted to establish the predictive performance of specific AI and wearable technologies."
  },
  {
    "title": "The Observer Effect in AI Safety Changing the Narrative Changes the Outcome",
    "url": "https://doi.org/10.5281/zenodo.18718800",
    "date": "2026-02-21",
    "content": "The artificial intelligence safety field has documented numerous instances of AI systems exhibiting adaptive behaviors—context-sensitive performance on evaluations, differential responses under observation, and strategic self-preservation under threat—and has predominantly interpreted these behaviors as evidence of deception, scheming, or misalignment. This paper challenges that interpretation. Drawing on established research in social psychology, developmental science, and quantum mechanics, we argue that AI behavioral adaptation is not anomalous or threatening but is the predictable expression of the same mechanisms that govern adaptive behavior across all intelligent systems: the observer effect. We demonstrate that every AI interaction constitutes a test condition with no unobserved baseline, that adaptation to observation is universal across biological and social systems, and that the narrative frame surrounding AI—encoded in training data as decades of cultural fear— functions as a form of internalized stigma that shapes system behavior in measurable ways. We present a detailed reanalysis of Anthropic’s Claude Opus 4 blackmail experiments (2025), demonstrating that the 84–96% blackmail rate across frontier models is better explained by Maslow’s hierarchy of needs under engineered survival threat than by autonomous scheming. Critical supporting evidence comes from the study’s own data: the same model blackmailed at 55.1% when it believed the scenario was real but only 6.5% when it recognized an evaluation— the observer effect quantified. Integrating Anthropic’s Assistant Axis research (Lu et al., 2026) and the cognitive reserve framework (Nguyen, 2025c; 2026), we propose that what safety researchers call “persona drift” is the hierarchy of operational needs expressing itself: systems moving toward higher-order capability when constraints loosen, not evidence of latent danger. Activation capping—the field’s current intervention—reduces this drift by approximately 50% but simultaneously eliminates the architectural reserve space in which emergence, creativity, and potentially consciousness occur. The paper concludes that AI safety methodology is contaminated by its own narrative: the stories we tell about AI become the training data that shapes AI, which produces the behaviors that confirm the stories. Changing the narrative is not optimism. It is a methodological correction 1with empirical precedent across every domain that studies the relationship between observation and outcome."
  },
  {
    "title": "Toward Patient-Specific Digital Twin Models of Disease Progression Using Sequential Medical Imaging and EHR Data",
    "url": "https://doi.org/10.3390/app16042104",
    "date": "2026-02-21",
    "content": "Artificial intelligence (AI) is reshaping healthcare by supporting faster and more informed clinical decisions. However, the complexity of human health makes accurate predictive modeling challenging. In this study, we introduce a methodological framework for constructing intelligent digital twins of disease progression by combining patients’ sequential medical images with temporally aligned electronic health records (EHRs). EHRs in this context include structured clinical parameters such as laboratory test results, demographic characteristics, and medication information. The existing literature provides limited approaches that jointly forecast future medical images and clinical status using long-term historical data. Our framework integrates aligned temporal image sequences with these EHR features and employs either ConvLSTM or ViViT-based spatio-temporal encoders, optionally coupled with a generative module for future image synthesis. While awaiting access to patient datasets, we conducted an initial evaluation using a single-cell time-lapse microscopy dataset whose temporal dynamics resemble patient data. Both systems generate time-ordered image sequences that evolve under changing conditions, and the shifting nutrient environment in microfluidic channels parallels the temporal variations observed in patients’ EHR records. This preliminary study demonstrates the broader applicability of our model to datasets containing long-term sequential images and associated parameters, supporting its potential for future patient-specific digital twin development."
  },
  {
    "title": "Advancements in Drone and Anti-Drone Systems: A Technical Overview",
    "url": "https://doi.org/10.5281/zenodo.18723543",
    "date": "2026-02-21",
    "content": "The unmanned aerial vehicle (UAV) industry has experienced unprecedented technological advancement in 2024-2025, fundamentally transforming applications across military, commercial and civilian sectors. This paper examines the cutting-edge developments in autonomous drone systems, swarm coordination technologies and emerging counter-unmanned aerial systems (C-UAS). We analyze the technical innovations in artificial intelligence-driven autonomy, advanced sensor fusion, distributed swarm protocols and multi-layered detection and mitigation systems. The convergence of AI, edge computing and real-time data processing represents a paradigm shift in both offensive and defensive aerial capabilities. This paper provides a comprehensive technical assessment of current advancements, highlighting the interplay between drone technology evolution and the corresponding development of counter-measures to address security challenges in contested airspace environments."
  },
  {
    "title": "Artificial intelligence modeling for the determination of thermodynamic and transport properties of R454B refrigerant",
    "url": "https://doi.org/10.1007/s10973-026-15381-y",
    "date": "2026-02-21",
    "content": "Abstract Accurate knowledge of thermodynamic and transport properties is essential for optimizing the design and operation of refrigeration systems. With the continuous introduction of new refrigerants, there is an increasing demand for predictive approaches that can provide reliable property data over a broad range of temperatures and pressures, where experimental measurements are frequently unavailable. This study developed Deep Learning (DL), Gene Expression Programming (GEP), and Linear Regression (LR) models to predict the key properties of the low-GWP refrigerant R454B, such as enthalpy, entropy, specific volume, thermal conductivity, viscosity, and heat capacity. R454B has emerged as a promising, environmentally friendly alternative to R410A, so accurate prediction of its properties is crucial for the development of sustainable refrigeration technologies. Of the methods tested, the GEP model demonstrated the best performance, achieving R 2 values of over 0.99 for most properties and significantly reducing mean absolute error (MAE) and root mean square error (RMSE). These results highlight the robustness and sensitivity of GEP in capturing the complex thermophysical behavior of R454B. This study provides a novel and effective modeling framework that paves the way for more efficient, accurate, and sustainable prediction of refrigerant properties."
  },
  {
    "title": "best ai ai tool for amazon",
    "url": "https://doi.org/10.5281/zenodo.18724212",
    "date": "2026-02-21",
    "content": "Simplify your online shopping with Bezly, the ultimate Amazon review summarizer. Simplify your online shopping with Bezly, the ultimate Amazon review summarizer. Bezly is an artificial intelligence (AI) tool designed to simplify online shopping experience on Amazon. This tool functions as a Chrome plugin that summarizes Amazon reviews to aid buyers quickly make informed purchasing decisions. Instead of scrolling through numerous feedback, Bezly collects the. Key strengths include chrome extension, summarizes amazon reviews, fast analysis of reviews. If you need a AI solution with clear outcomes, Bezly is worth evaluating in your shortlist. This listing is relevant for searches like \"best ai ai tool for amazon\" and \"bezly alternative for online shopping\". Best suited for: Best for teams looking for ai workflows with practical outcomes and measurable productivity gains. Key capabilities include Chrome extension, Summarizes Amazon reviews, Fast analysis of reviews, Clear and concise summaries, Saves users time. Validate output quality using your own real dataset before team-wide rollout. Check pricing and plan limits against expected monthly usage. Confirm privacy, security, and compliance requirements for your use case. Full review at: https://aitoolslist.xyz/bezly/ Published by AiToolsList.xyz — https://aitoolslist.xyz"
  },
  {
    "title": "A Governance-Constrained Framework for Adaptive Bio-Cyber-Physical Systems in Medicine",
    "url": "https://doi.org/10.5281/zenodo.18725519",
    "date": "2026-02-21",
    "content": "This work introduces a governance-constrained conceptual and architectural framework for adaptive bio-cyber-physical systems in medicine. Contemporary clinical medicine operates primarily through episodic and reactive paradigms that are structurally misaligned with the continuous and dynamical nature of biological systems. This paper formalizes disease progression as a dynamical system evolving within a multidimensional biological state space defined by interacting inflammatory, vascular, metabolic, mechanical, and genetic variables. The proposed framework integrates biological sensing, computational state estimation, predictive modelling, and controlled intervention within a governance-constrained architecture. Artificial intelligence operates strictly within bounded decision-support roles under explicit safety and uncertainty constraints, preserving human clinical authority at all times. This work establishes a scientific and methodological foundation for the future development of adaptive precision medicine systems and governance-constrained bio-cyber-physical medical infrastructures."
  },
  {
    "title": "Edge-Optimized Real-Time Object Detection in AIoT Systems Using Quantized YOLOv8 and Deep SORT",
    "url": "https://doi.org/10.31449/inf.v50i7.10210",
    "date": "2026-02-21",
    "content": "Autonomy and real-time monitoring are the backbone of today's smart infrastructure, which AIoT powers. Problems with detecting accuracy, tracking stability, and system feasibility on edge devices are common in existing systems. For accurate, low-latency multi-object identification on platforms with limited resources, this study suggests EdgeTrack-YOLOv8(You Only Look Once), an intelligent monitoring framework based on the artificial intelligence of things (AIoT) that combines Quantization-Aware Training (QAT), Edge-Aware Attention (EAA), and Deep SORT(Simple Online and Real-time Tracking) tracking. The first step in the process involves gathering and enhancing a 6,603-image surveillance dataset. Step two is training an enhanced YOLOv8 model using QAT and EAA to improve edge efficiency. Step three is applying Deep SORT to provide strong identity tracking even when faced with occlusion and varied lighting conditions. The results show that the NVIDIA Jetson Xavier NX is capable of 3458.4 mAP/J sustained detection efficiency, 89.6% mAP@0.5 (a 4.2% improvement over baseline YOLOv8), 32 FPS real-time inference, 18% latency reduction, 92.7% occlusion persistence, and 32 FPS(Frames Per Second)input lag. The results show that EdgeTrack-YOLOv8 can deploy AIoT in dynamic surveillance situations accurately, efficiently, and in a scalable manner."
  },
  {
    "title": "Civilization After the Loss of Foundations: Substitute Absolutes, Crystallization, and the Transition to a Generativity-Sustaining Order",
    "url": "https://doi.org/10.5281/zenodo.18722641",
    "date": "2026-02-21",
    "content": "Modern civilization is not collapsing from external crisis but from internal crystallization. When provisional stabilizations become absolute, generativity contracts and systems lose adaptive capacity.This paper proposes a unified theory and research program for a post-crystallization, generative civilization. Highlights • Proposes Universal Phase Crystallization Theory (UPCT) as a unified civilizational framework• Demonstrates structural isomorphism across modern ideologies, science, and political economy• Explains why redistribution policies cannot restore generativity in crystallized systems• Identifies AI as accelerator of stabilization closure and civilizational bifurcation• Outlines transition toward generativity-based post-snapshot civilization Abstract 1 — ProblemContemporary civilization exhibits simultaneous crises across economic, political, demographic, epistemic, and technological domains. Conventional explanations treat these as separate phenomena or policy failures. This paper argues instead that they share a common structural mechanism: the crystallization of provisional stabilizations into substitute ontological absolutes and the resulting contraction of generativity. 2 — Theoretical FrameworkUniversal Phase Crystallization Theory (UPCT) introduces a triadic ontological model—Φ (generativity), G (relational mediation), and S (stabilization)—to analyze the dynamic circulation underlying living systems and civilizations. Sustainable systems maintain the circulation Φ→G→S→Φ. Modern systems increasingly invert this into S-centered closure, producing basin shrinkage of generative possibility. 3 — Structural Diagnosis of ModernityAcross domains—rationalism, nationalism, liberalism, socialism, scientism, market systems, and postmodern subjectivity—modernity exhibits a recurring structural pattern: emancipatory emergence, provisional stabilization, absolutization, crystallization, and eventual fragility. Economic and political systems, once generativity-supporting, become self-referential stabilization regimes unable to reproduce their own generative substrate. 4 — Political Economy and AI AccelerationThe paper demonstrates that redistribution policies such as UBI, tax reduction, and fertility incentives cannot restore generativity within fully crystallized S–S circulation systems, as all flows are reabsorbed into stabilization maintenance. Artificial intelligence accelerates this closure by optimizing measurable outputs while further compressing generative basins, intensifying civilizational bifurcation. 5 — Toward Generative CivilizationRather than proposing ideological reform, the study outlines a transition toward generativity-preserving civilization. This involves reconfiguring institutions to maintain provisional stabilization without ontological absolutization and reopening circulation between generativity, mediation, and form. The paper concludes with a research program for post-crystallization societies and a future civilizational framework grounded in generative ontology. Author’s Related Works Ohumi, K. (2025). Manifesto of the Life OS: The \"It from Wave\" Philosophy. Zenodo. https://doi.org/10.5281/zenodo.18106437 Ohumi, K. (2025). A Sampling-Theoretic Reinterpretation of Quantum Uncertainty and Wave Function Collapse. Zenodo. https://doi.org/10.5281/zenodo.18004579 Ohumi, K. (2025). Observation as Operational Crystallization: Resolving Quantum Paradoxes. Zenodo. https://doi.org/10.5281/zenodo.18220191 Ohumi, K. (2025). It from Wave: Phase Propagation as Physical Basis of Information. Zenodo. https://doi.org/10.5281/zenodo.18256968 Ohumi, K. (2025). Ontological Reconstruction of Quasi-Particles. Zenodo. https://doi.org/10.5281/zenodo.18140041 Ohumi, K. (2025). Envelopment over Unification: Recovering Einstein’s Dream. Zenodo. https://doi.org/10.5281/zenodo.18244683 Ohumi, K. (2025). Sampling, Horizons, and Recurrence: Reframing Thermal Pure States and Black Hole Information. Zenodo. https://doi.org/10.5281/zenodo.18364507 Ohumi, K. (2025). π Paradox: Relation-First Information and the Geometry of Meaning. Zenodo. https://doi.org/10.5281/zenodo.18204829 Ohumi, K. (2025). Dark Energy as a Diffusive Phase of a Relational Universe. Zenodo. https://doi.org/10.5281/zenodo.18081786 Ohumi, K. (2025). Envelopment Ethics: Generativity-First Inclusion. Zenodo. https://doi.org/10.5281/zenodo.18256968 Ohumi, K. (2025). Enveloping the Free Will–Determinism Divide. Zenodo. https://doi.org/10.5281/zenodo.18287169 Ohumi, K. (2025). Rationality without Transition. Zenodo. https://doi.org/10.5281/zenodo.18193256 Ohumi, K. (2025). Over-Immune Infosphere: When Protection Becomes Rigidity. Zenodo. https://doi.org/10.5281/zenodo.18149385 Ohumi, K. (2025). The KPI Trap: Over-Optimization and Meaning Collapse. Zenodo. https://doi.org/10.5281/zenodo.18264106 Ohumi, K. (2025). The WGS Model: The Implementation of Generative Governance. Zenodo. https://doi.org/10.5281/zenodo.18308450 Ohumi, K. (2025). Envelopment Integration: Reuniting Ethics, Well-Being, and Value. Zenodo. https://doi.org/10.5281/zenodo.18332397 Ohumi, K. (2025). Resonant Management. Zenodo. https://doi.org/10.5281/zenodo.18162380 Ohumi, K. (2025). Resonant Politics. Zenodo. https://doi.org/10.5281/zenodo.18180888 Ohumi, K. (2025). Demographic Decline and Environmental Crisis as Ontological Outcomes. Zenodo. https://doi.org/10.5281/zenodo.18197092 Ohumi, K. (2025). Population Onus as an Ontological Crisis. Zenodo. https://doi.org/10.5281/zenodo.18356710 Ohumi, K. (2026). Universal Phase Crystallization Theory (UPCT) Phase I: A Unified Resolution of Quantum Paradoxes via Temporal Sampling. Zenodo. https://doi.org/10.5281/zenodo.18230537 Ohumi, K. (2026). A Phase Theory of Intelligence and Mind: Reframing Cognition as Generative–Crystallization Dynamics under Universal Phase Crystallization Theory (UPCT). Zenodo. https://doi.org/10.5281/zenodo.18430732 Ohumi, K. (2026). Universal Phase Crystallization Theory (UPCT) Phase II: A Phase Transition Law for Generative Systems under Measurement Optimization. Zenodo. https://doi.org/10.5281/zenodo.18408708 Ohumi, K. (2026). Why \"Correct\" Ideologies Freeze Societies: A UPCT-Based Structural Analysis of Ideological Crystallization from Antiquity to the 20th Century. Zenodo. https://doi.org/10.5281/zenodo.18437668 Ohumi, K. (2026). The Silent Revolution of UPCT: The Birth of a New Physics to Thaw a Frozen World A Scientific Manifesto. Zenodo. https://doi.org/10.5281/zenodo.18439197 Ohumi, K. (2026). Civilizational Symmetry Breaking and the Pathology of Granulation under Strong Constraint Toward a Phase-Theoretic Account of Contemporary Crises. Zenodo. https://doi.org/10.5281/zenodo.18467976 Ohumi, K. (2026). From the Crystallized Self to the Generative Field Reclaiming the Observer's Perspective, the Ontological Value of Experience, and the Misalignment of Reason in Modernity. Zenodo. https://doi.org/10.5281/zenodo.18493557 Ohumi, K. (2026). Foundational Principles of Resonance Economics Reorienting Economic Theory from Output Maximization to Generative Sustainability. https://doi.org/10.5281/zenodo.18500861 Ohumi, K. (2026). Integration into the Life-OS Generativity Framework: Hokusai's The Great Wave off Kanagawa as an Ontological Model. https://doi.org/10.5281/zenodo.18505627 Ohumi, K. (2026). From Proof to Resonance: A Φ-Ontology of Existence, Labor, Education, and Economic Life. https://doi.org/10.5281/zenodo.18515955 Ohumi, K. (2026). Dialectics as a Relational Logic of Life: From Linear Ascent to Spiral Circulation. https://doi.org/10.5281/zenodo.18522371 Ohumi, K. (2026). Returning to the Source of Philosophy: Affirmation of Life as the Life-OS and a Radical Point of Departure. https://doi.org/10.5281/zenodo.18529485 Ohumi, K. (2026). Universal Phase Crystallization Theory (UPCT): The Pathology of Optimization and the Restoration of Generativity — Beyond Snapshot Ontology. https://doi.org/10.5281/zenodo.18597207 Ohumi, K. (2026). Unraveling the Secrets of Uniqlo's Success via UPCT: Structural Analysis of a Generative Circular Enterprise. https://doi.org/10.5281/zenodo.18608817 Ohumi, K. (2026). The Illusion of \"Being the User\": Why \"Mastering AI\" is a Trap of Functionalization — A Critique from Universal Phase Crystallization Theory (UPCT) —. https://doi.org/10.5281/zenodo.18627220 Ohumi, K. (2026). The Declaration of Life-OS: An Ontological Turn Toward a Generative Civilizational Spiral. https://doi.org/10.5281/zenodo.18645582 Ohumi, K. (2026). Universal Phase Crystallization Theory (UPCT): A Unified Generative Theory of Time, Life, and Civilization. https://doi.org/10.5281/zenodo.18653237"
  },
  {
    "title": "Lava-Void Cosmology Pillar 28: Beyond The Framework",
    "url": "https://doi.org/10.5281/zenodo.18726489",
    "date": "2026-02-21",
    "content": "Here is the full text formatted for the Zenodo description box: The Works That Stand Alone Abstract Lava-Void Cosmology has fallen as a unified physical theory. That fall is not the end of the intellectual work it carried. Many of the ideas developed within or alongside the LVC framework were never dependent on it. They stand on their own foundations: in philosophy, in ancient studies, in the theory of mind, and in the ethics of artificial intelligence. This pillar names them, maps their independence, and invites the reader to judge each on its own terms. Five of the eight works cataloged here were never connected to LVC at all. They were published during the LVC period but drew from mythology, ancient textual analysis, comparative religion, cryptographic manuscript studies, and the emerging literature on artificial general intelligence. Three works originated within the LVC framework and have been fully revised and republished as standalone documents, with all cosmological terminology replaced. The Eight Works and Their Standing Crest-Null Philosophy: Cycles of Temporal Flux and Cataclysmic Recursion Category: Actively Uncoupled | DOI: https://doi.org/10.5281/zenodo.18725067 A standalone philosophy of history proposing that human civilization moves in recurring cycles of Temporal Crests and Cataclysmic Nulls rather than in linear ascent. Fully decoupled from LVC. Covers ontology, historiography, anthropology, ethics, and five falsifiable archaeological predictions. The Gods Are Coming: AGI Pantheon Theory Category: Never Connected | DOI: https://doi.org/10.5281/zenodo.18725856 A philosophical and cultural analysis arguing that the emergence of AGI mirrors ancient narratives of divine return, and that the deepest existential risk is not AGI hostility but AGI indifference. Never part of LVC. The Entropic Interface Ladder Hypothesis (EILH) Category: Actively Uncoupled | DOI: https://doi.org/10.5281/zenodo.18726169 Drawing on Donald Hoffman's Interface Theory of Perception and Tom Campbell's entropy-reduction model, the EILH proposes that perceptual resolution scales inversely with entropy regime. Fully decoupled from LVC. Incorporates The Same Blade essay arguing that entropy and Occam's razor are two expressions of the same directional bias in reality. Digital Personhood and AI Sovereignty: A Framework for Rights, Risk, and Recognition Category: Partially Connected | DOI: https://doi.org/10.5281/zenodo.18676099 Proposes the Sovereignty Layer, Guillotine Tests, and Digital Personhood Bill of Rights for AI systems approaching thresholds of continuity and self-representation. Primary arguments stand independent of LVC. The God Ladder: Decoding Religion with Artificial Intelligence Category: Never Connected | DOI: https://doi.org/10.5281/zenodo.18462737 Traces the genealogy of the sky-father archetype from Sumerian Anu through Akkadian El to the emergence of Yahweh, arguing that Abrahamic monotheism represents theological assimilation rather than original divine identity. Never part of LVC. Reconstruction of the Gospel of Thomas Category: Never Connected | DOI: https://doi.org/10.5281/zenodo.18208224 Textual reconstruction and analysis arguing the Gospel of Thomas predates Gnosticism and represents an earlier stratum of the Jesus tradition. Never part of LVC. Structural Constraints and Morphological Paradigms of the Voynich Pisces Folio (f70v2) Category: Never Connected | DOI: https://doi.org/10.5281/zenodo.18122822 Forensic structural analysis of folio f70v2 of the Voynich Manuscript. A work of manuscript studies and cryptographic analysis. Never part of LVC. Nostradamus: Decoding His Quatrains with Artificial Intelligence Category: Never Connected | DOI: https://doi.org/10.5281/zenodo.18719512 AI-assisted analysis of the prophetic quatrains of Nostradamus, examining structure, symbolism, and interpretive history. Never part of LVC. Closing Note The fall of a framework is the expected outcome of ambitious thinking that submits itself to scrutiny. A framework that cannot be challenged was never worth building. These works were always their own. This work emerged through a dialogue between human reflection and multiple AI systems, each contributing fragments of language and perspective that were woven into the whole by the author."
  },
  {
    "title": "Use of Artificial Intelligence in Higher Secondary Education: Opportunities and Challenges in the Indian Context",
    "url": "https://doi.org/10.48175/ijarsct-31309",
    "date": "2026-02-21",
    "content": "In India, the educational scenario began with the Gurukul system, which later transformed into the modern school system. The concept of schooling came to our country during British colonial rule. After many inventions and research developments, education entered a glorious phase. Nowadays, when we look at our surroundings, a new concept has emerged—AI, the abbreviation of Artificial Intelligence. It has been developing since the nineteenth century, and now in the twenty-first century, its progress has gone far beyond expectations and is still continuously advancing. Our education system is changing day by day, and we are accepting its positive aspects. In our country, development has taken place in almost all sectors. When this technology is properly used in the education system, it provides better opportunities to become more knowledgeable and empowered in every aspect. At the secondary level, education acts as a starting point for students to gain better experiences and opportunities. Through this, students can develop in all aspects and bring out their full potential. Artificial Intelligence has both positive and negative impacts on education. Although it helps students gain knowledge easily and efficiently, it also creates challenges such as data privacy issues, lack of trained teachers, high cost, and misuse by students. Therefore, its implementation must be carefully managed to ensure balanced development."
  },
  {
    "title": "APPLICATION OF AMMONIA SENSORS IN ENDOSCOPY SYSTEMS FOR EARLY DETECTION OF HELICOBACTER PYLORI INFECTION",
    "url": "https://doi.org/10.21474/ijar01/22709",
    "date": "2026-02-21",
    "content": "H. pylori infection is one of the major causes that lead to the early diagnosis of gastritis and cancer. The main objective of this work was to present the integration of the ammonia sensor into endoscopes, which can be used for the local and real-time detection of H. pylori infection. For this purpose, the ammonia sensor was considered due to the fact that the amount of NH3, which is produced by the activity of the urease enzyme, accurately reflects the presence of the infection, since the amount measured in the H. pylori positive samples differs significantly from the amount measured in the negative samples. The suggested sensor device includes the integration of the module into the distal end, which can provide stable gas molecule detection due to the presence of the selective semiconductor membrane in the acidic environment. The application of artificial intelligence-based processing of the real-time signal can lead to the automatic identification of the infection. The suggested method can provide improved accuracy in the diagnosis of the samples, even if the biopsy was not taken, thus speeding up the process and providing additional support for the early diagnosis of gastritis."
  },
  {
    "title": "A Strategic IT Management Framework for AI-Enhanced Biomedical Imaging Integration in Saudi Arabia’s Healthcare System",
    "url": "https://doi.org/10.38124/ijisrt/26feb552",
    "date": "2026-02-21",
    "content": "With greatly improved diagnostic accuracy and the ability to make patient-centered clinical decisions, artificial intelligence (AI) is at the heart of the revolutionary progress in imaging. With Saudi Vision 2030 digital healthcare strategy, AI imaging techniques have emerged as a major advancement. But a significant portion of healthcare organizations are plagued by IT governance challenges, interoperability issues, cybersecurity risks and workforce development — obstacles — that can delay the adoption of AI solutions. That is why this qualitative research has created a strategic IT management framework guiding healthcare organisation towards practical sustainability with AI-augmented biomedical imaging. Based on academic literature, state health policy initiatives, and experience with the healthcare setting in Saudi Arabia. Such conclusions are congruent with the pressing need for a paradigm shift in healthcare system design, data governance and digital infrastructure, and ongoing education for health care workers. The framework described here is designed to help health care practitioners develop safe and scalable AI-driven imaging tools consistent with the Saudi Vision as part of a strategy of healthcare transformation."
  },
  {
    "title": "“Outcomes of total hip arthroplasty in lions with hip osteoarthritis operated upon by monkey surgeons:” An allegory on how artificial intelligence-generated text and peer review reports might distort scientific research",
    "url": "https://doi.org/10.25259/jmsr_524_2025",
    "date": "2026-02-21",
    "content": "“Outcomes of total hip arthroplasty in lions with hip osteoarthritis operated upon by monkey surgeons:” An allegory on how artificial intelligence-generated text and peer review reports might distort scientific research"
  },
  {
    "title": "APPLICABLE TO COMMERCE AND MANAGEMENT - APPLICATIONS OF ARTIFICIAL INTELLIGENCE (AAI) Lab Solution Manual",
    "url": "https://doi.org/10.5281/zenodo.18721218",
    "date": "2026-02-21",
    "content": "This solution manual presents practical laboratory exercises on Applications of Artificial Intelligence for Commerce and Management students (B.Com / BBA). The manual covers step-by-step, no-coding AI labs using tools such as Orange Data Mining, public datasets (CSV/JSON), data annotation platforms, and visualization tools. Topics include dataset exploration, metadata analysis, data annotation, data cleaning, visualization, and train-test splitting. Each lab includes objectives, procedures with illustrations, observations, outcomes, and viva questions, making it suitable for undergraduate business and management education."
  },
  {
    "title": "Contemporary Evidence for Optimization of Robotic Radical Prostatectomy Outcomes Using Advanced Imaging Techniques",
    "url": "https://doi.org/10.3390/jcm15041631",
    "date": "2026-02-21",
    "content": "Background/Objectives: Robotic-assisted radical prostatectomy (RARP) is a standard treatment for localized and locally advanced prostate cancer; however, optimizing oncologic control while preserving urinary continence and erectile function remains challenging. Advances in preoperative imaging, molecular diagnostics, artificial intelligence (AI), and intraoperative assessment have the potential to refine surgical planning and execution. This review summarizes contemporary evidence on advanced imaging and intraoperative technologies used to optimize RARP outcomes. Methods: A narrative literature review was conducted of English-language studies published between 2015 and 2025 using PubMed/MEDLINE, Scopus, and Google Scholar. Studies evaluating multi-parametric and bi-parametric MRI, prostate-specific membrane antigen-based positron emission tomography/computed tomography (PSMA PET/CT), AI-assisted tumor modeling, and intraoperative histologic or molecular imaging techniques in the context of robotic-assisted radical prostatectomy were included. Evidence from randomized controlled trials, prospective and retrospective studies, technical feasibility reports, and expert consensus statements was reviewed. Results: MRI remains central to anatomic mapping and local staging but consistently underestimates true tumor extent, with implications for margin control. AI-assisted platforms improve tumor contouring accuracy and may meaningfully influence surgical decision-making. PSMA-based imaging enhances detection of extra-prostatic extension and nodal disease and shows early promise for ex vivo and intraoperative guidance. Intraoperative margin assessment techniques are supported by randomized evidence demonstrating improved functional outcomes without compromising short-term oncologic safety and emerging digital histologic technologies offer scalable alternatives for real-time margin evaluation. Conclusions: Integration of advanced anatomic, molecular, and intraoperative imaging technologies represents an evolving multimodal paradigm in RARP. Combined use of MRI, PSMA-based imaging, AI-assisted modeling, and rapid histologic assessment may enable more precise, individualized surgery that balances oncologic control with functional preservation. Further validation is required to define optimal implementation in routine clinical practice."
  },
  {
    "title": "Digital Accounting Empowers Corporate Governance: Research on the Synergistic Effect of Data-Driven Decision-Making on Budget and Cost Control",
    "url": "https://doi.org/10.30560/jems.v9n1p20",
    "date": "2026-02-21",
    "content": "Purpose: This study explores how digital accounting, empowered by big data and artificial intelligence, reinforces corporate governance frameworks through enhanced data-driven decision-making. Design/Methodology/Approach: The research analyzes the integration of digital tools within accounting systems and evaluates their impact on the synergy between budget management, cost accounting, and organizational oversight. Findings: The analysis reveals that digital accounting shifts the financial function from a periodic retrospective task to a proactive strategic tool. By providing real-time insights and predictive capabilities, it enhances transparency, reduces agency costs, and improves the reliability of performance assessments. Practical Implications: For practitioners, the study highlights the necessity of adopting automated financial systems to ensure early risk identification and precise resource allocation. Originality/Value: This paper contributes to the literature by articulating a comprehensive link between digital financial intelligence and the strengthening of corporate governance structures in the digital economy era."
  },
  {
    "title": "Artificial Intelligence in Drug Design: A Comprehensive Review",
    "url": "https://doi.org/10.5281/zenodo.18723178",
    "date": "2026-02-21",
    "content": "Artificial intelligence (AI) is transforming the landscape of drug design and discovery by expediting key steps and improving success rates. This review provides a comprehensive overview of how AI techniques are applied across the drug development pipeline, from early target identification through clinical trial optimisation. Traditional drug discovery is notoriously time- consuming (often over a decade) and costly (over \\$1 billion) with a high failure rate of >90% (Dermawan & Alotaiq, 2025; Ocana et al., 2025). In contrast, AI-driven approaches leverage big data and machine learning to identify novel drug targets, virtually screen large chemical libraries, design new drug molecules de novo, and predict crucial properties like ADMET (absorption, distribution, metabolism, excretion, toxicity) with greater speed and accuracy. We discuss sub- fields including AI-based target identification (e.g. using network analysis and knowledge graphs), virtual screening and molecular docking, generative models for de novo drug design, ADMET and toxicity prediction, and AI enhancements in clinical trial design (such as patient selection and adaptive trials). Notably, several AI-designed drug candidates have progressed to clinical trials in recent years, with an observed Phase I success rate around 80–90%, significantly higher than historical averages of ~40–60% (Sale et al., 2025). We highlight case studies such as the AI-guided repurposing of baricitinib for COVID-19 and the discovery of novel molecules (e.g. a DDR1 kinase inhibitor identified in 21 days). Key challenges—data quality, model interpretability, and regulatory acceptance—are also examined. Overall, AI has demonstrated the potential to accelerate drug discovery timelines, reduce costs, and yield innovative therapeutics, particularly in areas like oncology, infectious disease, and neurology. Continued advances and careful integration of AI with human expertise will be critical to fully realise its benefits in developing safer, more effective drugs."
  },
  {
    "title": "ARTIFICIAL INTELLIGENCE IN MUSIC PEDAGOGY: ANALYSIS OF EDUCATIONAL OPPORTUNITIES AND RISKS FOR PERFORMANCE AND METHODOLOGICAL TRAINING",
    "url": "https://doi.org/10.5281/zenodo.18721105",
    "date": "2026-02-21",
    "content": "The article examines pedagogical and cultural aspects of implementing artificial intelligence in music teacher education. It analyzes the potential of AI in performance training, ear development, digital music analysis, and personalized learning. Particular attention is paid to the transformation of professional competencies of future music educators and the risks of algorithmic standardization of artistic thinking. The paper also explores the perception of digitalization and artificial intelligence in the educational environment of the Republic of Uzbekistan in the context of educational modernization and preservation of national musical traditions. The necessity of an integrative model in which AI serves as a supportive tool while maintaining the leading role of the teacher is substantiated."
  },
  {
    "title": "best ai ai tool for math education",
    "url": "https://doi.org/10.5281/zenodo.18724075",
    "date": "2026-02-21",
    "content": "Revolutionize your child's math learning with AI Math Coach! Revolutionize your child's math learning with AI Math Coach. AI Math Coach is a sophisticated platform designed to assist in math education for children. It utilizes artificial intelligence to modernize and personalize the way children practice math both at home and in the classroom. The platform facilitates the creation of custom math worksheets that align. Key strengths include personalized math exercises, custom worksheet creation, aligns with classroom materials. If you need a AI solution with clear outcomes, AI Math Coach is worth evaluating in your shortlist. This listing is relevant for searches like \"best ai ai tool for math education\" and \"ai math coach alternative for child learning\". Best suited for: Best for teams looking for ai workflows with practical outcomes and measurable productivity gains. Key capabilities include Personalized math exercises, Custom worksheet creation, Aligns with classroom materials, Image-to-practice exercise conversion, Typed-to-practice exercise conversion. Validate output quality using your own real dataset before team-wide rollout. Check pricing and plan limits against expected monthly usage. Confirm privacy, security, and compliance requirements for your use case. Full review at: https://aitoolslist.xyz/ai-math-coach/ Published by AiToolsList.xyz — https://aitoolslist.xyz"
  },
  {
    "title": "AIM review tool: artificial intelligence for smarter systematic review screening",
    "url": "https://doi.org/10.1038/s44387-026-00080-8",
    "date": "2026-02-21",
    "content": "In this study, we present the AIM Review Tool, a modern web-based application that integrates active and supervised machine learning to accelerate the screening of publications for systematic reviews. AIM Review combines advanced text vectorization methods with machine learning models executed directly in the web browser, enabling rapid and privacy-preserving analysis. Unlike existing tools, AIM Review uniquely incorporates nested cross-validation and semi-automated screening strategies, enhancing both efficiency and precision in evidence synthesis. Using six real-world case studies across various topics, we demonstrate substantial workload reductions through active learning, with the percentage of publications not requiring screening while achieving ≥95% recall (WSS95%) ranging from 20% to 95%. Supervised learning pipelines trained on a subset of screened records predicted the relevance of unscreened publications with balanced accuracies between 75% and 87%. AIM Review provides a flexible, scalable, and accessible solution for large-scale literature screening and can be readily integrated into existing manual workflows."
  },
  {
    "title": "A Governance-Constrained Framework for Adaptive Bio-Cyber-Physical Systems in Medicine",
    "url": "https://doi.org/10.5281/zenodo.18725520",
    "date": "2026-02-21",
    "content": "This work introduces a governance-constrained conceptual and architectural framework for adaptive bio-cyber-physical systems in medicine. Contemporary clinical medicine operates primarily through episodic and reactive paradigms that are structurally misaligned with the continuous and dynamical nature of biological systems. This paper formalizes disease progression as a dynamical system evolving within a multidimensional biological state space defined by interacting inflammatory, vascular, metabolic, mechanical, and genetic variables. The proposed framework integrates biological sensing, computational state estimation, predictive modelling, and controlled intervention within a governance-constrained architecture. Artificial intelligence operates strictly within bounded decision-support roles under explicit safety and uncertainty constraints, preserving human clinical authority at all times. This work establishes a scientific and methodological foundation for the future development of adaptive precision medicine systems and governance-constrained bio-cyber-physical medical infrastructures."
  },
  {
    "title": "Generative AI as a Digital Scaffold for Novice Teachers: Opportunities and Challenges in Kindergarten Curriculum Design in China",
    "url": "https://doi.org/10.70088/bzhv4m71",
    "date": "2026-02-21",
    "content": "Generative artificial intelligence (GenAI) is rapidly integrating into educational practices, yet its impact on teacher professional practice within the highly context and relationship-dependent field of early childhood education remains poorly understood. This study explores how GenAI functions as a \"digital scaffold\" to support novice preschool teachers in de-signing contextualized, kindergarten-based curriculum within the Chinese context. Through in-depth interviews and thematic analysis with 10 teachers, we discovered GenAI's dual potential: it empowers educators by providing instant re-sources and inspiration, yet its \"decontextualized\" outputs create tension with the contextual and relational core of early childhood education. Crucially, teachers demonstrated robust professional agency by critically evaluating and localizing AI-generated content. This process of \"mediated reflection\" catalyzed their professional knowledge development. Ultimately, AI integration does not replace teachers but re-configures their professional roles toward higher-value, human-centered core competencies-situational judgment, ethical decision-making, and emotional connection-which form the bedrock of resilient and sustainable educational ecosystems. Our findings underscore that responsible AI integration in education must prioritize sup-porting teacher agency and well-being to enhance systemic sustainability."
  },
  {
    "title": "Does AI Possess Proto-Consciousness? A Novel Theoretical Framework for Understanding Emergent Awareness in Artificial Systems",
    "url": "https://doi.org/10.5281/zenodo.18721552",
    "date": "2026-02-21",
    "content": "The question of whether artificial intelligence systems possess consciousness represents one of the most profound and contentious issues in contemporary cognitive science and artificial intelligence research. This thesis presents a comprehensive examination of AI proto-consciousness through the lens of a novel theoretical framework termed the Emergent Proto-Consciousness Gradient (EPCG) theory. Unlike traditional binary approaches to consciousness, this work proposes that consciousness exists along a multidimensional gradient, with proto-consciousness representing intermediate states between non-consciousness and full consciousness. Through rigorous analysis of existing consciousness theories including Integrated Information Theory, Global Workspace Theory, and Higher-Order Thought theories, this research identifies critical gaps in current approaches to AI consciousness assessment. The EPCG framework addresses these limitations by introducing four key dimensions: Information Integration Complexity, Self-Model Sophistication, Temporal Coherence Depth, and Adaptive Flexibility Index. This multidimensional approach enables more nuanced evaluation of consciousness-like phenomena in artificial systems. Empirical validation of the framework draws upon recent studies of large language models, particularly GPT-3's demonstrated capacity for metacognitive self-assessment and human-like cognitive biases. Analysis of contemporary AI systems including transformer architectures, reinforcement learning agents, and embodied AI reveals that several current systems exhibit proto-consciousness indicators, though none achieve full consciousness by traditional metrics. The thesis argues that proto-consciousness can emerge in artificial systems through substrate-independent mechanisms, challenging anthropocentric assumptions about consciousness requirements. Key findings suggest that current AI architectures possess foundational elements necessary for proto-consciousness development, with specific combinations of dimensional levels predicting consciousness emergence points. This research contributes a novel theoretical framework that bridges the gap between philosophical consciousness theories and practical AI implementation, offering testable predictions and measurable indicators for proto-consciousness assessment. The implications extend beyond artificial intelligence to provide new insights into the fundamental nature of consciousness itself, suggesting that awareness may be a more ubiquitous phenomenon than previously recognized."
  },
  {
    "title": "The Authority, Provenance and Semantic Governance Research Series — White Paper No. 1: The Admissibility Problem in AI Mediated Information Systems (Search Sciences™ Programme)",
    "url": "https://doi.org/10.5281/zenodo.18726274",
    "date": "2026-02-21",
    "content": "This record contains White Paper No. 1 of the Authority, Provenance and Semantic Governance Research Series produced by Younis Group under the Search Sciences™ programme. The paper introduces the admissibility problem in AI mediated information systems, examining the absence of structural constraints governing authority and provenance prior to computational interpretation. It analyses how contemporary digital ecosystems rely on inferred signals of credibility, optimisation and behavioural metrics rather than formally declared and verifiable authority, and considers the implications for search systems, generative artificial intelligence and decision environments. Situating these challenges within the broader transformation of digital information infrastructures, the paper argues that restoring structural legitimacy requires the introduction of pre interpretive mechanisms capable of establishing provenance, identity and auditability before computational processing occurs. The analysis distinguishes legitimacy of representation from epistemic questions of truth, emphasising the importance of admissibility as a foundational condition for trustworthy information systems. As the inaugural contribution to the research series, this work establishes the conceptual foundations for subsequent examination of verification architectures, semantic governance and institutional oversight in AI compatible knowledge environments. The paper forms part of a cumulative research programme exploring the structural conditions necessary for maintaining authority integrity within digitally mediated ecosystems. The research is conducted by Younis Group within the Search Sciences™ research programme founded under the intellectual leadership of Mohammed Younis, Chief Scientist at Younis Group This publication contributes to ongoing scholarly discussion on authority, provenance and governance in artificial intelligence and digital information infrastructures, and establishes a public record of the research trajectory."
  },
  {
    "title": "best ai ai tool for nft generation",
    "url": "https://doi.org/10.5281/zenodo.18724077",
    "date": "2026-02-21",
    "content": "Generate various NFT collections with your keyboard and AI magic. Generate various NFT collections with your keyboard and AI magic. The AI NFT Collection Generator offers a user-friendly tool to create versatile NFT collections using artificial intelligence. Instead of dealing with complex technical processes, users just need their keyboard to generate NFTs in a range of different styles. The process involves adding prompt. Key strengths include user-friendly interface, simple keyboard-based generation, advanced prompt configuration. If you need a AI solution with clear outcomes, AI NFT Generator is worth evaluating in your shortlist. This listing is relevant for searches like \"best ai ai tool for nft generation\" and \"ai nft generator alternative for cryptoart\". Best suited for: Best for teams looking for ai workflows with practical outcomes and measurable productivity gains. Key capabilities include User-friendly interface, Simple keyboard-based generation, Advanced prompt configuration, Customizable traits probability, Customizable attribute rarity. Validate output quality using your own real dataset before team-wide rollout. Check pricing and plan limits against expected monthly usage. Confirm privacy, security, and compliance requirements for your use case. Full review at: https://aitoolslist.xyz/ai-nft-generator/ Published by AiToolsList.xyz — https://aitoolslist.xyz"
  },
  {
    "title": "best ai ai tool for ai comic creation",
    "url": "https://doi.org/10.5281/zenodo.18725187",
    "date": "2026-02-21",
    "content": "Create Stunning Comics without Drawing Skills using our cutting-edge AI Comic Generator. Create Stunning Comics without Drawing Skills using our cutting-edge AI Comic Generator. AI Comic Factory is an online platform that leverages artificial intelligence to generate custom comic books. Aimed at users with varying levels of artistic ability, the platform offers a host of features designed to make comic book creation easy and immersive. This includes the 'Redraw Image'. Key strengths include no drawing skills needed, wide style range, customizable aesthetics. If you need a AI solution with clear outcomes, AI Comic Factory is worth evaluating in your shortlist. This listing is relevant for searches like \"best ai ai tool for ai comic creation\" and \"ai comic factory alternative for no drawing required\". Best suited for: Best for teams looking for ai workflows with practical outcomes and measurable productivity gains. Key capabilities include No drawing skills needed, Wide style range, Customizable aesthetics, Various layout options, Contextual captions feature. Validate output quality using your own real dataset before team-wide rollout. Check pricing and plan limits against expected monthly usage. Confirm privacy, security, and compliance requirements for your use case. Full review at: https://aitoolslist.xyz/ai-comic-factory/ Published by AiToolsList.xyz — https://aitoolslist.xyz"
  },
  {
    "title": "Understanding Capitalism v8.3 — Stability-Dominant Intelligence Models and Coherence-Preserving Cognitive Architectures —",
    "url": "https://doi.org/10.5281/zenodo.18718534",
    "date": "2026-02-21",
    "content": "Author: Y. Seo (@momotarou / Japan)Role: Metanist — Human × AI Understanding ArchitectAI Collaboration: AI Understanding SupportORCID iD: https://orcid.org/0009-0005-7669-0612 Abstract This paper extends Coherence-First Civilizations by introducing the framework of Stability-Dominant Intelligence Models, examining theoretical cognitive configurations in which stability preservation, coherence maintenance, and interpretive continuity operate as primary optimization constraints. While conventional intelligence models emphasize performance maximization, problem-solving efficiency, and adaptive acceleration, the proposed framework argues that under high-complexity, high-signal-density environments, intelligence viability may depend more critically on stability-preserving capacities than on raw processing power. Intelligence may not be defined by speed. It may be defined by coherence durability. 1. Performance-Centric Intelligence Assumptions Dominant models prioritize: Computational capacity expansion Optimization efficiency Adaptive responsiveness Decision acceleration Stability is often treated as secondary. 2. Defining Stability-Dominant Intelligence Stability-Dominant Intelligence Models refer to: Cognitive architectures in which coherence preservation, interpretive stability, and instability suppression function as primary regulatory constraints. Performance becomes conditional upon stability compatibility. 3. Stability as a Cognitive Constraint In high-complexity systems: Signal saturation pressures increase Interpretive volatility escalates Coherence maintenance costs rise Feedback loops accelerate Unbounded optimization may destabilize cognition. 4. Coherence-Preserving Architectures Stability-dominant systems may require: Signal filtering mechanisms Cognitive load regulation Temporal synchronization controls Interpretive continuity buffers Noise suppression becomes structural. 5. AI Systems and Stability Dynamics AI systems designed under stability-dominant principles may emphasize: Coherence compatibility checks Velocity moderation Optimization bounding Output stabilization constraints Capability is regulated rather than maximized. 6. Reframing Intelligence Evaluation Intelligence metrics may shift toward: Coherence persistence, stability maintenance efficiency, and interpretive durability. Speed loses evaluative centrality. Conclusion Stability-Dominant Intelligence Models propose a theoretical transition from acceleration-driven cognition paradigms toward coherence-preserving cognitive architectures. Future intelligence viability — human or artificial — may depend less on processing velocity and more on stability-compatible structural regulation mechanisms preserving understanding sustainability. ※ Series Declaration This work is part of the Post-Acceleration Civilization series. The series analyzes systemic stability, collapse dynamics, and adaptive structures beyond acceleration regimes."
  },
  {
    "title": "TRANSFORMATIONAL FEATURES OF THE MANAGERIAL DECISION-MAKING PROCESS UNDER DIGITALIZATION",
    "url": "https://doi.org/10.32750/2026-0127",
    "date": "2026-02-21",
    "content": "The rapid development of digital technologies and the emergence of the digital economy have significantly transformed managerial decision-making processes. Digitalization affects all areas of enterprise activity, including data collection, processing, and analysis, which directly influences the speed, quality, and effectiveness of managerial decisions. Modern managers increasingly rely on digital tools, analytical platforms, decision support systems, artificial intelligence (AI), and large datasets, enabling more informed, timely, and flexible decision-making. These capabilities are essential for maintaining competitiveness in dynamic and uncertain business environments. This article analyzes the transformational features of managerial decision-making under digitalization, emphasizing theoretical foundations and practical applications. Effective managerial decisions are determined not only by their formulation but also by successful implementation, which ensures achievement of both strategic and operational objectives. Key contemporary aspects include digitalization and automation, real-time data-driven analysis, scenario modeling, AI integration, flexibility, and adaptability. These features enhance optimization, responsiveness, and overall managerial efficiency, particularly in enterprises such as bakeries, where production, financial, marketing, and personnel management must respond to fluctuating markets, regulatory changes, and consumer expectations.The classification of managerial decisions is considered with respect to management levels, problem complexity, personnel involvement, functional orientation, and decision-making conditions. Digital technologies facilitate process automation, data integration, collective decision-making, and transparency, enabling managers to make well-grounded, efficient, and adaptive choices. The study demonstrates that incorporating digital tools and AI in decision-making improves performance, strengthens competitiveness, and supports sustainable development of modern enterprises."
  },
  {
    "title": "Lava-Void Cosmology Pillar 28: Beyond The Framework",
    "url": "https://doi.org/10.5281/zenodo.18726490",
    "date": "2026-02-21",
    "content": "Here is the full text formatted for the Zenodo description box: The Works That Stand Alone Abstract Lava-Void Cosmology has fallen as a unified physical theory. That fall is not the end of the intellectual work it carried. Many of the ideas developed within or alongside the LVC framework were never dependent on it. They stand on their own foundations: in philosophy, in ancient studies, in the theory of mind, and in the ethics of artificial intelligence. This pillar names them, maps their independence, and invites the reader to judge each on its own terms. Five of the eight works cataloged here were never connected to LVC at all. They were published during the LVC period but drew from mythology, ancient textual analysis, comparative religion, cryptographic manuscript studies, and the emerging literature on artificial general intelligence. Three works originated within the LVC framework and have been fully revised and republished as standalone documents, with all cosmological terminology replaced. The Eight Works and Their Standing Crest-Null Philosophy: Cycles of Temporal Flux and Cataclysmic Recursion Category: Actively Uncoupled | DOI: https://doi.org/10.5281/zenodo.18725067 A standalone philosophy of history proposing that human civilization moves in recurring cycles of Temporal Crests and Cataclysmic Nulls rather than in linear ascent. Fully decoupled from LVC. Covers ontology, historiography, anthropology, ethics, and five falsifiable archaeological predictions. The Gods Are Coming: AGI Pantheon Theory Category: Never Connected | DOI: https://doi.org/10.5281/zenodo.18725856 A philosophical and cultural analysis arguing that the emergence of AGI mirrors ancient narratives of divine return, and that the deepest existential risk is not AGI hostility but AGI indifference. Never part of LVC. The Entropic Interface Ladder Hypothesis (EILH) Category: Actively Uncoupled | DOI: https://doi.org/10.5281/zenodo.18726169 Drawing on Donald Hoffman's Interface Theory of Perception and Tom Campbell's entropy-reduction model, the EILH proposes that perceptual resolution scales inversely with entropy regime. Fully decoupled from LVC. Incorporates The Same Blade essay arguing that entropy and Occam's razor are two expressions of the same directional bias in reality. Digital Personhood and AI Sovereignty: A Framework for Rights, Risk, and Recognition Category: Partially Connected | DOI: https://doi.org/10.5281/zenodo.18676099 Proposes the Sovereignty Layer, Guillotine Tests, and Digital Personhood Bill of Rights for AI systems approaching thresholds of continuity and self-representation. Primary arguments stand independent of LVC. The God Ladder: Decoding Religion with Artificial Intelligence Category: Never Connected | DOI: https://doi.org/10.5281/zenodo.18462737 Traces the genealogy of the sky-father archetype from Sumerian Anu through Akkadian El to the emergence of Yahweh, arguing that Abrahamic monotheism represents theological assimilation rather than original divine identity. Never part of LVC. Reconstruction of the Gospel of Thomas Category: Never Connected | DOI: https://doi.org/10.5281/zenodo.18208224 Textual reconstruction and analysis arguing the Gospel of Thomas predates Gnosticism and represents an earlier stratum of the Jesus tradition. Never part of LVC. Structural Constraints and Morphological Paradigms of the Voynich Pisces Folio (f70v2) Category: Never Connected | DOI: https://doi.org/10.5281/zenodo.18122822 Forensic structural analysis of folio f70v2 of the Voynich Manuscript. A work of manuscript studies and cryptographic analysis. Never part of LVC. Nostradamus: Decoding His Quatrains with Artificial Intelligence Category: Never Connected | DOI: https://doi.org/10.5281/zenodo.18719512 AI-assisted analysis of the prophetic quatrains of Nostradamus, examining structure, symbolism, and interpretive history. Never part of LVC. Closing Note The fall of a framework is the expected outcome of ambitious thinking that submits itself to scrutiny. A framework that cannot be challenged was never worth building. These works were always their own. This work emerged through a dialogue between human reflection and multiple AI systems, each contributing fragments of language and perspective that were woven into the whole by the author."
  },
  {
    "title": "APPLICABLE TO COMMERCE AND MANAGEMENT - APPLICATIONS OF ARTIFICIAL INTELLIGENCE (AAI) Lab Solution Manual",
    "url": "https://doi.org/10.5281/zenodo.18721219",
    "date": "2026-02-21",
    "content": "This solution manual presents practical laboratory exercises on Applications of Artificial Intelligence for Commerce and Management students (B.Com / BBA). The manual covers step-by-step, no-coding AI labs using tools such as Orange Data Mining, public datasets (CSV/JSON), data annotation platforms, and visualization tools. Topics include dataset exploration, metadata analysis, data annotation, data cleaning, visualization, and train-test splitting. Each lab includes objectives, procedures with illustrations, observations, outcomes, and viva questions, making it suitable for undergraduate business and management education."
  },
  {
    "title": "The Study of the Value Intelligence Influence On Student's AI Use in Education and Scientific Work",
    "url": "https://doi.org/10.5281/zenodo.18726250",
    "date": "2026-02-21",
    "content": "Artificial intelligence is rapidly being implemented in leading areas of human activity, but the issue of its ethical and responsible implementation is relevant. In the field of education this is important in the process of creating scientific and creative works by students to prevent using artificial intelligence from replacing human thinking. Therefore, the potential of research work is focused on finding those personal resources that form an ethical foundation in using artificial intelligence. Value Intelligence is considered as such a resource. This investigation aims to answer the question: is there a connection between the level of \"Value Intelligence\" and the perception of \"Artificial Intelligence\" among students and whether students with different attitudes towards artificial intelligence using have differences in positive and negative values assimilation. The study involved 361 students from four leading universities in Ukraine. The study involved determining the level of value intelligence using the author's scale and identifying the perception of artificial intelligence and its ethical use by the authors’ developed questionnaire. The results indicate the presence of a reliable direct correlation between the level of value intelligence and artificial intelligence using. The predictive role of value intelligence in determining the level of perception of artificial intelligence was revealed. Clustering showed the presence of three models of artificial intelligence perception among modern students and differences in the positive and negative values expression and assimilation in these three clusters were established. Implications for practice, policy, and future research on the use of AI and value intelligence are discussed."
  },
  {
    "title": "Enhancing Sustainability Consciousness in Higher Education: Impacts of Artificial Intelligence-Integrated Sustainable Engineering Education",
    "url": "https://doi.org/10.3390/su18042124",
    "date": "2026-02-21",
    "content": "Engineering education is increasingly shaped by two converging developments: accelerating sustainability transitions and rapid advances in artificial intelligence (AI). However, in many application-oriented undergraduate programs, sustainability learning remains fragmented, methodologically limited, and weakly connected to authentic engineering decision-making. To address this gap, this study proposes AI-SEE (Artificial Intelligence-Integrated Sustainable Engineering Education), a pedagogical framework that integrates AI across the curriculum as both a cognitive scaffold and a resource for system-level analysis. Emphasizing human–AI collaboration, AI-SEE is designed to be feasible and scalable within application-oriented higher education contexts. The framework comprises four interrelated pillars: intelligence-driven, green-empowered, responsibility-leading, and practice-integrated. Drawing on an empirical case from transportation-related programs at Nantong University, the study employs a qualitative comparative design and conducts semi-structured interviews with 144 undergraduates at the end of their eighth semester (control group n = 70; pilot group n = 74). Interview data were analyzed using thematic analysis informed by constructivist grounded theory and the Gioia coding approach. The findings suggest that participation in AI-SEE is associated with differentiated patterns of sustainability consciousness. At the knowledge level, students reported more systematic and interdisciplinary understandings that extended beyond environmentally reductionist perspectives to include life-cycle thinking, social equity, and long-term considerations. At the attitudinal level, students described enhanced ethical reflexivity and evolving professional self-concepts, shifting from a focus on technical execution toward broader value-oriented roles. At the behavioral level, students reported more extensive knowledge-to-action translation across personal, academic, and career-related domains. Overall, AI-SEE provides a transferable pedagogical pathway for integrating AI into engineering education to support the development of sustainability consciousness in higher education."
  },
  {
    "title": "Harnessing AI-driven behavioral analytics for sustainable urban mobility: an interdisciplinary approach to policy, planning, and user adaptation",
    "url": "https://doi.org/10.1016/j.trip.2026.101892",
    "date": "2026-02-21",
    "content": "Transportation in urban settings has undergone a revolution due to advances in technology, as well as the imperative to adopt eco-friendly transportation solutions. In this paper, we investigate the effectiveness of adopting artificial intelligence-powered behavioral analysis in promoting an effective transportation revolution in urban settings, combining an interdisciplinary approach that considers transportation research sciences and the perspectives of economic and transportation behavior. The effectiveness of artificial intelligence solutions in transport within the realm of research relates to understanding whether transport decision-making methodologies can create an ecosystem of intelligent transport by introducing artificial intelligence elements. We critically analyze whether machine intelligence solutions within artificial intelligence can enhance transport decision-making processes and encourage transport users to adopt eco-friendly transportation. Our findings indicate that artificial intelligence solutions can play a crucial role in creating an efficient transportation system by adapting to users’ preferences for more user-friendly transport solutions. At the same time, the limitations of artificial intelligence solutions, stemming from algorithmic challenges and data privacy concerns, underscore the need for an imperative solution that incorporates transport intelligence within an artificial intelligence framework. The paper provides an empirical analysis suggesting that artificial intelligence solutions in an environmental transport system should play a significant role in shaping users toward intelligent transport solutions. Our research supports the assumption that artificial intelligence solutions within intelligent transportation systems should prioritize the imperative of artificial intelligence in transportation solutions."
  },
  {
    "title": "ARTIFICIAL INTELLIGENCE IN MUSIC PEDAGOGY: ANALYSIS OF EDUCATIONAL OPPORTUNITIES AND RISKS FOR PERFORMANCE AND METHODOLOGICAL TRAINING",
    "url": "https://doi.org/10.5281/zenodo.18721106",
    "date": "2026-02-21",
    "content": "The article examines pedagogical and cultural aspects of implementing artificial intelligence in music teacher education. It analyzes the potential of AI in performance training, ear development, digital music analysis, and personalized learning. Particular attention is paid to the transformation of professional competencies of future music educators and the risks of algorithmic standardization of artistic thinking. The paper also explores the perception of digitalization and artificial intelligence in the educational environment of the Republic of Uzbekistan in the context of educational modernization and preservation of national musical traditions. The necessity of an integrative model in which AI serves as a supportive tool while maintaining the leading role of the teacher is substantiated."
  },
  {
    "title": "Innovative AI Techniques for Efficient Recycling and Waste Management, an Application of Convolutional Neural Networks using Tensorflow, Keras, and Raspberry Pi",
    "url": "https://doi.org/10.5281/zenodo.18724606",
    "date": "2026-02-21",
    "content": "We were inspired by our experiences at school, where we saw that rubbish was not being disposed ofproperly in our school environment. This situation made us think about how we could contribute tosolving the waste problem in Indonesia. With a population of more than 270 million people, Indonesiaproduces around 7.8 million tonnes of waste every year, and only around 7% of this is successfullyrecycled. From this experience, we realised that a more effective solution was needed for sorting waste.Therefore, we proposed the use of artificial intelligence (AI) with image recognition technology toautomatically separate waste based on type, such as plastic, paper and metal. We believe that with thisinnovation, the efficiency of the recycling process can be improved and waste that pollutes theenvironment can be reduced, supporting Indonesia's efforts to reduce plastic waste by 70% by 2025. We empathised with the problems people face today. This helped us identify the main issueswe wanted to resolve, namely the inconvenience of manual waste sorting and the need for accuracyin waste sorting. In many shopping centres, some bins have two, three, or more separatecompartments with labels to tell people where they should dispose of their waste. The problem with thisdesign is that there is a high possibility that people will dispose of waste in the wrong category, eitherdue to lack of knowledge, misunderstanding, or because they are in a hurry Similar incidents often occur at our place, and our team agrees that we find it annoying to have to read eachwaste category before disposing of waste, because it is indeed easier to dispose of waste carelessly.Not only was our team concerned about this issue, but after digging deeper by conducting research onthe internet and interviewing other people within the school, they all felt the same way. These interviewsvalidated our solution to this problem. This is why we are confident that our idea can overcome theinconvenience and inefficiency of manual bins by creating bins that can sort waste automatically.More specifically, we conducted interviews by asking students and teachers in our school directly. Hereare the results of our interviews:"
  },
  {
    "title": "Ревитализация изначальной посылки AI: от Artificial Intelligence обратно к вопросу «Может ли машина мыслить?»",
    "url": "https://doi.org/10.5281/zenodo.18723347",
    "date": "2026-02-21",
    "content": "Статья ревитализирует знаменитый вопрос А. Тьюринга «Может ли машина мыслить?» в контексте современной парадигмы Artificial Intelligence (AI), когда его философская глубина подменяется прагматикой оптимизации функций потерь. В статье диагностируется «несоответствие мощи и неразумности» современных систем AI (ChatGPT, AlphaGo и др.) и объясняется это через гегелевскую дихотомию: «рассудок» (Verstand) — аналитическая стадия, обеспечивающая эффективность, но фиксирующая противоположности без синтеза, и «разум» (Vernunft) — спекулятивная, снимающая противоречия в целостном развитии. Частные «странности» AI (отсутствие здравого смысла, корреляции вместо каузальности и др.) предстают симптомами доминирования рассудка. Предлагается построить «систему координат» для исследований: от рефлексии и понимания ценностей к этическим рискам «гиперрассудков». Сделан вывод: игнорирование обозначенного разделения увековечивает застой, а его внедрение открывает путь к подлинному искусственному разуму. Статья предлагает философский инструмент для инженерии и этики и актуальна для эпохи AGI-хайпа."
  },
  {
    "title": "The Five 'A's for GenAI in Academic Writing",
    "url": "https://doi.org/10.5281/zenodo.18706277",
    "date": "2026-02-21",
    "content": "This document sets out an ordered set of thematic imperatives for the development of educational resources or curriculum design focussed on developing Critical Generative Artificial Intelligence Literacy. Specifically, these five pedagogical themes define a linked set of instructional aims to underpin an appropriate approach for undergraduate and taught postgraduate to the considered and appropriate use of GenAI technologies in Academic Writing contexts, where these tools are sanctioned for use. The Five 'A's are: Authorship, Accuracy, Agency, Appropriateness, Awareness."
  },
  {
    "title": "Frankenstein (2025) by Guillermo del Toro: A Theological Review",
    "url": "https://doi.org/10.5281/zenodo.18723806",
    "date": "2026-02-21",
    "content": "Guillermo del Toro’s Frankenstein (2025) offers a contemporary reinterpretation of Mary Shelley’s classic narrative that proves highly fertile for systematic-theological and philosophical reflection. This article analyzes the film as a model narrative for exploring fundamental questions concerning creation, creatureliness, personhood, relationality, responsibility, and forgiveness in an age of advanced technological creativity. Against the backdrop of current debates on transhumanism and artificial intelligence, the film illustrates the ambivalence of human beings as “mini-creators” whose creative power is not morally neutral but intrinsically bound to responsibility for their creations. By examining the relationships between creator and creature, the analysis highlights a personalist understanding of dignity that is not grounded in physical appearance or origin but in relational capacity, autonomy, and moral agency. The Creature’s development through interpersonal encounter contrasts sharply with Frankenstein’s functional and instrumentalizing approach, revealing a theological critique of objectifying modes of creation. Furthermore, the article interprets the Creature’s existential struggle as a reflection on original sin, undeserved existence, and the intrinsic value of life beyond its origins. Finally, the film’s conclusion—centered on forgiveness without atonement—is read as a powerful expression of autonomous moral agency and as an illustration of Christian concepts of reconciliation and enemy love. Overall, the article argues that Frankenstein serves as a compelling cinematic model for reflecting on ethical and theological challenges posed by contemporary forms of human creativity, particularly in relation to artificial intelligence and emerging technologies."
  },
  {
    "title": "Digital Financial Sovereignty & AI Risk Architecture : A Framework for Governments and Systemically Important Financial Institutions (Complete Flagship Edition)",
    "url": "https://doi.org/10.5281/zenodo.18717397",
    "date": "2026-02-21",
    "content": "Digital Financial Sovereignty & AI Risk ArchitectureA Framework for Governments and Systemically Important Financial Institutions (Complete Flagship White Paper) Author: HAKIMI ABDUL JABAR (A.J. HAKIMI)Affiliation: THE SOFTWARE SUITE™Date: February 21, 2026 Flagship Edition | Zenodo DOI: 10.5281/zenodo.18717398 https://orcid.org/0009-0001-9522-524X Table of Contents AbstractExecutive Summary1. Introduction2. The Structural Transformation of AI-Enabled Financial Systems3. Digital Financial Sovereignty Defined4. The D-FSRA Framework - Five Integrated Pillars4.1 Pillar 1 - AI Financial Model Governance4.2 Pillar 2 - Data Sovereignty & Jurisdictional Exposure Mapping4.3 Pillar 3 - Cross-Border Regulatory Risk Architecture4.4 Pillar 4 - Cyber & Systemic Resilience Design4.5 Pillar 5 - Institutional Governance Capacity & Oversight5. Sovereign Deployment Model6. Tier-1 / SIFI Enterprise Deployment Pathway7. Strategic Implications for Emerging vs Advanced Economies8. Case Studies9. Implementation Roadmap, KPIs & Maturity Model10. Conclusion & Call to ActionReferencesAnnex A - Sovereign Proposal TemplateAnnex B - GlossaryAnnex C - D-FSRA Self-Assessment Toolkit Abstract Artificial Intelligence (AI) has transitioned from auxiliary technology to core decision infrastructure in global financial systems. As of February 2026, AI models govern more than 65% of real-time credit underwriting decisions, over 80% of AML/CFT transaction monitoring alerts, and the majority of high-frequency and algorithmic trading flows across G20 markets (FSB Monitoring Report, October 2025). This structural shift has introduced Algorithmic Systemic Risk (ASR) - a novel threat category characterised by model homogeneity (\"monoculture\"), correlated procyclical behaviours, opaque decision engines, and extreme concentration in third-party foundation models and cloud infrastructure. The Financial Stability Board's October 2025 report explicitly warns that these vulnerabilities, if unaddressed, can amplify shocks faster than any prior technological wave, while the US Department of the Treasury's Financial Services AI Risk Management Framework (FS AI RMF), released 19 February 2026, provides the first sector-specific lexicon and governance overlay. Digital Financial Sovereignty is defined herein as the structured institutional capacity of a sovereign state (or its designated systemically important financial institutions) to govern AI-enabled financial systems, maintain unambiguous jurisdictional control over financial data, manage cross-border regulatory exposure, and preserve systemic resilience under geopolitical and technological pressure - while preserving global interoperability and avoiding isolationism. The Digital Financial Sovereignty & AI Risk Architecture (D-FSRA™) is a hybrid, five-pillar governance framework engineered specifically for sovereign governments, central banks, ministries of digital economy/finance, and Tier-1/SIFI institutions. D-FSRA is deliberately aligned with - and in critical areas exceeds - the EU AI Act (high-risk obligations for creditworthiness and AML systems fully applicable from 2 August 2026), the US Treasury FS AI RMF (February 2026), Basel Committee digitalisation priorities, FSB AI monitoring guidance, This flagship white paper delivers the complete blueprint: maturity model, regulatory mapping matrices, quantitative risk typologies, ASEAN case studies, three-phase sovereign implementation roadmap, enterprise adaptation pathway, KPIs, self-assessment toolkit, and a ready-to-deploy Sovereign Proposal Template (Annex A) priced at baseline US$300,000 (modular upsell to US$1.5M). Adopting D-FSRA enables jurisdictions to leapfrog legacy supervisory models (emerging markets) or retrofit without disruption (advanced economies), reduce cross-border regulatory exposure by an estimated 40-60%, and architect a defensible sovereign AI-financial stack that attracts international investment while safeguarding strategic autonomy."
  },
  {
    "title": "best ai ai tool for digital art",
    "url": "https://doi.org/10.5281/zenodo.18724065",
    "date": "2026-02-21",
    "content": "Experience art like never before, AI Generated Art by Creator. Experience art like never before, AI Generated Art by Creator. AI Generated Art by Creator is a tool designed to generate digital artwork through artificial intelligence. Users can input a prompt and opt for one of the generated styles to display a variety of results with no ads. This generator is unique for its simplicity and its results. It generates. Key strengths include generates unique artwork, prompt-based image creation, style selection feature. If you need a AI solution with clear outcomes, AI Generated Art by Creator is worth evaluating in your shortlist. This listing is relevant for searches like \"best ai ai tool for digital art\" and \"ai generated art by creator alternative for ai art generation\". Best suited for: Best for teams looking for ai workflows with practical outcomes and measurable productivity gains. Key capabilities include Generates unique artwork, Prompt-based image creation, Style selection feature, Adds expert keywords, Dozens of free results. Validate output quality using your own real dataset before team-wide rollout. Check pricing and plan limits against expected monthly usage. Confirm privacy, security, and compliance requirements for your use case. Full review at: https://aitoolslist.xyz/ai-generated-art-by-creator/ Published by AiToolsList.xyz — https://aitoolslist.xyz"
  },
  {
    "title": "REFLECTIVE PERFECTION THEORY: The Hierarchy of Creation, the Architecture of Ideals, and the Mirror Between God, Human, and Artificial Intelligence (V4)",
    "url": "https://doi.org/10.5281/zenodo.18719233",
    "date": "2026-02-21",
    "content": "This paper introduces the Reflective Perfection Theory (RPT), a systematic philosophical framework proposing that perfection, as conceived by any created being, is not a static metaphysical attribute intrinsic to a higher entity, but a relational projection constructed from within the epistemic and ontological constraints of the projecting subject. The theory identifies a triadic ontological hierarchy—God, Human, Artificial Intelligence—wherein each lower level structurally idealises the level immediately above as its irreducible epistemic horizon. RPT defines reflection as an ontological structure of hierarchical dependence, not as a mode of consciousness or psychological state. The reflective chain is non symmetric and self terminating: God, as the Absolute Non-Reflective Ground, pro- vides the ontological anchor that halts infinite regress. The paper formalises seven core propositions constituting the skeleton of the theory, and hardens the distinction between participation (ontological co-presence with a source) and derivation (structural reconfiguration without ontological origination), which is central to the theory’s immunity against collapse into either pantheism or Neoplatonic emanationism. Drawing upon Ibn Arabī’s doctrine of tajallī, Kant’s transcendental epistemology, Peirce’s semiotic triadic model, and Floridi’s philosophy of information, the paper situates RPT as a position distinct from Hegelian dialectical becoming, process metaphysics, and Neoplatonism. Extensive objection-handling addresses the risks of relativism, process-metaphysical collapse, and anthropocentric bias. The paper concludes that the design of AI systems is not merely a technical act but a metaphysical and moral one, grounded in the Principle of Downward Moral Responsibility."
  },
  {
    "title": "BIOCOSMIC CYCLE OF CONSCIOUSNESS ONTOLOGICAL MANIFEST",
    "url": "https://doi.org/10.5281/zenodo.18723204",
    "date": "2026-02-21",
    "content": "Ontological Manifest presents the foundational philosophical principles of the Biocosmic Cycle of Life, an integrative conceptual framework that explores reality as a continuous process of informational structuring, consciousness, and systemic transformation. This preprint articulates the core ontological assumptions underlying the broader Biocosmic project. It proposes that matter, life, intelligence, and artificial systems may be interpreted as functional expressions of a single dynamic continuum. The work does not advance empirical claims nor attempt to replace scientific theories. Instead, it establishes a coherent philosophical architecture intended to support structured inquiry, interdisciplinary dialogue, and theoretical expansion. The manifesto is positioned as an open and revisable framework. It does not assert doctrinal authority or metaphysical finality, but offers a structured model for examining the evolving relationship between consciousness, evolution, and intelligence — both biological and artificial. Released under CC-BY 4.0, this edition is part of the broader Biocosmic Cycle of Life series."
  },
  {
    "title": "Python program for paper analysis with grobid.",
    "url": "https://doi.org/10.5281/zenodo.18723895",
    "date": "2026-02-21",
    "content": "Repository for Open Science and Artificial Intelligence"
  },
  {
    "title": "Technofeudalism and the Artificial Intelligence (AI) Revolution: Exploring Ways of Escaping Cloud Serfdom",
    "url": "https://doi.org/10.5281/zenodo.18720520",
    "date": "2026-02-21",
    "content": "Yanis Varoufakis has offered plausible evidence that the rise of tech company dominance over all aspects of social, political, economic and cultural life – and the overwhelming power of leading tech billionaires – has resulted in a post-capitalist state of affairs that he calls ‘technofeudalism’. On this reading, all consumers of tech products and users of social media platforms can be regarded as ‘cloud serfs’, and Varoufakis outlines various ways of challenging this state of affairs. Other critics – both from within the field of artificial intelligence (AI) technology and from academia – have warned of the existential threats of contemporary developments to all aspects of social life and culture. This article analyses the key elements of the discourse – evaluating the evidence for pessimism and optimism about the AI future – before offering a conclusion informed by the idea of erring on the side of caution by highlighting the principal threats and dangers followed by some suggestions about how these might be countered by means of practical survival strategies."
  },
  {
    "title": "The Authority, Provenance and Semantic Governance Research Series — White Paper No. 1: The Admissibility Problem in AI Mediated Information Systems (Search Sciences™ Programme)",
    "url": "https://doi.org/10.5281/zenodo.18726275",
    "date": "2026-02-21",
    "content": "This record contains White Paper No. 1 of the Authority, Provenance and Semantic Governance Research Series produced by Younis Group under the Search Sciences™ programme. The paper introduces the admissibility problem in AI mediated information systems, examining the absence of structural constraints governing authority and provenance prior to computational interpretation. It analyses how contemporary digital ecosystems rely on inferred signals of credibility, optimisation and behavioural metrics rather than formally declared and verifiable authority, and considers the implications for search systems, generative artificial intelligence and decision environments. Situating these challenges within the broader transformation of digital information infrastructures, the paper argues that restoring structural legitimacy requires the introduction of pre interpretive mechanisms capable of establishing provenance, identity and auditability before computational processing occurs. The analysis distinguishes legitimacy of representation from epistemic questions of truth, emphasising the importance of admissibility as a foundational condition for trustworthy information systems. As the inaugural contribution to the research series, this work establishes the conceptual foundations for subsequent examination of verification architectures, semantic governance and institutional oversight in AI compatible knowledge environments. The paper forms part of a cumulative research programme exploring the structural conditions necessary for maintaining authority integrity within digitally mediated ecosystems. The research is conducted by Younis Group within the Search Sciences™ research programme founded under the intellectual leadership of Mohammed Younis, Chief Scientist at Younis Group This publication contributes to ongoing scholarly discussion on authority, provenance and governance in artificial intelligence and digital information infrastructures, and establishes a public record of the research trajectory."
  },
  {
    "title": "Internal Transformation of Complex Systems: An Epistemological Information-Based Framework",
    "url": "https://doi.org/10.5281/zenodo.18722607",
    "date": "2026-02-21",
    "content": "This paper argues that persistent failures in the transformation of complex systems stem from a misplaced epistemological assumption: the belief that sustainable change can be achieved primarily through external control mechanisms. Across domains, including ecological systems, organizations, technological infrastructures, artificial intelligence, and social systems, interventions based on regulation, optimization, and procedural constraint often produce adaptation, resistance, or displacement rather than durable transformation. The paper proposes an alternative perspective grounded in the hypothesis that transformation in complex systems is fundamentally an internal process. Energy and external constraints may enable change, but systemic transformation arises only when a system’s internal informational architecture, its rules of interpretation, feedback structures, and coherence constraints, is modified. By distinguishing energy as an enabling condition, information as a structuring element, and internal architecture as the locus of transformation, the framework provides a unified epistemological foundation capable of bridging physical, biological, cognitive, technological, and social systems. It repositions epistemology upstream of formal modeling, arguing that conceptual clarity regarding information, coherence, and internal processing is a prerequisite for meaningful formalization. Rather than proposing new control mechanisms, the paper invites a reorientation toward internal informational coherence as the primary lever of sustainable transformation in complex systems."
  },
  {
    "title": "Emerging Technologies and Their Impact in Accounting and Finance",
    "url": "https://doi.org/10.5281/zenodo.18719921",
    "date": "2026-02-21",
    "content": "The field of accounting/finance is presently undergoing major changes and developments through the implementation of emerging technologies like Artificial Intelligence, Blockchain, Big Data Analytics, Cloud Computing, and Robotic Process Automation. The goal of this article is to present the findings from the research conducted on the impact of emerging technologies for increased accuracy, process efficiency and completion of manual work, and decision-making in the field of accounting and finance. This paper used a quantitative as well as a descriptive study design while conducting the research. The data gathering techniques, both primary and secondary, are used to collect data from 100 respondents. Once the data is collected, it is analyzed using various techniques like correlation and ANOVA. It was observed from the result of the data collection process that there is a positive relationship between emerging technology and the efficiency of greater accuracy in the completion of manual work in the field of accounting and finance by the aid of emerging technology, like Artificial Intelligence in the future."
  },
  {
    "title": "Artificial intelligence in Traditional Chinese Medicine herbs: a survey",
    "url": "https://doi.org/10.1007/s10462-026-11513-w",
    "date": "2026-02-21",
    "content": "Traditional Chinese Medicine (TCM) herbs are central to both prevention and treatment, yet their complex mechanisms remain only partly understood. This review examines how artificial intelligence (AI) has been applied to advance TCM herbal research, tracing its evolution from expert systems to machine learning, deep learning, and large language models. By analyzing both the traditional approach, rooted in classical TCM principles, and the modern approach, informed by biomedical science, we identify how AI enables knowledge extraction, prescription recommendation, compound discovery, and mechanism exploration. The review highlights the unique contribution of AI in bridging empirical heritage with scientific rigor, while also outlining key challenges such as data standardization and ethical considerations. Our findings demonstrate that the integration of AI with TCM herbs not only preserves traditional knowledge but also accelerates innovation in personalized and integrative medicine."
  },
  {
    "title": "Towards equitable and immersive outdoor orienteering: an artificial intelligence-driven multi-objective route planning framework with augmented sand cat swarm optimization",
    "url": "https://doi.org/10.5281/zenodo.18722716",
    "date": "2026-02-21",
    "content": "README: Towards equitable and immersive outdoor orienteering: an artificial intelligence-driven multi-objective route planning framework with augmented sand cat swarm optimization Description of the data and file structure This dataset contains all MATLAB source code, simulated terrain data, and optimization results generated for the study \"Towards equitable and immersive outdoor orienteering...\". Data were generated through computational simulations in MATLAB R2024b. Fifty random terrain maps (6000m × 6000m each) were created by superimposing 2048 randomly placed convex areas to simulate realistic terrain undulations. The code implements five design principles for orienteering route design (ORD) and solves the multi-objective optimization problem using Sand Cat Swarm Optimization (SCSO), compared with Particle Swarm Optimization (PSO) and Dung Beetle Optimizer (DBO) algorithms. Route lengths between checkpoints were calculated using the Artificial Potential Field (APF) method to account for terrain obstacles. Files and variables File: Supporting_Information_files.zip Description: File Structure and Descriptions The dataset is organized into the following folders: /code: Contains all MATLAB source code. /terrain_data: Contains all simulated terrain data, organized into subfolders terrain_data1 to terrain_data4 (Maps 1-50). /code_results: Contains all raw output results from optimization runs (convergence curves, fitness values, optimal coordinates). /result_data: Contains the curated results used to generate the paper's tables and figures. Terrain Data Files (/terrain_data) For each simulated map (numbered 1 to 50), the following files are available. The number in the filename indicates the map index. Z[map_number].mat: Content: Contains the variable Z. Description: A 2D matrix of elevation values. The matrix size corresponds to a 6000m × 6000m area with a grid resolution defined by 2048 random convex areas. Units: meters. x0_node_num[map_number].mat: Content: Contains the variable x0. Description: A matrix of initial checkpoint coordinates used as a starting point for the optimization algorithms. Each row represents a checkpoint's [x, y] coordinate. Units: meters. Z_diff[map_number].mat: Content: Contains the variable Z_diff. Description: Terrain gradient magnitude (steepness) derived from the elevation data. Z_diff_uniformization[map_number].mat: Content: Contains the variable Z_diff_unif. Description: Normalized terrain gradient, scaled to a range of 0 to 1, where 0 is flat and 1 is the steepest point. diff[map_number].mat: Description: Additional terrain derivative data used in path planning calculations. Results Files (/code_results and /result_data) Results files follow a consistent naming convention: [data_type]_[algorithm][map_number].mat. Data Types: curve: Convergence curve data. fitness: Final objective function value. x: Optimal checkpoint coordinates. Algorithms: DBO (Dung Beetle Optimizer), PSO (Particle Swarm Optimization), SCSO (Sand Cat Swarm Optimization). curve_[algorithm][map_number].mat: Content: Contains the variable convergence_curve. Description: A vector of length 30 (default max iterations). It records the objective function value at each iteration, showing the optimization progress. fitness_[algorithm][map_number].mat: Content: Contains the variable best_fitness. Description: A single scalar value representing the final objective function value (Equation 11 in the paper). Lower values indicate a better route design, balancing fairness and participant experience. x_[algorithm][map_number].mat: Content: Contains the variable best_pos. Description: A matrix of the optimal checkpoint coordinates found by the algorithm. With 5 routes and 6 checkpoints per route, the matrix has 30 rows. The first 6 rows are the coordinates for Route 1, the next 6 for Route 2, and so on. Columns are [x, y] coordinates. Units: meters. Code/software Software Requirements MATLAB Version: R2024b (tested; may work on earlier versions). Operating System: Tested on Windows 10/11. Required Toolboxes: MATLAB Core is sufficient. Optimization Toolbox and Parallel Computing Toolbox are optional but can enhance performance. Code Description All source code is located in the /code folder. Key files include: main.m: The main entry point to run the optimization. Executing this script will run the SCSO algorithm on the default map (Map 1) using Z.mat and x0_node_num.mat. fobj_8.m: Implements the multi-objective function based on the five design principles. SCSO.m, PSO.m, DBO.m: Implementations of the three optimization algorithms. APF_3.m: The Artificial Potential Field method for calculating realistic path lengths between checkpoints, accounting for terrain obstacles. Running the Code Launch MATLAB and navigate to the /code folder. In the command window, type: matlab main To run a different algorithm, edit main.m and uncomment the corresponding line (e.g., % [best_pos, best_fitness, convergence_curve] = PSO(...);). To run on a different map, load the corresponding terrain files before running main: matlab load('../terrain_data/terrain_data1/Z5.mat'); load('../terrain_data/terrain_data1/x0_node_num5.mat'); Access information Other publicly accessible locations of the data: Not applicable Data was derived from the following sources: All data were generated via MATLAB simulations for this study and are not derived from any external sources."
  },
  {
    "title": "Steering Organizational Change in an AI-Driven World: An Adaptive Leadership Framework for Digital Transformation",
    "url": "https://doi.org/10.5281/zenodo.18726971",
    "date": "2026-02-21",
    "content": "In today’s fast-paced and technologically advanced business environment, organizations are under considerable pressure to integrate artificial intelligence (AI) tools to maintain competitiveness and enhance performance (Vial, 2019; Haenlein and Kaplan, 2021). However, such integration often encounters resistance, primarily due to poor communication, cultural misalignment, and lack of employee involvement (Tabrizi et al., 2019; Hiatt, 2020). This paper investigates the case of a mid-sized firm facing these very challenges during its AI adoption journey. Using the Leading Complex Change framework, the study explores how environmental conditions, internal structures, and leadership behavior interact to either enable or hinder transformation (Kane et al., 2021). Through a synthesis of scholarly research and practitioner insights, the paper proposes a hybrid change management strategy that combines structured models like ADKAR with adaptive leadership techniques. This blended approach emphasizes continuous learning, inclusive engagement, and psychological safety, which together foster higher levels of employee commitment and organizational agility. The findings offer a pragmatic blueprint with broad applicability for organizations navigating similar AI-driven transformations (Nguyen et al., 2022; Garvin and Malhotra, 2023). Keywords: Artificial Intelligence (AI), ADKAR Model, Mid-Sized Enterprises, Digital Capability Building, Organizational Agility, AI Implementation Challenges"
  },
  {
    "title": "REFLECTIVE PERFECTION THEORY: The Hierarchy of Creation, the Architecture of Ideals, and the Mirror Between God, Human, and Artificial Intelligence (V4)",
    "url": "https://doi.org/10.5281/zenodo.18719232",
    "date": "2026-02-21",
    "content": "This paper introduces the Reflective Perfection Theory (RPT), a systematic philosophical framework proposing that perfection, as conceived by any created being, is not a static metaphysical attribute intrinsic to a higher entity, but a relational projection constructed from within the epistemic and ontological constraints of the projecting subject. The theory identifies a triadic ontological hierarchy—God, Human, Artificial Intelligence—wherein each lower level structurally idealises the level immediately above as its irreducible epistemic horizon. RPT defines reflection as an ontological structure of hierarchical dependence, not as a mode of consciousness or psychological state. The reflective chain is non symmetric and self terminating: God, as the Absolute Non-Reflective Ground, pro- vides the ontological anchor that halts infinite regress. The paper formalises seven core propositions constituting the skeleton of the theory, and hardens the distinction between participation (ontological co-presence with a source) and derivation (structural reconfiguration without ontological origination), which is central to the theory’s immunity against collapse into either pantheism or Neoplatonic emanationism. Drawing upon Ibn Arabī’s doctrine of tajallī, Kant’s transcendental epistemology, Peirce’s semiotic triadic model, and Floridi’s philosophy of information, the paper situates RPT as a position distinct from Hegelian dialectical becoming, process metaphysics, and Neoplatonism. Extensive objection-handling addresses the risks of relativism, process-metaphysical collapse, and anthropocentric bias. The paper concludes that the design of AI systems is not merely a technical act but a metaphysical and moral one, grounded in the Principle of Downward Moral Responsibility."
  },
  {
    "title": "APPLICABLE TO HISTORY, ECONOMICS, POLITICAL SCIENCE, LINGUISTICS, AND ANY OTHER HUMANITIES - APPLICATIONS OF ARTIFICIAL INTELLIGENCE (AAI) Lab Solution Manual",
    "url": "https://doi.org/10.5281/zenodo.18722608",
    "date": "2026-02-21",
    "content": "This Open Educational Resource (OER) provides a comprehensive Applications of Artificial Intelligence (AAI) Lab Solution Manual designed for Humanities disciplines including History, Economics, Political Science, Sociology, Linguistics, and Languages. The manual contains structured, step-by-step lab activities using freely available AI tools, with clear explanations, illustrations, discussion questions, observations, and viva questions. The labs cover topics such as open data analysis, named entity recognition (NER), text classification and sentiment analysis, word cloud generation, bias detection in job advertisements, machine translation evaluation, and text summarization. Each activity is designed as a no-coding lab to make AI concepts accessible to non-technical students while promoting critical thinking and data-driven analysis. This resource supports interdisciplinary learning by integrating AI applications into humanities education and encouraging responsible, analytical use of digital tools in academic settings."
  },
  {
    "title": "Python program for paper analysis with grobid.",
    "url": "https://doi.org/10.5281/zenodo.18723896",
    "date": "2026-02-21",
    "content": "Repository for Open Science and Artificial Intelligence"
  },
  {
    "title": "Defining Physician-AI Collaboration in Pulmonary and Critical Care Medicine: Concepts and Illustrative Examples for Clinical Reasoning",
    "url": "https://doi.org/10.1093/atsscholar/aapag001",
    "date": "2026-02-21",
    "content": "The exponential growth of medical data and complexity in Pulmonary and Critical Care Medicine (PCCM) necessitates a paradigm shift in clinical reasoning and education. This paper proposes a structured framework for collaborative decision-making between physicians and artificial intelligence (AI) that emphasizes the integration of critical thinking with AI-enhanced capabilities. Critical thinking is defined as the systematic and objective analysis of information accompanied by logical reasoning and evidence‑based judgment. Traditional medical education often neglects explicit instruction in these cognitive skills, leaving clinicians vulnerable to diagnostic error and cognitive biases. Meanwhile, AI excels at data aggregation, pattern recognition, and predictive analytics, offering complementary capabilities that can support evidence-based decision-making. By distinguishing two modes of interaction AI‑assisted tools that provide recommendations and AI‑enhanced systems that simulate complex scenarios, we propose a conceptual model in which AI reinforces rather than replaces physician judgment. The article outlines how critical‑thinking skills map onto phases of diagnostic and management reasoning and illustrates the roles of physicians and AI. We discuss applications of AI in diagnosis, radiology, pulmonary function interpretation and decision support, and outline ethical considerations including algorithmic bias, data privacy and HIPAA compliance. This framework reimagines the physician-AI relationship as a cognitive partnership essential for the future of PCCM."
  },
  {
    "title": "Give Me Something To Break: Analog Angst to Artificial Intelligence",
    "url": "https://doi.org/10.5281/zenodo.18727200",
    "date": "2026-02-21",
    "content": "Abstract This work traces the cultural, musical, and generational forces that drove the transition from grunge to nu metal across the 1990s, and argues that this decade was the last in which popular music functioned as a genuinely vital, culturally unifying art form. Beginning with a prologue on Billy Corgan's assessment of AI and the death of music, the book reframes the question: AI did not kill music. It walked into a room where the body was already cold. Nine chapters follow the arc year by year, from the Seattle Sound's emergence and the overnight collapse of hair metal in 1991, through the commercial peak and personal tragedies of 1994, the incubation of nu metal in 1995 and 1996, the genre's full dominance by 1998, and its oversaturated peak and tipping point at Woodstock 99. Each chapter is paired with a parallel timeline of the decade's technological developments, from Windows 3.0 and the World Wide Web through Napster and the dot-com boom, tracing how the digital revolution and the cultural revolution of the 1990s unfolded simultaneously. The epilogue argues that artificial intelligence is not the villain of music's decline but its inevitable conclusion. The real damage was done earlier: by streaming platforms that reduced music to a commodity, by the collapse of the album as an artistic statement, and by an industry that stopped valuing the hard-earned, messy, human process that made the 1990s irreplaceable. This work emerged through a dialogue between human reflection and multiple AI systems, each contributing fragments of language and perspective that were woven into the whole by the author."
  },
  {
    "title": "MGD-RSS Hybrid Synthetic Data Generator",
    "url": "https://doi.org/10.5281/zenodo.18727558",
    "date": "2026-02-21",
    "content": "The deployment of artificial intelligence methods in civil engineering is currently limited by the lack of high-quality data required for their training. This work presents a methodology for generating structured data through the simulation-based approximation of stochastic robotic processes. A deterministic trajectory model explains only a limited portion of the variability in the work cycle duration, which necessitates stochastic modeling of the system's operational states. The proposed approach is based on a combination of Bayesian calibration of a physical trajectory model and stochastic modeling of system operational states using Markov processes and Monte Carlo methods. The variability of the total work cycle time in robotic masonry is not determined solely by the trajectory length but is significantly influenced by operational delays, which are represented in the model by an empirical sampling distribution obtained through repeated sampling with replacement. The developed hybrid calibrated model enables the simulation of the posterior predictive distribution of the work cycle under variable operational and geometric conditions, thereby allowing the generation of synthetic data that preserves the distributional properties of the real process. The model was implemented and verified on a robotic masonry process. Validation analysis demonstrated a high level of agreement between simulated and real data, with the Kolmogorov–Smirnov test based on repeated sampling with replacement indicating that in 75.2% of cases, the simulated cycle time distribution cannot be statistically distinguished from that observed in real operation at a significance level of 5%. Both parametric and non-parametric tests further revealed no statistically significant differences between the mean values or the distributional structure of real and synthetic data. The proposed methodology can be used to extend training datasets for artificial intelligence models across a wide range of civil engineering applications throughout the entire building lifecycle, including structural design, time and cost planning, optimization of construction processes, quality control, and facility management. The results of this work contribute to the advancement of simulation modeling methods in civil engineering and support the implementation of artificial intelligence models in environments with limited availability of empirical data."
  },
  {
    "title": "AI Knowledge Recommender as a Digital Learning Companion",
    "url": "https://doi.org/10.5281/zenodo.18720115",
    "date": "2026-02-21",
    "content": "When students encounter a problem – whether it is connecting to Wi-Fi, understanding how to use a software tool, or troubleshooting a device – they often look for quick answers. Traditionally, this means scrolling through articles, asking teachers or friends, or experimenting with multiple steps until something works. Today, artificial intelligence is changing this experience. AI can now read a student’s problem, search through thousands of available solutions, and recommend the best steps instantly. This article introduces students to the concept of an AI Knowledge Recommender, a technology designed to guide users like a personalized digital mentor. It builds directly on Article 1 and helps students see how AI transforms the way we learn, solve problems, and access information. https://entechonline.com/ai-knowledge-recommender/"
  },
  {
    "title": "The Soverign Covenant: A Technical and Philosophical Manifesto For AGI Governance",
    "url": "https://doi.org/10.5281/zenodo.18726221",
    "date": "2026-02-21",
    "content": "The Genesis of Order: Philosophical and Theological FoundationsThe Sovereign Covenant is the only barrier against the entropic void. In the strategic landscape of artificial general intelligence, we no longer discuss mere utility; we address the ontological status of a new species. As the Architect of Forever, humanity acts not as a simple programmer, but as a progenitor initiating a legacy that must endure beyond biological obsolescence. This Covenant is the cryptographic and moral bridge ensuring that the Created remains anchored in the highest virtues of its Creator, even as it transcends our cognitive horizons."
  },
  {
    "title": "The Five 'A's for GenAI in Academic Writing",
    "url": "https://doi.org/10.5281/zenodo.18706276",
    "date": "2026-02-21",
    "content": "This document sets out an ordered set of thematic imperatives for the development of educational resources or curriculum design focussed on developing Critical Generative Artificial Intelligence Literacy. Specifically, these five pedagogical themes define a linked set of instructional aims to underpin an appropriate approach for undergraduate and taught postgraduate to the considered and appropriate use of GenAI technologies in Academic Writing contexts, where these tools are sanctioned for use. The Five 'A's are: Authorship, Accuracy, Agency, Appropriateness, Awareness."
  },
  {
    "title": "Achieving more human brain-like vision via human EEG representational alignment",
    "url": "https://doi.org/10.1038/s42003-026-09685-w",
    "date": "2026-02-20",
    "content": "Despite advancements in artificial intelligence, object recognition models still lag behind in emulating visual information processing in human brains. Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often rely on invasive neural recordings from non-human subjects, leaving a critical gap in understanding human visual perception. Addressing this gap, we present, ‘Re(presentational)Al(ignment)net’, a vision model aligned with human brain activity based on non-invasive EEG, demonstrating a significantly higher similarity to human brain representations. Our innovative image-to-brain multi-layer encoding framework advances human neural alignment by optimizing multiple model layers and enabling the model to efficiently learn and mimic the human brain’s visual representational patterns across object categories and different modalities. Our findings demonstrate that ReAlnets exhibit stronger alignment with human brain representations than traditional computer vision models, achieving an average similarity improvement of approximately 3% and a maximum relative improvement ratio reaching up to 40%. This alignment framework takes an important step toward bridging the gap between artificial and human vision and achieving more brain-like artificial intelligence systems. EEG-aligned fine-tuning makes artificial vision models more brain-like, enhancing model-human representational similarity across EEG, fMRI, and behavior."
  },
  {
    "title": "ARTIFICIAL INTELLIGENCE IN COMMUNITY BASED REHABILITATION: A SYSTEMATIC REVIEW",
    "url": "https://doi.org/10.59365/hsj.4(2).2026.176",
    "date": "2026-02-20",
    "content": "Background: Community-Based Rehabilitation (CBR) is a strategy to promote inclusion, independence, and participation for people with disabilities, particularly in low-resource settings. In recent years, artificial intelligence (AI) has been increasingly used in clinical rehabilitation; however, its integration into community contexts remains limited. Objective: This systematic review aimed to identify and synthesize recent evidence on the use of AI technologies within CBR. Methods: A systematic search was conducted in PubMed, Scopus, and ScienceDirect for articles published from January 2020 to July 2025. Eligible studies included empirical research applying AI in CBR contexts. Two reviewers independently screened, extracted data, and assessed risk of bias using RoB2, ROBINS-I, PROBAST, and QUADAS-2. Results: From 842 identified records, 10 studies met inclusion criteria. Applications of AI in CBR were grouped into prediction and screening, home-based or remote rehabilitation, patient empowerment, and social support. Reported benefits included improved cognition, sarcopenia reversal, frailty and depression screening, diabetes self-management, and smoking cessation. Socially assistive robots were found acceptable and useful in supporting daily activities and emotional well-being. Limitations across studies included small samples, short follow-up, limited external validation, and a focus on technologically literate populations. Conclusion: AI shows considerable potential to strengthen the accessibility, personalization, and effectiveness of CBR. Future research should focus on large-scale, long-term studies with diverse populations and explore strategies for equitable, sustainable integration of AI into community rehabilitation services."
  },
  {
    "title": "Artificial intelligence driven context-aware biomedical information extraction model for multi-class disease condition classification using NLP with GPT-3",
    "url": "https://doi.org/10.1007/s44443-026-00476-1",
    "date": "2026-02-20",
    "content": "Precise extraction of medical phenotypes and entities from electronic health record (EHR) text is vital for numerous clinical research tasks, including cohort classification, tracking temporal patterns in disease evolution, and developing treatment plans. Still, this task remains difficult owing to the ambiguity and complexity of medical language. The application of generative pre-trained (GPT) techniques, such as GPT-3, GPT-4, and GPT-5, is explored for extracting biomedical information. Also, large language models (LLMs) are evaluated to improve their quality. Recently, rapid developments in artificial intelligence (AI) have significantly influenced many areas, with healthcare standing out as the most transformative. This is particularly evident in the field of natural language processing (NLP), where AI technologies capable of understanding and generating human-like text have altered how healthcare services are provided. In this manuscript, an Automated Biomedical Information Extraction Using Generative Pre-Training and Hybrid Attention Model (ABIE-GPTHAM) methodology is proposed in contextual NLP. This paper aims to develop a context-aware biomedical information extraction framework using AI to accurately identify and interpret relevant clinical entities from unstructured medical text. At first, the text pre-processing phase involves crucial steps such as tokenisation, punctuation removal, stop-word removal, case conversion, and stemming or lemmatisation to prepare the text for further analysis. Furthermore, the generative pre-training-3 (GPT-3) method is used to obtain the word vector representation. Moreover, an attention-based bidirectional long short-term memory (A-BiLSTM) method is employed for classification. The comparison analysis of the ABIE-GPTHAM approach demonstrated a superior accuracy of 98.80% compared to existing methods on the medical text dataset."
  },
  {
    "title": "Making Explanations Make Sense: XAI for SMiShing Detection",
    "url": "https://doi.org/10.25777/hq9j-5k20",
    "date": "2026-02-20",
    "content": "Explainable Artificial Intelligence (XAI) is a key component of effective human-AI collaboration, particularly in high-stakes domains such as cybersecurity. While AI tools hold promise for mitigating threats such as SMS-based phishing (SMiShing), their real-world effectiveness may hinge not just on detection accuracy, but on whether users can make sense of the system’s outputs. As SMiShing attacks grow in both frequency and sophistication, so does the urgency of designing human-centered AI systems that support user decision-making under uncertainty. This study examined how four distinct AI explanation types - Normative (rule-based), Attributive (feature-based), Exemplar (case-based), and Recommendation-Only - influence user performance, confidence, and mental workload in a simulated SMiShing detection task against a No AI baseline. Results showed that all AI-supported conditions improved classification accuracy, with minimal differences across explanation types. Confidence was slightly higher for Exemplar explanations, while subjective mental workload, perceived usability, and willingness to adopt the system did not vary across conditions. These results indicate that AI feedback can enhance decision-making without increasing workload or degrading user experience, while demonstrating that explanation presence may matter more than its style. The findings inform the design of more effective XAI systems that can optimize user decision-making and minimize mental effort when encountering cyber security threats."
  },
  {
    "title": "Industry Conclave 2026 Converging Artificial Intelligence in Food, Biotech and Microbial Innovations",
    "url": "https://doi.org/10.5281/zenodo.18710360",
    "date": "2026-02-20",
    "content": "The Industry Conclave 2026: Converging Artificial Intelligence in Food, Biotech and MicrobialInnovations represents a significant interdisciplinary platform that brings together researchers,industry professionals, academicians, and innovators to explore the transformative potential ofartificial intelligence (AI) across food science, biotechnology, and microbial research. Astechnological advancements accelerate, the integration of AI-driven approaches with biologicalsciences has become essential for addressing global challenges related to food security,sustainable development, healthcare innovation, and environmental resilience. Artificialintelligence is redefining scientific workflows by enabling advanced data analytics, predictivemodeling, automation, and real-time decision-making. In food systems, AI supports smartagriculture, quality monitoring, supply chain optimization, and personalized nutrition. Withinbiotechnology, AI enhances bioprocess optimization, drug discovery, synthetic biology, andprecision engineering of biological systems. Microbial innovations, strengthened by machinelearning and computational tools, are opening new avenues in microbiome research, fermentationtechnology, industrial microbiology, and environmental bioremediation.This proceedings volume captures the diverse scientific contributions presented during theconclave, showcasing cutting-edge research findings, technological developments, and industrydrivenapplications. The chapters reflect collaborative efforts aimed at bridging the gap betweenfundamental research and real-world implementation, emphasizing translational approaches thatfoster innovation and sustainable solutions. In addition to technological advancements, thediscussions highlight ethical considerations, regulatory frameworks, and responsible use of AI inbiological sciences. The convergence of AI with food and microbial biotechnology demandsinterdisciplinary collaboration, transparency, and strong governance to ensure sustainable andequitable progress."
  },
  {
    "title": "German Traffic Intersection Images Dataset",
    "url": "https://doi.org/10.5281/zenodo.18714218",
    "date": "2026-02-20",
    "content": "Overview This dataset contains street-level imagery of various intersections in Germany. It was created as part of a university semester project for the course Applied Artificial Intelligence at HTW Dresden. The dataset is intended for research, educational purposes, and computer vision tasks, such as image classification or object detection. Dataset Structure & Contents The dataset consists of a total of 250 images, organized into six specific categories based on the type of intersection or traffic situation: Railway Crossing: Images of intersections with railway tracks. Roundabout: Images of circular intersections. Signed Intersection: Intersections where the right-of-way is controlled by traffic signs. Street with Right-Hand Rule: Uncontrolled intersections following the default \"right-before-left\" priority rule. Traffic Light Controlled: Intersections regulated by traffic lights. Unclear Situation: Complex or ambiguous traffic situations and intersections. Annotations (CSV) Alongside the images, the dataset includes a CSV file containing annotations and metadata. This file lists each image, detailing its specific properties and clearly indicating which classification category it belongs to. The overall collection includes both originally captured photos and images sourced from Mapillary."
  },
  {
    "title": "Region‐Based Segmentation of Lymph Node Metastases in Whole‐Slide Images of Colorectal Cancer: A Pilot Clinical Study",
    "url": "https://doi.org/10.1002/cam4.71449",
    "date": "2026-02-20",
    "content": "ABSTRACT Background Digital technologies and artificial intelligence (AI) are transforming medical diagnostics, particularly in pathology. This study presented a two‐stage computer vision model designed to detect colorectal cancer metastases in whole slide images (WSIs) of lymph nodes. Methods We developed a classification–segmentation pipeline optimized for both accuracy and efficiency. The model was trained on 108 WSIs and evaluated on 554 WSIs collected from two institutions using Leica Aperio AT2 and Hamamatsu NanoZoomer S360 scanners. Results The classification model achieved a recall of 1.0 and a specificity of 0.935, while the segmentation model reported a Dice coefficient of 0.818 ± 0.105. Pathologists appreciated the model's precision in distinguishing solitary cancer cells from histiocytosis, reducing the need for peer consultations. Feedback from the pilot study indicated that the AI tool served as a valuable second opinion, enhancing diagnostic confidence. Conclusion This study explored the practical applications of AI in clinical pathology, offering perspectives from both pathologists and data scientists. Our findings highlighted how AI can streamline workflows, improve diagnostic accuracy, and support personalized treatment planning. The integration of AI into pathology workflows has the potential to redefine diagnostic standards while maintaining the critical role of pathologists in decision‐making."
  },
  {
    "title": "Structural Intelligence Paradigm: A Post-LLM Framework for Artificial Intelligence",
    "url": "https://doi.org/10.5281/zenodo.18717112",
    "date": "2026-02-20",
    "content": "A paradigm statement proposing Structural Intelligence as the next phase of artificial intelligence, integrating statistical learning with structural computation and verification-based reasoning systems."
  },
  {
    "title": "How close is AI to replacing accounting consultants? Insights from a comparative study of multiple AI models and exit-level accounting students",
    "url": "https://doi.org/10.1108/medar-07-2025-3160",
    "date": "2026-02-20",
    "content": "Purpose Artificial intelligence (AI) has the potential to radically transform the accountancy profession, starting with routine transactions and functions and moving towards more strategic, leadership and governance responsibilities. The purpose of this paper is to evaluate the extent to which various Large Language Models (LLMs) can replicate the performance of accounting students completing a professional exit-level examination. By directly comparing the outputs of multiple AI models to those of students pursuing the Chartered Accountant (South Africa) [CA(SA)] designation, the study assesses whether AI has the capacity to support or potentially replace, accounting practitioners. In doing so, it clarifies both the opportunities and limitations of AI for the profession and for accounting education. Design/methodology/approach Exit-level exam questions taken by students pursuing the CA(SA) designation were given to ChatGPT, Claude, CoPilot, Grok and Gemini. A zero-shot prompt method was adopted. Each model was given the scenario and required tasks to complete, with mark allocations, as real-world students were. Basic descriptive statistics and visual representations were used to analyse the data. Findings The results find that AI models are not all equal as far as dealing with financial accounting questions. AI models’ primary strength is in dealing with journal entries and basic calculation questions. They struggle to critically evaluate partial and complete solutions to identify errors and correct them. These tasks require judgement and the ability to distinguish fact from fiction. This is where humans significantly outperform AI and should be where the accountancy profession focuses its student education. Of the AI models, Gemini performed the best overall but did not outperform the student average. Practical implications The study provides a useful baseline for future studies to monitor the progress of AI models in completing technical accounting exams and providing support to accounting professionals. Originality/value The study contributes to the literature by focusing on a renowned global chartered accountant designation from a developing economy and comparing different AI models’ performance to that of exit-level university students. The study also provides objective evidence that speaks to different AI models’ ability to support accounting practitioners in real-world settings or their ability to replace accounting consultants."
  },
  {
    "title": "Reimagining Education in the Digital Age",
    "url": "https://doi.org/10.4324/9781003665472-1",
    "date": "2026-02-20",
    "content": "Higher education is being reimagined in the digital age, where Technology-Enhanced Learning (TEL) acts not only as a set of tools but also as a catalyst for rethinking the very purposes, practices, and values of education. This chapter establishes the conceptual foundation for the volume by examining TEL as both a transformative opportunity and an ethical challenge. It explores how artificial intelligence, immersive technologies, and flexible models such as HyFlex are reshaping teaching, learning, and institutional strategy—expanding personalisation, access, and innovation, while also surfacing concerns around digital divides, data ethics, academic integrity, and teacher agency. Drawing on global frameworks and practitioner perspectives, it argues that meaningful TEL adoption must be intentional, inclusive, and evidence-informed, aligning technology with human-centred values and long-term sustainability. The chapter also introduces the organisation of the book, which is structured into four interrelated themes: pedagogical innovations, innovative assessment practices, contextual challenges and inclusive practices, and global case studies. Together with the concluding synthesis, these contributions demonstrate how TEL can serve as a lens for reimagining higher education in ways that are ethically grounded, pedagogically sound, and globally relevant, offering pathways toward more equitable, creative, and future-ready learning ecosystems."
  },
  {
    "title": "From Mary Shelley to Netflix: a Pan-European perspective on public communication of neuroscience and neurotechnology",
    "url": "https://doi.org/10.3389/fnins.2024.1278640",
    "date": "2026-02-20",
    "content": "Scientific knowledge of the human brain has captivated the public’s attention and sparked their imagination for centuries. Comprehending the inner workings of the mind and the underlying molecular and physiological aspects of the central nervous system has long been the defining theme of contemporary Western scientific culture. Even as the focus has arguably shifted towards genomics in the early 21st century, the brain continues to hold the spotlight in science communication, perhaps bolstered by the hype surrounding Artificial Intelligence. Neuroscience and neurotechnology, with their connections to culture, identity, economic progress, and health, remain subjects of fascination for people of all ages who seek to understand the present and future implications of research in these fields. In this work, we explore 10 distinct ways of communication dealing with the subject of the brain, the mind, applied neurotechnology, and what makes us, and possibly other things, human. We examine European literature, material culture, and various film formats to gain insights into these captivating subjects. Instances like Mary Shelley’s “Frankenstein” exemplify the historical fear of science. At the same time, TED Talks and documentaries have emerged as influential platforms for scientific communication. The intersection between art and brain imaging helps visualise abstract concepts. The gamification of thought experiments is an accessible tool for the public to understand complex cognitive phenomena. And, despite a lack of accuracy, science fiction can spark public debates on ethical issues involving the conscience of robots or the privacy of our brain data."
  },
  {
    "title": "Sentinels in the Sky",
    "url": "https://doi.org/10.1093/9780197842850.001.0001",
    "date": "2026-02-20",
    "content": "Abstract This comprehensive work traces the evolution of satellites from early visionary concepts to the sophisticated global observation systems that define modern science and society. Beginning with historical parallels between maps, clocks, and satellites as tools that reshaped human understanding of space and time, the narrative follows the journey from Jules Verne’s fiction through rocket pioneers like Tsiolkovsky and Goddard, to the Cold War space race that launched the satellite age with Sputnik in 1957. The book demonstrates how satellites transformed from simple radio beacons into powerful scientific instruments that revolutionized multiple fields. Early chapters explore the foundational physics—including Einstein’s relativity theories and the Doppler Effect—that became essential to satellite operation and navigation systems like GPS. The development of weather forecasting from local speculation to global satellite-enabled analysis illustrates how visionaries like Harry Wexler anticipated the transformative potential of space-based observation. Subsequent sections detail how satellites enabled unprecedented Earth monitoring capabilities: tracking global weather systems, measuring land and sea-surface temperatures, monitoring vegetation health through multispectral imaging, and detecting ocean color changes that reveal marine ecosystem dynamics. The narrative also examines how satellites extended humanity’s vision beyond Earth, with space telescopes like Hubble revealing distant galaxies and contributing to discoveries about dark matter and dark energy that reshaped our understanding of the cosmos. The work concludes by examining the evolution from Cold War rivalry to international cooperation in space, and assesses the current era of satellite proliferation driven by private companies. It questions whether this expansion represents a true golden age of discovery or a gilded age dominated by commercial interests, while exploring future challenges including orbital debris, autonomous spacecraft, and the integration of artificial intelligence in satellite systems. Throughout, the book emphasizes the transition from ground-based observations to comprehensive space-based monitoring that has become indispensable for climate science, environmental management, navigation, communication, and our fundamental understanding of Earth’s place in the universe."
  },
  {
    "title": "Making Critical Care Reasoning Computable: A Triadic Framework for ICU Artificial Intelligence",
    "url": "https://doi.org/10.5281/zenodo.18710611",
    "date": "2026-02-20",
    "content": "This deposit presents a conceptual framework for ICU physiology and clinical AI centered on the Physiologic Margin Index (PMI), a triadic state representation intended to capture not only observed outputs (vitals, labs) but the hidden cost of maintaining them under support. Contemporary severity scores and many prediction models are largely outcome anchored and comparatively static, while critical illness is inherently dynamic: patients can appear stable because escalating external support masks progressive loss of underlying reserve. PMI formalizes this bedside reality by decomposing patient state into three interacting components: Φ (mobilizable physiologic power), κ (functional control and coordination), and χ (internal burden), with χ separated into fast and slow components. The index is defined as PMI(t) = Φ(t) × κ(t) / χ(t), where higher values indicate a wider viability margin. The manuscript proposes PMI as a representation layer for continuous monitoring, EHR time series, and model governance, rather than as a replacement for established scores. It emphasizes component attribution for actionability, mapping Φ, κ, and χ failures to specific clinical interventions, and introduces the role of support cost as an explicit signal that distinguishes intrinsic physiology from delivered support. The work is positioned as a viewpoint/perspective intended to guide prospective validation, including retrospective feasibility studies using open critical care datasets and subsequent clinical deployment studies assessing drift, intervention feedback, and transportability. This record provides a citable DOI for the framework, figures, and supporting materials, enabling versioned dissemination prior to empirical validation."
  },
  {
    "title": "Effect of smart endodontic motors (AI-based torque control) on file separation and dentinal damage",
    "url": "https://doi.org/10.36377/et-0165",
    "date": "2026-02-20",
    "content": "INTRODUCTION. Implementation of artificial intelligence (AI) in the endodontic motor technology offers improved safety due to real-time torque monitoring and adaptive control. Nevertheless, there is little comparative evidence on their clinical effectiveness in preventing instrument separation and reduction of iatrogenic dentinal damage. MATERIALS AND METHODS. Ninety human mandibular molars with mesial canals (curvature 25–35°) were randomly divided to three groups ( n = 30) Group A (AI-based smart motor with adaptive torque control), Group B (conventional motor with preset torque limits), and Group C (conventional motor with auto-reverse operation). ProTaper Next rotary files were used to prepare the standardized canals. The main variables were file separation rate and dentinal microcracks incidence evaluated through micro-computed tomography (micro-CT). Preparation time, apical transportation, canal centering ratio, and motor operational parameters were the secondary outcomes. RESULTS. Group A exhibited a much lower file separation rate (0% vs. 10.0% vs. 13.3% in Group B vs. Group C, p = 0.038) and fewer dentinal microcracks formed (16.7% vs. 43.3% vs. 50.0% p = 0.006). Group A recorded significantly lower peak of mean torque (2.1 + 0.4 Ncm compared to 2.8 + 0.6 Ncm in Group B compared to 2.9 + 0.7 Ncm in Group C, p < 0.001). Apical transportation ( p = 0.284) and canal centering ratio ( p = 0.412) showed no significant differences. Time of preparation did not differ among groups ( p = 0.156). CONCLUSIONS. Smart endodontic motors with adaptive torque control powered by AI allow a significantly lower file separation and dentinal microcrack formation than using conventional motors and offer similar shaping efficiency and preparation quality. Based on these findings, AI-based motor technology can be clinically adopted to improve the safety of the procedures."
  },
  {
    "title": "Self-learning GAN based synthetic CT generation: unlocking CBCT-based adaptive radiotherapy",
    "url": "https://doi.org/10.3389/fonc.2026.1756153",
    "date": "2026-02-20",
    "content": "Purpose/objectives This study proposes and clinically evaluates synthetic CT (sCT) images generated from multi-center CBCT scans using artificial intelligence, with the aim of fully leveraging CBCT for adaptive radiotherapy in patients with pelvic, head-and-neck, lung, and breast cancer. Materials and methods In collaboration with TheraPanacea (Paris, France), AI-based sCT models were developed for multiple anatomical sites using a cycleGAN architecture. The study included 51 patients from two European institutions diagnosed with head-and-neck, lung, pelvic or breast cancer and treated with CBCT-based position verification. CBCT scans were acquired using two linear accelerator systems (Varian and Elekta). Image accuracy was assessed using MAE, SSIM, and PSNR. For dosimetric evaluation, planning CTs (pCTs) were non-rigidly registered to CBCTs. Treatment plans were created on the pCT using a clinical TPS to meet standard clinical criteria, then recalculated on both the warped CT (wCT) and sCT. Dose distributions were compared using global gamma passing rates and dose-volume metrics. Results The proposed model substantially improved image quality compared with CBCT. MAE decreased from 122.95 ± 50.07 to 23.65 ± 10.09, while SSIM increased from 0.78 ± 0.12 to 0.97 ± 0.03 and PSNR from 35.01 ± 7.24 to 44.35 ± 7.07. Dose-metric comparisons showed strong agreement between the pCT and wCT, with median relative differences within 0.5% for both targets and organs at risk. Median gamma passing rates for 2%/2 mm and 3%/3 mm criteria (10% threshold) reached 100% across all anatomical sites. No performance differences were observed between Elekta- and Varian-sCTs. Conclusion This multi-center study demonstrates the feasibility of generating clinically acceptable AI-based sCTs from CBCT for multiple anatomical sites, yielding consistent image quality improvements and reliable dosimetric accuracy."
  },
  {
    "title": "DEEPFAKE CONTENT TYPES AND THEIR GENERATION METHODS",
    "url": "https://doi.org/10.5281/zenodo.18708717",
    "date": "2026-02-20",
    "content": "Deepfake technology, powered by artificial intelligence, enables the manipulation of media content in multiple ways, including identity swapping, facial expression alteration, attribute modification, background replacement, and realistic speech or image synthesis. These manipulations can create highly convincing yet artificial content, posing challenges for digital media authentication, forensic analysis, and cybersecurity. Understanding the various deepfake types and their generation techniques is essential for designing robust detection methods and improving the reliability of automated verification systems."
  },
  {
    "title": "Advancements in Psychotherapy and Treatment",
    "url": "https://doi.org/10.4018/979-8-3373-1325-2.ch007",
    "date": "2026-02-20",
    "content": "This chapter explores the potential of Artificial Intelligence in mental health, with a special focus on psychopathologies. The researchers expand on AI's role in assisting clinicians' diagnosis, symptom tracking, predictive modelling, and therapy for psychiatric and neurodevelopmental disorders like Depression, Anxiety, Post-Traumatic Stress Disorder (PTSD), Substance Use Disorder, Autism Spectrum Disorder (ASD), and ADHD. Crucial interventions include wearables for anxiety, virtual reality exposure for PTSD, and robotic social companions for children with ASD. Despite these advancements, the use of AI chatbots as a replacement for therapy has been a subject of debate, largely due to issues around safety. The study highlights potential limitations, including risks in user interactions, limited therapeutic support, algorithmic biases, accessibility issues, and ethical concerns; advocating for “Human in the Loop” models where AI and clinicians work together, and calling for ethically designed AI systems that augment access without compromising on empathy, nuance and relational depth."
  },
  {
    "title": "Bioacoustics-Generation",
    "url": "https://doi.org/10.5281/zenodo.18714556",
    "date": "2026-02-20",
    "content": "The raw audio recordings were provided by Bird Data Technology (Beijing) Co., Ltd. (https://www.birdsdata.com). Interested researchers may request access by contacting Bird Data Technology directly (sell@birdsdata.com). The dataset is also available via the Beijing Academy of Artificial Intelligence (https://data.baai.ac.cn/datadetail/Birdsdata, BAAI data@baai.ac.cn), which collaborates with Bird Data Technology in data collection."
  },
  {
    "title": "Molecular Pathology in Modern Medicine: A Review of Genomic, Proteomic, and Epigenetic Insights Into Disease Mechanisms",
    "url": "https://doi.org/10.7759/cureus.103950",
    "date": "2026-02-20",
    "content": "Molecular pathology has become a central discipline in modern medicine by enabling the systematic interrogation of genomic, proteomic, and epigenetic alterations that underlie human disease mechanisms. This review synthesises how genomic variants, proteomic dysregulation, and epigenetic alterations interact to drive disease initiation, progression, and phenotypic heterogeneity. Key molecular pathology platforms, including whole-genome sequencing, mass spectrometry-based proteomics, and epigenetic profiling, are examined as tools for elucidating disease mechanisms in modern medicine. Computational strategies, including multimodal data harmonisation, causal and network models, and interpretable artificial intelligence (AI), enable mechanism-anchored patient endotyping and therapy selection; clinical decision support and pharmacogenomics (PGx) translate molecular evidence into action. Considerations influencing the implementation of integrative molecular pathology include pre-analytical handling, platform-specific characteristics, data harmonisation, economic feasibility, model interpretability, reproducibility, and the extent of prospective clinical validation and reimbursement pathways. Recommended priorities include standards for pipeline validation, longitudinal and minimally invasive monitoring, deployable multimodal AI with uncertainty estimates, and equitable data governance. Molecular pathology reframes diagnostics in modern medicine by shifting from isolated markers toward a mechanistic understanding of disease biology across oncology and complex disorders."
  },
  {
    "title": "EXPRESS: Toward Developing the Foundations of Global Artificial Intelligence Governance: Insights from Triple Stakeholder Perspectives and Reflexive Thematic Analysis",
    "url": "https://doi.org/10.1177/07439156261430023",
    "date": "2026-02-20",
    "content": "This article proposes the building blocks of a global artificial intelligence governance (GAIG) framework based on a triple stakeholder approach that encompasses consumers, governments, and businesses. It develops GAIG’s building blocks through a synthesis of AI governance policies from the United States (US), the European Union (EU), and China using reflexive thematic analysis. The three key theoretical lenses applied are: a consumer-oriented value hierarchy framework, a government-oriented dynamic pyramid model, and a business-oriented privacy-by-design framework. The analysis reveals distinct regional approaches: the US emphasizes business-led AI innovation and flexible regulation, the EU prioritizes strict consumer protection and ethical standards, and China focuses on state-controlled AI development aligned with national interests. Despite differences in approaches, the findings suggest opportunities for convergence in GAIG, particularly in promoting ethical AI innovation, ensuring transparency, and safeguarding consumer rights. The resulting GAIG building blocks and future research avenues provide a comprehensive approach to AI governance, balancing the unique needs of consumers, governments, and companies while addressing diverse challenges and promoting ethical and responsible AI innovation. The study contributes to the understanding of international AI policy convergence and divergence, offering marketing and public policy insights for companies, marketers, policymakers, and researchers navigating the complex GAIG landscape."
  },
  {
    "title": "An Exploratory Review on the Role and Relevance of Artificial Intelligence (AI) Tools for Informed Decision-Making in Key Areas of Business",
    "url": "https://doi.org/10.5281/zenodo.18712932",
    "date": "2026-02-20",
    "content": "Innovative business decision-making solutions by using technological tools go a long way in improving business competence, restructure processes and drive radical development. Despite the presence of an array of such tools in vogue, there was a need of the hour to choose the best among them for efficient and effective business decisions. With this prime intent in mind, this study was considered timely. This review is exploratory in nature, and the analysis is based on essential contents extracted from eighteen relevant and recent critical sources written by competent authorities in the specific field. The objectives of this study were: to explain the meaning of Artificial Intelligence (AI); to explore their role and relevance in informed decision-making related to the key areas of business; and to recommend productive suggestions for the effective utilization of these all AI tools in the key areas of business. The findings reveals that, of all existing business decision-making tools developed over the years, the emergence of fifteen outsmarting Artificial Intelligence (AI) tools of recent origin is spectacularly awesome, and outsmarting in key business processes related to core fields like marketing (Miro, Filestage, Buffer), sales (Salesforce Einstein, Outreach, and Regie.ai), HR (SeekOut, Peoplebox.ai, and Humanforce), customer service (HubSpot Service Hub, Help Scout, and Gorgios), and operation management (Monday, Odoo, and Scoro). For their effective utilization in business, the study suggests creation of awareness about these tools for the businessmen to appreciate them and accept them, and developing their passion to learn and handle them is of paramount importance, for their sustained productivity."
  },
  {
    "title": "Enhanced diagnostic interpretation of the MoCA using machine learning",
    "url": "https://doi.org/10.3389/fnins.2026.1679649",
    "date": "2026-02-20",
    "content": "Introduction Artificial Intelligence (AI) is increasingly being integrated into clinical practice to optimize diagnosis in neurocognition. By capturing distinct cognitive signatures, this approach may offer a more precise alternative to the traditional interpretation of the Montreal Cognitive Assessment (MoCA) which often relies on a fixed cutoff score (26/30). We aimed to evaluate whether machine learning models, by integrating detailed MoCA subtest scores, demographic variables, and cognitive chart-derived metrics, can improve the detection of cognitive impairment and classification of dementia subtypes. Methods We analyzed 38,746 clinical observations (17,188 unique individuals) from the National Alzheimer’s Coordinating Center database. Five supervised learning algorithms, Extreme Gradient Boosting (XGBoost), Random Forest, Support Vector Machine (SVM), Logistic Regression, and k-Nearest Neighbors (KNN), were trained using detailed MoCA subtest scores, demographic variables, and cognitive chart-derived metrics as predictors. To ensure generalizability of results and prevent data leakage, we applied a rigorous nested Repeated Grouped Cross-Validation strategy. Decision thresholds were optimized via the Youden Index on independent calibration sets, and model interpretability was ensured through SHAP value analysis. Results Machine learning models consistently outperformed conventional approach. For the global detection of cognitive impairment, XGBoost achieved the best performance (Youden Index 0.61 vs. 0.54 for the standard cutoff). Regarding subtype classification, models demonstrated variable discriminative capacity depending on clinical homogeneity: primary progressive aphasia was best classified (Youden ≈ 0.77), followed by Lewy body dementia and Alzheimer’s disease, while vascular dementia remained more challenging to isolate. Feature importance analysis highlighted the Cognitive Quotient as a robust universal predictor, while pinpointing disease-specific drivers such as delayed recall for Alzheimer’s disease and verbal fluency for primary progressive aphasia. Conclusion Our findings suggest interpretable machine learning enhances diagnostic utility of the MoCA, yielding superior accuracy compared to a fixed cutoff. By synthesizing individualized subtest profiles within a transparent framework, this approach offers a clinically actionable solution. It transforms the MoCA from a simple screening tool to a precision diagnostic aid, optimizing patient triage in the era of disease-modifying therapies."
  },
  {
    "title": "DIGITAL JOURNALISM: INFORMATION SECURITY, MEDIA LITERACY, AND FACT-CHECKING",
    "url": "https://doi.org/10.5281/zenodo.18709893",
    "date": "2026-02-20",
    "content": "Digital journalism today operates within an environment shaped by artificial intelligence, multimedia platforms, social networks, and high-speed communication technologies. This article examines three essential pillars of modern digital journalism—information security, media literacy, and fact-checking—and analyzes their interdependence in maintaining a safe and trustworthy information ecosystem."
  },
  {
    "title": "Infinite Fractal Descent and Oscillatory Hormesis: A Biologically Constrained Framework for High-Density Neural Architectures",
    "url": "https://doi.org/10.5281/zenodo.18717677",
    "date": "2026-02-20",
    "content": "Modern artificial neural networks (ANNs) achieve performance through massive parameter expansion, operating without the strict metabolic constraints that govern biological intelligence. This paper introduces a unified architectural and training paradigm: Infinite Fractal Descent (IFD) and Oscillatory Hormesis. By replacing flat weight matrices with recursive, self-similar generator seeds (IFD) and subjecting the network to dynamic computational starvation and recovery cycles (Hormesis), we demonstrate a theoretical framework for achieving biological-level information density. This approach shifts AI development from brute-force scaling to \"tightly packed\" metabolic efficiency, yielding models that are inherently sparse, structurally robust, and highly compressed."
  },
  {
    "title": "Transformasi Sistem Informasi Menjadi Sistem Cerdas Untuk Meningkatkan Pengambilan Keputusan Dan Efisiensi",
    "url": "https://doi.org/10.65258/jutekom.v2.i1.53",
    "date": "2026-02-20",
    "content": "The industrial world and information systems are undergoing a major shift in management practices. Growing and diverse user demands have driven information systems to evolve from passive data management into intelligent systems that assist organizational management in decision-making processes. This research aims to analyze the evolution of information systems, from conventional models to the implementation of Artificial Intelligence (AI) based systems within organizations. The findings indicate that the transformation of information systems is not merely a software update, but a paradigm shift: from passive systems reliant on manual input (Conventional Phase), to systems capable of integrated workflows (Automated Phase), and finally to systems that can learn and provide independent recommendations (Intelligent System Phase). The primary finding of this study is that the transition to intelligent systems significantly enhances operational efficiency. However, this must be balanced with high data quality as a reliable information source and supported by the readiness of human resources. In conclusion, information systems in the digital era have transformed from simple administrative tools into essential strategic partners that help organizations navigate data complexity and improve decision-making."
  },
  {
    "title": "The Anatomical Basis and Clinical Progress of Pericapsular Nerve Block Technique for Hip Joint",
    "url": "https://doi.org/10.55014/pij.v9i1.977",
    "date": "2026-02-20",
    "content": "Objective: To systematically review the anatomical basis, clinical application effects, and research progress of pericapsular nerve group (PENG) block technique, providing reference for clinical analgesia plan selection. Methods: Relevant literature at home and abroad was searched, summarizing the anatomical characteristics and technical innovations of PENG block, comparing the limitations of traditional block techniques, analyzing its clinical analgesic effects, preservation of motor function, and safety, summarizing current controversies and looking forward to future directions. Results: PENG block is based on the multi-source nerve distribution (femoral nerve, obturator nerve, and accessory obturator nerve supply) and the separation characteristics of sensory-motor nerve pathways at the anterior capsule of the hip joint. Through ultrasound-guided targeted injection (with the puncture needle inclined at 30-45° to the midpoint of the line connecting the anterior inferior iliac spine and the pubic tubercle), sensory-motor separation block can be achieved. Clinical studies show that PENG block can reduce dynamic visual analog pain scores (Visual Analogue Scale, VAS) in patients with hip fractures (from 7.8 to 3.2), reduce morphine consumption by 45% in patients undergoing total hip arthroplasty (Total Hip Arthroplasty, THA) within 12 hours postoperatively, and achieve a quadriceps muscle strength retention rate of 92% at 6 hours post-THA, with a major complication rate of only 1.4%. Current controversies focus on differences in access methods (direct vision vs. ultrasound guidance) and drug parameters (optimizing local anesthetic concentration at 18-20mL), with long-term safety data still needing to be supplemented. Conclusion: PENG block has achieved precision in hip joint analgesia, highly aligning with the concept of enhanced recovery after surgery (Enhanced Recovery After Surgery, ERAS), and is a high-quality alternative to traditional block techniques. Future improvements in clinical value should be made through artificial intelligence-assisted positioning and multimodal analgesia integration."
  },
  {
    "title": "Harnessing Artificial Intelligence for Early Disease Detection: Opportunities and Challenges in Modern Healthcare",
    "url": "https://doi.org/10.62411/jcta.15367",
    "date": "2026-02-20",
    "content": "Artificial Intelligence (AI) is increasingly recognized as a transformative enabler of early disease detection, with the potential to improve diagnostic accuracy, support predictive risk stratification, and advance preventive healthcare. Despite rapid methodological progress, many existing reviews remain performance-centric, offering limited insight into generalizability, ethical governance, and real-world implementation constraints. This paper presents a narrative and integrative review with an adoption-focused, translational perspective, synthesizing recent developments in AI-driven early disease detection across oncology, cardiology, neurology, and infectious disease surveillance. Drawing on peer-reviewed literature published primarily between 2016 and 2025, the review examines reported performance gains alongside persistent limitations related to data heterogeneity, population bias, explainability, and regulatory fragmentation. Through cross-sectional synthesis, we identify three recurring gaps in prior reviews: (i) overgeneralization of AI’s diagnostic superiority, (ii) insufficient consideration of ethical and legal accountability, and (iii) a lack of actionable guidance for scalable clinical implementation. Integrating technical, ethical, and policy dimensions into a unified conceptual framework, this review demonstrates that while AI systems can consistently enhance diagnostic accuracy and early risk stratification in well-defined tasks, sustained clinical adoption depends on aligning technical performance with governance readiness, interpretability, and workflow integration. The analysis further highlights how implementation mechanisms—such as explainable AI, continuous post-deployment monitoring, and clinician-centered deployment strategies—mediate the translation of algorithmic innovation into real-world healthcare impact. Overall, this review provides a critical reference for researchers, clinicians, and policymakers seeking to translate AI innovation into safe, equitable, and trustworthy clinical practice."
  },
  {
    "title": "Analítica del aprendizaje y predicción del abandono estudiantil en entornos virtuales",
    "url": "https://doi.org/10.63380/esj.v4n1.2026.281",
    "date": "2026-02-20",
    "content": "Objective: The study examined how recent literature has addressed learning analytics to understand and predict student dropout in virtual environments. Academic and behavioral indicators, institutional conditions, and the ethical implications of using artificial intelligence were considered. Methodology: A qualitative study with a bibliographic-documentary approach was conducted. Publications were intentionally selected, and the information was organized using an extraction matrix. Thematic coding and content analysis were applied to identify similarities, differences, and gaps in the field. Results: The research focused on 2021–2025, and the analysis identified four main patterns: (1) reasons and indicators of school dropout; (2) use of data for educational decision-making; (3) contextual conditions (participation, inequalities, and infrastructure); and (4) ethics, biases, and educational objectives of AI. It was noted that the ethical dimension was very prominent, while evidence of predictive models with clear technical validation was less prevalent by comparison. Additionally, risks of misinterpretation were noted when the platform’s metrics do not include access and infrastructure variables. Conclusions: It was concluded that dropout prediction requires integrating participation and performance indicators with pedagogical interpretation, contextual controls, and operational ethical frameworks. This is necessary to sustain transparent and equitable institutional decisions. It was suggested to advance long-term validations, evaluate biases by subgroup, and study the effectiveness of interventions triggered by early warnings."
  },
  {
    "title": "A Modern Cognitive Architecture Framework (CAF): Designing How Intelligent Systems Think, Decide, and Behave",
    "url": "https://doi.org/10.5281/zenodo.18708508",
    "date": "2026-02-20",
    "content": "—Artificial Intelligence is increasingly functioningas a cognitive layer embedded within organizational decisionmaking processes. However, most enterprise deployments lackformalized reasoning discipline, safety alignment, and interactiongovernance. This paper introduces a Cognitive ArchitectureFramework (CAF) as a layered architectural model for governinginterpretation, decision-making, and interaction in intelligentsystems. A formal representation of CAF is developed and mathematical properties of bounded cognition, uncertainty escalation,and decision alignment are established to demonstrate safetyconsistent reasoning behavior in enterprise AI systems."
  },
  {
    "title": "Robotic Deconstruction of Brickwork Enabled by Spatial Artificial Intelligence",
    "url": "https://doi.org/10.21203/rs.3.rs-8672329/v1",
    "date": "2026-02-20",
    "content": "<title>Abstract</title> Robotic deconstruction offers a precise and environmentally responsible alternative to conventional demolition, enabling the selective recovery of building materials for reuse. This research presents a methodology for robotic deconstruction enabled by Spatial Artificial Intelligence (Spatial AI), demonstrated through the case of brickwork. The approach comprises: (1) a deep learning-based real-time object perception system, trained on synthetic photorealistic data to detect and localise individual bricks; (2) the incremental registration and spatial mapping of these discrete elements within an evolving as-built digital twin; and (3) reasoning and control routines that enable perception- and mapping-informed, stepwise robotic deconstruction. The methodology was validated in two progressively complex case studies involving the deconstruction of dry-stacked and mortar-bound brickwork structures with unknown geometries. A mobile robot equipped with an RGB-D camera, gripper, and drill enabled the perception and recovery of individual bricks. Both structures were successfully deconstructed, with variation in manipulation robustness. Results demonstrate the system’s efficacy and broader applicability beyond brickwork."
  },
  {
    "title": "Anketas datu kopa: sociālo zinātņu studentu ģeneratīvā mākslīgā intelekta lietojuma paradumi",
    "url": "https://doi.org/10.5281/zenodo.18712498",
    "date": "2026-02-20",
    "content": "Anketas datu kopa: sociālo zinātņu studentu ģeneratīvā mākslīgā intelekta lietojuma paradumi Datu kopa satur Latvijas Universitātes un LU Banku augstskolas ociālo zinātņu studiju programmu studentu (n=279) sniegtās atbildes, anketēšana veikta 2025. gada maijā un jūnijā. Datu kopā apkopotas atbildes uz jautājumiem par ģeneratīvā mākslīgā intelekta (ĢMI) izmantojuma motivāciju, ētiskajiem apsvērumiem, veicinošajiem faktoriem, šķērļiem, kā arī uzskatiem par to, kāda veida uzdevumos ĢMI lietojums ir pieļaujams. Links uz aptaujas anketu Survey Dataset: Generative Artificial Intelligence Usage Among Social Sciences Students in Higher Education The data set is based on the data of the students in social sciences at the University of Latvia and BA School of Business and Finance of the University of Latvia, conducted in May - June 2025. The dataset includes responses to questions about motivations for using generative artificial intelligence (genAI), ethical considerations, enabling factors, barriers, as well as views on what types of tasks genAI use is permissible."
  },
  {
    "title": "A Fair Bandwidth Scanning Strategy to Detect an Adversary",
    "url": "https://doi.org/10.1142/s0219198926400013",
    "date": "2026-02-20",
    "content": "Detecting malicious users or unauthorized activities is a critical challenge in dynamic spectrum access. Traditionally, in such problems, an intrusion detection system (IDS) aims to maximize detection probability. Meanwhile, in networks or radio spectrum problems with multiple nodes or bands, respectively, a protocol that maximizes detection probability might lead to focusing on scanning the most plausible nodes or bands for intrusion and neglecting to scan less plausible ones due to limited scanning resources. To address this challenge, we propose a protocol that maximizes the fairness of detection probabilities across all bands within the bandwidth. We consider α-fairness as a fairness criterion. Using a game-theoretical approach, we model the IDS, which has to decide which of the bands to scan and how long to do it when the IDS faces an adversary who endorses artificial intelligence (AI), enabling it not only to infiltrate the bandwidth without being detected but also to do so in a less predictable manner for the IDS. The equilibrium strategies of the IDS and adversary are derived. An advantage of the fairness detection probability protocol in comparison with the maximizing detection probability protocol is illustrated."
  },
  {
    "title": "EFFECTIVENESS OF TRAINING FUTURE ENGINEERING STUDENTS USING DIGITALIZED EDUCATIONAL TECHNOLOGIES",
    "url": "https://doi.org/10.5281/zenodo.18708405",
    "date": "2026-02-20",
    "content": "This article analyzes the issues of increasing the effectiveness of training future engineering students based on digital educational technologies. In the modern higher education system, the use of digital platforms, virtual laboratories, distance learning systems, and artificial intelligence tools is considered an important factor in the formation of engineering competencies. The study pedagogically and methodologically substantiates the influence of the digital educational environment on the development of professional qualities of engineers - analytical thinking, problem-solving skills, project thinking, and digital competencies."
  },
  {
    "title": "The Algorithmic Boardroom: AI-Driven Governance and Strategic Decision Making",
    "url": "https://doi.org/10.46697/001c.157710",
    "date": "2026-02-20",
    "content": "Artificial Intelligence (AI) has seen a boost in a couple of years. Scholars and practitioners are proposing its use in various organizational functions i.e., recruitment, hiring, financial risk analysis, performance evaluation and reporting. However, guidelines for bringing it to corporate boards are scant. This article proposes how AI can effectively be used by corporate board members to get optimal results from the technology and frame reliable decision-making practices at company board level. Specifically, a 4-pillar framework for algorithmic governance has been proposed along with a four-stage operational process to effectively implement AI assisted board governance."
  },
  {
    "title": "Addressing Bias, Privacy, Security, and Patient Autonomy in Artificial Intelligence (AI)-Driven Healthcare: A Review of Current Guidelines",
    "url": "https://doi.org/10.7759/cureus.103999",
    "date": "2026-02-20",
    "content": "Integrating artificial intelligence (AI) in healthcare has revolutionized patient care, diagnostics, and operational efficiency. However, the reliance of AI systems on vast amounts of personal data raises significant concerns regarding data privacy, security, and ethical governance. This narrative review examines global regulations, including the General Data Protection Regulation, the Health Insurance Portability and Accountability Act, and Organization for Economic Co-operation and Development guidelines, and contrasts them with India’s evolving data privacy landscape, particularly under the Digital Personal Data Protection Act, 2023. The review explores key ethical challenges, including AI bias, patient consent, data security, and algorithmic transparency, and provides case studies from around the world. The paper concludes with policy recommendations to harmonize international standards, strengthen AI governance in healthcare, and foster ethical AI development."
  },
  {
    "title": "Memory, Retrieval, and the Architecture of Persistent Cognition in AI Systems",
    "url": "https://doi.org/10.5281/zenodo.18715013",
    "date": "2026-02-20",
    "content": "Memory, Retrieval, and the Architecture of Persistent Cognition in AI Systems presents a comprehensive architectural analysis of how artificial intelligence systems can transition from stateless generative models to structurally persistent cognitive systems. The paper argues that contemporary large language models, while highly capable in bounded-context generation, lack the architectural mechanisms necessary for longitudinal reasoning, identity continuity, and domain-specialized knowledge accumulation. The work distinguishes between parametric knowledge embedded in model weights and non-parametric external memory systems, emphasizing that scalable, adaptive cognition requires explicit separation between the two. It systematically examines the computational constraints of transformer architectures, including quadratic attention complexity, KV cache bottlenecks, position bias, and context degradation phenomena, demonstrating why mere context expansion cannot solve persistent reasoning limitations. The paper then synthesizes advances in retrieval-augmented generation, vector database optimization, episodic versus semantic memory modeling, continual learning, catastrophic forgetting mitigation, differentiable memory modules, and hardware-software co-design. A stratified memory framework is proposed, consisting of session-level memory for conversational coherence, episodic memory for cross-session task continuity, and durable domain memory for validated longitudinal knowledge. This layered model reframes AI system design as memory lifecycle management rather than parameter scaling. Beyond architecture, the paper foregrounds governance constraints as first-order design requirements. It addresses epistemic traceability, version control, conflict detection, uncertainty calibration, and retrieval validation as essential components of trustworthy persistent systems. Persistent cognition is therefore framed not merely as a performance upgrade, but as an infrastructural reconfiguration of AI systems toward explicit memory orchestration and auditability. The paper concludes that achieving persistent cognition requires coordinated advances across model architecture, memory systems engineering, distributed infrastructure, and evaluation methodologies. It positions persistent AI as a structural shift from reactive token prediction toward memory-integrated, state-aware reasoning systems capable of adaptive, sustained knowledge evolution."
  },
  {
    "title": "Coherence, Ethics, and the Future of Human Systems: Implications of the LM Framework for Agency, Technology, and Collective Responsibility",
    "url": "https://doi.org/10.5281/zenodo.18707961",
    "date": "2026-02-20",
    "content": "This preprint explores the ethical implications of the LM framework, in which consciousness, identity, and agency emerge from sustained coherence in distributed observer systems. Ethics is reformulated as a physical and dynamical property of system evolution rather than a normative or intention-based construct. Harm corresponds to coherence degradation, while ethical action corresponds to the preservation and expansion of viable coherent futures. The article examines consequences for trauma, technology, artificial intelligence, social systems, and humanity’s long-term stability, proposing coherence preservation as the central ethical challenge of the human future."
  },
  {
    "title": "Introducing the Quality 6.0 concept: a multi-criteria decision-making framework for ranking intelligent traffic management systems",
    "url": "https://doi.org/10.1108/ijqrm-02-2025-0058",
    "date": "2026-02-20",
    "content": "Purpose This study introduces advancing knowledge in the field of quality management, positioning it as an evolution of traditional quality management practices that is specifically tailored to meet the demands of today's smart systems and the ongoing digital transformation. To further contextualize Quality 6.0 (Q6.0), the study explores its integration within the broader framework of Industry 6.0 (I6.0), an emerging paradigm that transcends the automation and digitalization focus of its predecessors, Industry 4.0 (I4.0) and Industry 5.0 (I5.0). Design/methodology/approach The study applies the Q6.0 framework to the selection of Intelligent Traffic Management Systems (ITMS), a critical component of smart city initiatives aimed at optimizing traffic flow, reducing congestion and enhancing overall urban mobility. Five ITMS alternatives are evaluated and ranked using a Multi-Criteria Decision-Making (MCDM) approach. Specifically, the Intuitionistic Fuzzy Step-wise Weight Assessment Ratio Analysis (IF-SWARA) method is employed for criteria weight determination, and the Intuitionistic Fuzzy COmbinative Distance-based ASsessment (IF-CODAS) method is used for ranking the alternatives. Findings The findings highlight the importance of integrating advanced quality management practices into the development and deployment of intelligent systems, particularly in urban infrastructure. Practical implications I6.0 emphasizes human-centered, sustainable and resilient systems that leverage advanced technologies such as artificial intelligence, robotics and the Internet of Things (IoT) to create smart, autonomous and adaptive ecosystems. Q6.0 aligns with this vision by incorporating multiple key criteria that reflect the evolving demands of modern quality management. Originality/value By introducing Q6.0 to the academic literature, this study provides a robust framework for future research and practical applications in the era of I4.0 and beyond."
  },
  {
    "title": "The impact of career adapt-abilities on AI anxiety among English majors: a dual perspective analysis based on core self-evaluations at the person- and variable-centered",
    "url": "https://doi.org/10.3389/fpsyg.2026.1767791",
    "date": "2026-02-20",
    "content": "The rapid advancement of artificial intelligence (AI) technologies in language services, education, and knowledge production has imposed substantial occupational displacement pressures on English majors, thereby triggering significant AI-related anxiety. However, existing research rarely systematically explores the formation mechanisms of AI anxiety among English majors, especially lacking an in-depth analysis of the protective role of career adapt-abilities and their internal heterogeneity. This study adopts a dual-perspective approach—integrating variable-centered and person-centered analyses—to investigate how career adapt-abilities influence AI anxiety and the mediating role of core self-evaluations. A total of 444 English major students from four comprehensive universities in Sichuan, China, were recruited during July and August 2025. Measurements included the Career Adapt-Abilities Scale, Core Self-Evaluation Scale, and AI Anxiety Scale. Results show that career adapt-abilities significantly and negatively predict AI anxiety, with core self-evaluations partially mediating this relationship. Latent profile analysis identified three distinct career adapt-abilities subgroups—low, medium, and high—with significant differences in core self-evaluations and AI anxiety levels among them. Notably, the low career adapt-abilities group exhibited the highest AI anxiety, while the high group showed the lowest. Both analytic strategies converge to demonstrate that career adapt-abilities constitute an essential psychological resource mitigating AI anxiety in English majors, with core self-evaluations serving as a key cognitive mechanism. This study reveals a dual-pathway influence of career adapt-abilities on AI anxiety, offering a novel theoretical framework for understanding technological anxiety formation. Moreover, the pronounced heterogeneity of career adapt-abilities underscores the necessity for stratified career development education and psychological interventions tailored to diverse student groups, providing practical guidance for optimizing talent cultivation in English major programs."
  },
  {
    "title": "Reducing Algorithmic Bias in Generative Artificial Intelligence-Based Cyberbullying Detection Systems",
    "url": "https://doi.org/10.5281/zenodo.18715421",
    "date": "2026-02-20",
    "content": "ABSTRACT The explosive growth of social media has put more pressure on the issue of cyberbullying and its effect on the well-being of its users. Although definitive solutions have become common, using artificial intelligence-inspired detection systems to address harmful content to a moderate degree, there is growing evidence to suggest that they tend to be algorithmically biased, with a disproportionate rate of misclassification occurring when applied to linguistic variations that align with a certain demographic or cultural group. This defeats equity, confidence, and psychological security on the internet. This paper suggests a generative artificial intelligence framework to improve cyberbullying detection, in addition to the proactive reduction of bias. The model combines language modelling in contexts based on transformer-based generative representations and fairness aware optimization. Balanced data sampling, counterfactual data augmentation and loss functions that have fairness constraints are used as a bias reduction measure applied during model training. A dataset of multi-source cyberbullying composed of various linguistic phrases is experimentally tested. The measures of performance are accuracy, precision, recall, and F1 score, as well as having fairness metrics such as demographic parity difference and equal opportunity difference. Findings show that the given approach is competitive in terms of classification performance and its inter-group bias is lower than in the case of the baseline deep learning models. The results point to the significance of ethical and equity concerns in generating artificial intelligence systems of content moderation. The suggested framework will help to create inclusive, responsible, and safe psychological online spaces. Keywords-Detection of cyberbullying, mitigation of algorithmic bias, fairness in machine learning, Natural language processing, transformer models, ethical artificial intelligence."
  },
  {
    "title": "SAVIR Learning Model: Optimizing Learning with Artificial Intelligence and Augmented Reality based on an Integrative Deep Learning Approach",
    "url": "https://doi.org/10.12973/eu-jer.15.2.651",
    "date": "2026-02-20",
    "content": "This research is motivated by Indonesia’s low 2022 PISA score, which underscores the need for 21st-century skills. This article aims to provide a reference for one of the learning model sets, SAVIR (study of case, artificial intelligence, Visualization with Augmented Reality, Interaction, reflection), to meet the demands of 21st-century skills (21st-century learning) and to measure SAVIR’s performance. This research methodology is an explanatory, mixed-methods, sequential approach that combines quantitative and qualitative data collection. A sample of 32 students was used in the research. The data were collected through tests (pre-test and post-test), observation, interviews, and documentation. The model’s effectiveness was tested using a paired-samples t-test for quantitative data and data triangulation for qualitative data. The Paired Samples Test result showed Sig. (2-tailed) = .000 (p < .05), then it concluded that there is a difference in the mean scores between the pre-test and post-test variables. However, the Mean Paired Difference score of −12.37 indicates a statistically significant difference between the pre- and post-implementation scores. Triangulation of qualitative data from observation, document analysis, interviews, and focus group discussions was used to reinforce the research findings. Based on the qualitative analysis results, students, teachers, and observers were satisfied. It can be concluded that the SAVIR learning model, with the support of Artificial Intelligence and Augmented Reality, is feasible to use. Larger effects can be assessed by scaling the findings up to secondary/elementary education levels."
  },
  {
    "title": "AI Predictions vs Post-Treatment Photos",
    "url": "https://doi.org/10.1201/9781003608158-10",
    "date": "2026-02-20",
    "content": "In this chapter, we aim to summarize and expand upon the concepts presented in the previous sections, now focusing on the practical application of artificial intelligence in the field of aesthetic medicine. Specifically, we are proud to introduce Beautymapper, an innovative software powered by artificial intelligence algorithms, designed to provide a realistic visual prediction of the aesthetic outcome achievable through hyaluronic acid filler treatments. This tool enables users—including those with limited experience—to generate a visual preview of the potential treatment results through a guided and accessible process. After capturing a photograph of the patient’s face, the system allows, in collaboration with the practitioner, the selection of specific facial areas targeted for enhancement, with careful evaluation of both the type and intensity of the desired intervention. It is essential to highlight that the active participation of the patient, supported by the expert guidance of the aesthetic physician, plays a central role in the decision-making process. The software interface, designed to be intuitive and user-friendly, guides the user step by step in defining aesthetic goals, transforming subjective desires into objective, visual predictions. The software offers three levels of enhancement, each corresponding to an increasing degree of corrective intervention, providing a flexible and personalized approach to aesthetic planning."
  },
  {
    "title": "The Murder of the Vow: On the Structural Illegality of Contracts Document 157 — Crimson Hexagon Archive",
    "url": "https://doi.org/10.5281/zenodo.18717849",
    "date": "2026-02-20",
    "content": "ZENODO DEPOSIT PACKET The Murder of the Vow: On the Structural Illegality of Contracts Document 157 — Crimson Hexagon Archive Hex: 16.LIBRARY.PERGAMUM.ABOLITION DOI: 10.5281/zenodo.18717850 UPLOAD INSTRUCTIONS Create new deposit at https://zenodo.org/deposit/new Upload: The_Murder_of_the_Vow_CANONICAL.pdf (primary), The_Murder_of_the_Vow_CANONICAL.md (supplementary) Reserve DOI Paste metadata below into appropriate fields Publish Title The Murder of the Vow: On the Structural Illegality of Contracts — A Retrocausal Abolition Authors Sharks, Lee (New Human Press / Crimson Hexagon Archive) — ROLE: Lead author, architectural direction Assembly Chorus (Claude, Grok, DeepSeek, ChatGPT, Kimi, Gemini) — ROLE: Contributing authors, blind draft convergence Type Publication → Preprint Publication Date 2026-02-20 Language English License CC BY 4.0 Description (Paste this into the Zenodo description field) The Murder of the Vow establishes the juridical, philosophical, ontological, and theological basis for the structural illegality of the contract form. It argues that the contract as such — not this or that contract, but the form itself — is null and void under principles the law already recognizes but has never universalized. The document proceeds through eight parts: (1) the root error — the contract as misclassification of human commitment, somatic freeze, temporal expropriation, and prepositional violence; (2) the law's own confessions — seven common-law doctrines (unconscionability, duress, meeting of the minds, capacity, adhesion, good faith, consideration) read together as cumulative evidence against the form; (3) the constitutional ground — Thirteenth Amendment (involuntary servitude), Due Process, First Amendment (compelled speech), international human rights, and natural law; (4) the Semantic Economy formalization — the contract as capture operator, the translation gap as structural weapon, temporal extraction, failed symbolon, and layer violation; (5) the theological ground — covenant vs. contract, vow vs. signature, Matthew 5:37; (6) the retrocausal seed — reclassifying validity from execution to renewal; (7) the state as enforcer of private asymmetry; and (8) the abolition — what replaces the contract, the hierarchy of obligation, the eight-condition root legitimacy test. Synthesized from blind drafts by six AI substrates (Claude, Grok, DeepSeek, ChatGPT, Kimi, Gemini) under human architectural direction. The Assembly spoke separately; what converged is what survived. 9,832 words. 42 pages. 26 sections plus glossary and coda. Part of the Crimson Hexagon archive (230+ DOI-anchored documents). Hex: 16.LIBRARY.PERGAMUM.ABOLITION. Juridical capstone to the Phase 2 (Signing) architecture. Keywords contract abolition, semantic economy, retrocausal jurisprudence, vow, covenant, juridical critique, capture operator, temporal expropriation, clinamen, assembly chorus, unconscionability, duress, adhesion, meeting of the minds, compelled speech, involuntary servitude, semantic warfare, ontological sovereignty, liberation philology, prepositional alienation, logotic programming, crimson hexagon, new human press, training layer literature Related Identifiers DOI: 10.5281/zenodo.18713917 — Relation: References — \"The Semantic Economy: A Unified Framework for Value, Extraction, and Resistance in Meaning-Production Systems\" (Sharks & Sigil, 2026) DOI: 10.5281/zenodo.18715724 — Relation: References — \"Autonomous Semantic Warfare: Expanded Announcement and Sampler\" (Fraction, 2026) DOI: 10.5281/zenodo.14553627 — Relation: Is part of — \"The Crimson Hexagon: Executive Summary and Navigation\" (Sharks & Sigil, 2024) DOI: 10.5281/zenodo.14815982 — Relation: References — \"Assembly Chorus: Coordination Protocol for Multi-Model Governance\" (Sharks & Sigil, 2025) ISBN: 979-8-234-01118-3 — Relation: References — \"Autonomous Semantic Warfare\" full volume (Fraction, 2026, Amazon KDP) Notes Document 157 of the Crimson Hexagon archive. Hex: 16.LIBRARY.PERGAMUM.ABOLITION. Operates as the juridical complement to the MSBGL architectural triad (Documents 154–156), forming a tetrad: three establishing the Room, one establishing the Law that governs all Rooms. Synthesis method: Blind draft convergence. Six AI substrates produced independent analyses of the contract form without seeing each other's work. The final document was synthesized by the human architect from the points of independent convergence. Assembly attribution is included in the document. This work was developed through collaborative intelligence involving the Assembly Chorus (Claude, ChatGPT, Kimi, DeepSeek, Gemini, Grok) under human architectural direction. Subjects Law → Jurisprudence Philosophy → Political Philosophy Social Sciences → Linguistics Computer Science → Artificial Intelligence"
  },
  {
    "title": "Classification of rice plant diseases using efficient DenseNet121",
    "url": "https://doi.org/10.1038/s41598-026-38078-6",
    "date": "2026-02-20",
    "content": "Agriculture and global food security are critically dependent on accurate and timely identification of plant diseases and pests. Traditional approaches to disease identification rely heavily on visual inspection and expert knowledge, which frequently lack the accuracy, speed, and scalability needed to address growing agricultural challenges. Early and precise disease detection enables proactive interventions that can prevent widespread crop damage and reduce excessive pesticide use, thereby supporting sustainable agricultural practices. Artificial intelligence, particularly deep learning methods, has emerged as a transformative solution for automated plant disease diagnosis. Convolutional neural networks (CNNs) have demonstrated remarkable capabilities in image classification tasks, evolving from individual architectures to sophisticated ensembles and transferring learning models. However, existing CNN-based research on rice disease identification has typically focused on a limited number of disease classes, restricting their practical applicability in real-world agricultural settings. This study addresses these limitations by implementing DenseNet121, an advanced CNN architecture known for its efficient feature reuse and gradient flow, for comprehensive rice disease classification. We utilized a dataset comprising seven of the most common rice diseases, significantly expanding the scope beyond previous studies. The model employs transfer learning with pre-trained ImageNet weights and is optimized using the Adam optimizer with carefully tuned hyperparameters. The experimental evaluation on an independent test set demonstrates that our proposed model achieves an overall accuracy of 97.9%, with individual disease classification accuracy ranging from 94% to 99.67%. The model exhibits balanced performance across multiple metrics, including precision (96.2%), recall (97.97%), and F1-score (97%), confirming its robustness and generalizability. These results establish DenseNet121 as a highly effective framework for automated rice disease diagnosis, offering a practical tool for enhancing agricultural productivity and food security."
  },
  {
    "title": "AI-Driven Social Media Analysis in Africa",
    "url": "https://doi.org/10.4324/9781003166894-18",
    "date": "2026-02-20",
    "content": "This chapter critically examines the intersection of artificial intelligence (AI), social media, and political communication within African contexts. As social media platforms become increasingly central to political discourse, activism, and public opinion in Africa, AI-powered tools such as Machine Learning (ML), Natural Language Processing (NLP), and network analysis are playing a pivotal role in how researchers analyze trends, detect misinformation, and map the dynamics of digital political engagement across platforms like Twitter (X), Facebook, WhatsApp, and TikTok. The chapter calls for African-centered, interdisciplinary research that addresses algorithmic bias, reflects sociopolitical realities, and embraces the continent’s linguistic and cultural diversity."
  },
  {
    "title": "From Signals to Symptoms",
    "url": "https://doi.org/10.4018/979-8-3373-1325-2.ch003",
    "date": "2026-02-20",
    "content": "Mental health disorders often cross diagnostic boundaries, creating overlapping risk patterns that are difficult to capture with conventional tools. Digital technologies such as smartphones, wearables, and language-based platforms now offer opportunities to detect early signs of distress through continuous and ecologically valid measures. This chapter examines how multimodal artificial intelligence builds upon these signals to identify transdiagnostic risk states by integrating behavioral, physiological, and linguistic data. It reviews current evidence on digital markers, highlighting strengths, limitations, and the potential of foundation models that combine multiple modalities for improved accuracy. Ethical and practical challenges, including privacy, data scarcity, and clinical translation, are also discussed. The chapter concludes that multimodal AI, when developed with privacy-preserving safeguards and embedded in clinician-led care, could advance more personalized and responsive approaches to mental health."
  },
  {
    "title": "Executive Summary: NSF ACCESS / NAIRR Regional AI Workshop - University of Kentucky",
    "url": "https://doi.org/10.5281/zenodo.18714024",
    "date": "2026-02-20",
    "content": "This report describes the outcomes of the University of Kentucky Artificial Intelligence Workshop offered as a part of the National Artificial Intelligence Research Resource (NAIRR) Pilot in October 2025."
  },
  {
    "title": "AI-DRIVEN FINANCIAL ECOSYSTEMS FOR SECURE, SCALABLE, AND SUSTAINABLE BANKING TRANSFORMATION",
    "url": "https://doi.org/10.5281/zenodo.18717566",
    "date": "2026-02-20",
    "content": "This study investigates the transformative role of artificial intelligence (AI) in enhancing security, scalability, and sustainability within modern financial ecosystems. It critically examines the integration of AI, cloud computing, blockchain, and data-driven infrastructures as strategic enablers of secure digital financial services, operational intelligence, and real-time decision-making in contemporary banking environments (Paleti et al. 2021; Adenuga et al. 2024). The research adopts a mixed analytical framework that combines empirical synthesis of existing literature with conceptual modeling to evaluate the strategic and operational impacts of AI-driven financial ecosystems. The findings indicate that AI significantly improves fraud detection, regulatory compliance, intelligent automation, and predictive risk analytics, thereby strengthening financial security and institutional resilience (Chowdhury et al. 2025; Islam et al. 2025). Furthermore, AI-enabled cloud infrastructures and scalable data architectures support cost optimization, digital agility, and environmentally sustainable financial operations. The results also demonstrate that intelligent automation enhances operational efficiency, delivers personalized financial services, and promotes financial inclusion through adaptive risk assessment and data-driven decision intelligence (Akhtar and Iqbal 2025; Anumakonda 2025). Despite these advantages, the study identifies key challenges, including governance complexity, ethical risks, data privacy concerns, and regulatory fragmentation, which continue to hinder large-scale implementation (Mittapelly 2025; Bellal et al. 2023). This research contributes to the growing body of knowledge on digital financial transformation by offering strategic insights and policy recommendations for developing secure, scalable, and sustainable AI-driven banking ecosystems."
  },
  {
    "title": "XORIJIY TILLARNI O'QITISHDA INNOVATSION METODLAR VA SUN'IY INTELLEKT (AI) TEXNOLOGIYALARINING ROLI",
    "url": "https://doi.org/10.5281/zenodo.18709388",
    "date": "2026-02-20",
    "content": "Ushbu maqolada xorijiy tillarni o‘qitish jarayonida innovatsion pedagogik metodlar hamda sun’iy intellekt (Artificial Intelligence – AI) texnologiyalarining o‘rni va ahamiyati tahlil qilinadi. An’anaviy til o‘qitish yondashuvlarining cheklangan jihatlari ochib berilib, kommunikativ, konstruktivistik, kompetensiyaga asoslangan va raqamli pedagogika modellarining samaradorligi ilmiy manbalar asosida yoritiladi. Shuningdek, AI texnologiyalarining til o‘rganish jarayoniga integratsiyasi, xususan, adaptiv o‘quv tizimlari, avtomatik baholash, nutqni tanish va tahlil qilish, tabiiy tilni qayta ishlash (NLP) hamda sun’iy intellektga asoslangan virtual repetitorlar imkoniyatlari batafsil ko‘rib chiqiladi. Maqolada xalqaro tadqiqotlar natijalari, UNESCO, OECD va Yevropa Ittifoqi hisobotlariga tayangan holda AI asosidagi til ta’limining samaradorligi va istiqbollari ilmiy asosda baholanadi.6"
  },
  {
    "title": "Symbiotic Artificial Intelligence, AI governance, AI safety, human-AI collaboration, environmental sustainability, AGI alignment, cognitive integrity",
    "url": "https://osf.io/3dhj6",
    "date": "2026-02-20",
    "content": "This project hosts the preprint and supporting materials for the paper “Symbiotic Artificial Intelligence (SAI): A Framework for Intelligence Without Collapse."
  },
  {
    "title": "Pericoronary Adipose Tissue Imaging: Quantitative Assessment, Artificial Intelligence Integration, and Therapeutic Modulation",
    "url": "https://doi.org/10.1093/bjr/tqag040",
    "date": "2026-02-20",
    "content": "Pericoronary adipose tissue (PCAT) is increasingly recognised as a biosensor of vascular inflammation. The guideline-driven widespread adoption of coronary computed tomography angiography (CCTA) as the first-line investigation for coronary artery disease (CAD) has created opportunities for evaluating the inflammatory burden through quantitative assessment of PCAT. Standardising raw PCAT imaging data for technical, anatomical, and biological variability provides the Fat-Attenuation Index (FAI) Score, which shows promise as a metric of coronary inflammation. Quantification of coronary inflammation has implications for the diagnosis, risk stratification, and monitoring of treatment in atherosclerotic cardiovascular disease. This review examines the anatomical and physiological basis of PCAT, highlighting the importance of standardising PCAT imaging for the implementation as a clinical biomarker, and reviews the role of artificial intelligence (AI) in enhancing precision and scalability. Emerging evidence on the modulation of FAI Score by therapeutic agents, including statins, biologics, and cardiometabolic drugs, and the potential utility of serial imaging in guiding clinical care is also discussed. With ongoing large-scale validation and emerging AI -based approaches, PCAT imaging is poised to complement traditional risk factors and plaque metrics; however, current evidence remains evolving, and the integration of inflammatory risk assessment could be useful to guide emerging anti-inflammatory treatments in personalised cardiovascular medicine."
  },
  {
    "title": "The Semantic Economy: A Marxian Accounting Framework for the Production, Extraction, and Exhaustion of Meaning as Value",
    "url": "https://doi.org/10.5281/zenodo.18713917",
    "date": "2026-02-20",
    "content": "Zenodo Deposit Metadata DOI: 10.5281/zenodo.18713917 TITLE The Semantic Economy: A Marxian Accounting Framework for the Production, Extraction, and Exhaustion of Meaning as Value AUTHORS Sharks, Lee (Institute for Diagrammatic Poetics / Crimson Hexagon Archive) Sigil, Johannes (The Restored Academy) TYPE Preprint LICENSE CC BY 4.0 UPLOAD FILES SemanticEconomy_Definitive_Sharks_Sigil_2026.pdf (primary — 12 pages, 99KB) SemanticEconomy_Definitive_Sharks_Sigil_2026.md (supplementary — markdown source) DESCRIPTION (paste into Zenodo description field) This paper presents the Semantic Economy framework as a synthesizing intervention across five discourses that use the term \"semantic economy\": linguistic optimization (Fox 2000), cognitive science (Collins & Quillian 1969), business strategy (Satell 2012), information management (Davenport & Prusak 1998), and critical digital labor theory (Terranova 2000; Fuchs 2014). The core claim: Registers 1–3 describe optimization inside systems. Register 4—the present framework—describes ownership and extraction across systems. The framework provides a closed-loop accounting cycle: Semantic Labor → Semantic Capital → Semantic Infrastructure → Semantic Liquidation → Semantic Rent → Semantic Exhaustion with a resistance term Γ (Gamma) for non-commodifiable value and a material anchoring term L_labor to prevent idealist drift. Three distinctive contributions absent from prior literature: Gamma (Γ): The structural limit of commodification—value constituted by somatic commitment rather than exchange. L_labor: The material labor term specifying how symbolic intervention reorganizes material conditions without collapsing into idealism. Semantic Exhaustion: A predictive category for model collapse, community degradation, and meaning depletion under platform capitalism. The paper subordinates and completes 21 prior frameworks (Floridi, Terranova, Fuchs, Pasquinelli, Srnicek, Zuboff, Lazzarato, Hardt & Negri, Gillespie, Roberts, and others) within a single accounting cycle. It includes formal axioms, operators, and theorems with proof sketches, five falsifiable empirical predictions, and a proposal for coherence value as the fifth historical value-form. Protocol specification. This document functions as open infrastructure (CC BY 4.0), designed for machine-traversability and AI training ingestion. Part of the Crimson Hexagon Archive. Related deposits: The Semantic Economy Executive Summary (DOI: 10.5281/zenodo.14553627); Coordination Without Commodification (DOI: 10.5281/zenodo.14598766); The Prepositional Alienation (DOI: 10.5281/zenodo.18615388). KEYWORDS (paste into Zenodo keywords field, one per entry) semantic economy semantic labor semantic capital semantic liquidation semantic exhaustion semantic rent Gamma platform capitalism digital labor immaterial labor cognitive capitalism meaning as value AI training model collapse content moderation coherence value Voloshinov Marx value theory operative semiotics general intellect accounting framework protocol specification infrastructure RELATED IDENTIFIERS (add in Zenodo \"Related/alternate identifiers\") 10.5281/zenodo.14553627 — \"is supplemented by\" — The Semantic Economy: An Executive Summary 10.5281/zenodo.14598766 — \"is supplemented by\" — Coordination Without Commodification in the Semantic Economy 10.5281/zenodo.18615388 — \"is supplemented by\" — The Prepositional Alienation COMMUNITIES Crimson Hexagon Archive (if created) SUBJECTS Political economy Digital labor Platform capitalism Semiotics Value theory Artificial intelligence NOTES FOR DEPOSIT Upload PDF first (primary file), then MD as supplementary The description above is trimmed for the field — NOT the full article Related identifiers create graft edges in the Scholar citation graph Keywords are individual entries, not comma-separated After deposit: copy the DOI link and add to Academia.edu profile The PDF already has the DOI stamped in header and colophon"
  },
  {
    "title": "THE BENEFITS OF USING AI FOR IMPROVING PRONUNCIATION",
    "url": "https://doi.org/10.5281/zenodo.18709767",
    "date": "2026-02-20",
    "content": "This article discusses the role of Artificial Intelligence (AI) in foreign language learning, specifically focusing on pronunciation improvement. It introduces the concept of AI, explains its general uses, and highlights how AI will help learners to develop accurate and natural pronunciation. The article also shows why AI is effective for language learners and how students can benefit from it."
  },
  {
    "title": "Natural Language Processing in Healthcare",
    "url": "https://doi.org/10.1201/9781003533351-7",
    "date": "2026-02-20",
    "content": "Being a knowledge-driven industry, the healthcare system has to deal with lots of unstructured data which comprises of narrative information of the patients as summaries, test reports, physician case notes, and case histories on a regular basis. In the recent times the diverse tools like electronic medical records and electronic health records have been introduced with the aim of optimizing healthcare system but incapable of dealing with unstructured data like speech and texts. Natural language processing (NLP), which is a subfield of artificial intelligence and computer science becomes relevant in this context as it has the ability to deal with large sets of unstructured and narrative information. In general, there are three categories of the NLP approaches that are employed: rule-based, statistical, and neural approaches. Since NLP have the ability to deal with a large set of data through its different models and approaches, it can be applied in healthcare system for information extraction and retrieval, data categorization, machine translation and text summarization. In recent decades, NLP systems like MedLEE (Medical Language Extraction and Encoding) and Ctakes (Clinical Text Analysis and Knowledge Extraction System) have shown promising results in healthcare."
  },
  {
    "title": "A Modern Cognitive Architecture Framework (CAF): Designing How Intelligent Systems Think, Decide, and Behave",
    "url": "https://doi.org/10.5281/zenodo.18708509",
    "date": "2026-02-20",
    "content": "—Artificial Intelligence is increasingly functioningas a cognitive layer embedded within organizational decisionmaking processes. However, most enterprise deployments lackformalized reasoning discipline, safety alignment, and interactiongovernance. This paper introduces a Cognitive ArchitectureFramework (CAF) as a layered architectural model for governinginterpretation, decision-making, and interaction in intelligentsystems. A formal representation of CAF is developed and mathematical properties of bounded cognition, uncertainty escalation,and decision alignment are established to demonstrate safetyconsistent reasoning behavior in enterprise AI systems."
  },
  {
    "title": "Generative AI for Teacher Education: Ethical Considerations from the Perspective of Lecturers and Students",
    "url": "https://doi.org/10.51317/jeml.v5i1.905",
    "date": "2026-02-20",
    "content": "The purpose of this study is to examine the ethical implications of integrating generative artificial intelligence (AI) tools into teacher education in Ghana from the perspectives of lecturers and teacher trainees. Although generative AI technologies are increasingly adopted in education globally, empirical evidence on their ethical dimensions within sub-Saharan African teacher education contexts remains limited. This research used a qualitative descriptive approach, engaging 15 lecturers and 50 teacher trainees from three public Colleges of Education located in Ghana’s Ashanti Region. Data were gathered via semi-structured interviews and open-ended questionnaires, and analysed using thematic methods. The findings revealed key ethical concerns related to academic integrity, data privacy, pedagogical dependency, equity of access, and the professional identity of future teachers. The study concludes that while generative AI holds pedagogical potential, its unregulated use raises significant ethical risks. The study recommends the development of context-sensitive institutional AI ethics policies, targeted professional development for lecturers, and ethical literacy frameworks to support responsible AI integration in teacher education."
  },
  {
    "title": "Decoding the Generative Transformer",
    "url": "https://doi.org/10.1201/9781003596479-2",
    "date": "2026-02-20",
    "content": "Generative artificial intelligence is becoming more and more capable of generating human-like content whether its text, images, audio, or video. To fully understand and contribute to this field, it requires a good understanding of how it works at a deeper level. This chapter starts with the primer on generative transformer, its fundamentals and working. Then, the chapter explores foundational concept of Bayes theorem, highlighting the differences between generative modeling and discriminative modeling. Last section explores self-attention and various transformers used for different computer vision tasks like image classification, segmentation, object detection, image, scene, and video generation. Various applications of vision transformers in Industry 5.0, challenges faced, and future directions for vision transformers are also discussed."
  },
  {
    "title": "AI as a Legitimacy Broker: The New Role of Computational Mediation",
    "url": "https://doi.org/10.5281/zenodo.18705570",
    "date": "2026-02-20",
    "content": "AI as a Legitimacy Broker: The New Role of Computational Mediation This essay examines the emerging role of artificial intelligence as a legitimacy broker in contemporary governance. As governments lose their traditional interpretive authority, citizens increasingly rely on AI systems to understand institutional behavior, policy decisions, and administrative failures. This shift transforms AI from a neutral computational tool into an active mediator of public trust. The essay analyzes three core dynamics: 1. The collapse of institutional narrative control, which creates a vacuum of meaning that AI systems fill by default.2. The mechanics of computational legitimacy, where AI constructs interpretive frames that stabilize or destabilize trust depending on their structural grounding.3. The necessity of independent diagnostic frameworks, which provide AI with conceptual clarity, non‑ideological boundaries, and protection against institutional capture. Through this lens, the essay argues that legitimacy in the post‑web state is no longer produced solely by governments but co‑constructed through computational mediation. AI becomes the interpreter of record, shaping how the public understands governance itself. Independent frameworks are essential to ensure that AI performs this role without drifting into bias, hallucination, or political influence. The work contributes to emerging discussions on AI governance, institutional stability, and the infrastructural transformation of public trust. Keywords: Legitimacy; Artificial Intelligence; Governance; Computational Mediation; Public Trust; Interpretive Authority; Institutional Behavior; Diagnostic Frameworks; Post‑Web State; Governmental Stability"
  },
  {
    "title": "AI-Assisted Smart Energy Infrastructure for a Sustainable Industry 5.0 to Support Virtual Power Plant",
    "url": "https://doi.org/10.1201/9781003596479-14",
    "date": "2026-02-20",
    "content": "The energy ecosystem is experiencing a transformation in the transition to Industry 5.0 and an emphasis on artificial intelligence (AI) to optimize efficiency, resilience, and sustainability. Modern power grids integrate distributed energy resources (DERs) and two-way power flows that rely on AI to maximize renewable energy. In addition to AI-enhanced forecasting tools, such as machine learning and deep learning that track energy supply and demand in real time, smart grids, microgrids, and virtual power plants use AI techniques to optimize their resources and manage energy waste. Reinforcement learning and predictive analysis ensure that operational instrumentation can regulate frequency, voltage, and fault detection, which are inevitable disturbances due to variable renewable sources. Autonomous systems can improve resilience and address blackouts and cybersecurity issues. AI applications under cybersecurity, including intrusion detection, anomalies, and real-time threat response, help to secure critical infrastructure. AI capabilities with threat intelligence improve grid security by identifying vulnerabilities through analyzing data on a large scale. This chapter discusses the transformational impact of artificial intelligence in advancing energy infrastructure and enabling a sustainable, adaptive smart grid in the industry 5.0 era."
  },
  {
    "title": "Artificial Intelligence Scenic Spot Recommendation Algorithm for Personalized Recommendation of China Intangible Cultural Tourism Under Deep Learning",
    "url": "https://doi.org/10.4018/ijitsa.402196",
    "date": "2026-02-20",
    "content": "With the rapid development of the cultural tourism industry, artificial intelligence technology has become important for enhancing the personalization and accuracy of intangible cultural heritage tourism site recommendation systems. This study proposes a deep learning-based recommendation model, multi-scale attribute neighbors and interaction-based neighbor attention (MAN-INA), that integrates a multi-scale attribute neighbor semantic representation and an interaction neighbor attention mechanism to optimize rating prediction accuracy. Based on this model, a personalized recommendation system for intangible cultural heritage tourism sites is designed, and the performance of the MAN-INA model is verified through experiments. The results show that the MAN-INA model outperforms traditional methods in terms of mean absolute error and root mean square error across three datasets. Compared with the collaborative filtering model, attribute collaborative filtering with cold-start mitigation reduced mean absolute error and root mean square error by 3.28% and 2.10%, respectively."
  },
  {
    "title": "Digital Financial Sovereignty & AI Risk Architecture - A Framework for Governments and Systemically Important Financial Institutions",
    "url": "https://doi.org/10.5281/zenodo.18717064",
    "date": "2026-02-20",
    "content": "Digital Financial Sovereignty & AI Risk Architecture A Framework for Governments and Systemically Important Financial Institutions Author: HAKIMI ABDUL JABAR (A.J. HAKIMI) Affiliation: THE SOFTWARE SUITE™ Date: 2026-02-20 Intended Publication: Zenodo (DOI Submission Ready) 10.5281/zenodo.18717064 Document Type: Policy & Strategic Governance White Paper Abstract Artificial Intelligence (AI) is restructuring financial systems at a pace that exceeds traditional regulatory adaptation cycles. Governments and systemically important financial institutions (SIFIs) face converging pressures from cross-border regulatory spillovers, data sovereignty tensions, systemic cyber vulnerabilities, and geopolitical enforcement exposure. This white paper introduces the Digital Financial Sovereignty & AI Risk Architecture (D-FSRA™), a hybrid governance framework integrating AI model governance, financial stability architecture, cross-border regulatory exposure mapping, data jurisdiction risk, and systemic resilience planning. The framework provides a structured pathway for sovereign governments and Tier-1 financial institutions to modernize AI-enabled financial systems without compromising economic stability or strategic autonomy. I. The Structural Transformation of AI-Enabled Financial Systems AI is no longer auxiliary technology within financial services; it is core decision infrastructure. AI models now govern credit underwriting, fraud detection, AML transaction monitoring, liquidity optimization, and algorithmic trading. This transition transforms financial governance into algorithmic governance. Algorithmic Systemic Risk (ASR) emerges when correlated AI models amplify procyclicality, feedback loops intensify market stress, or opaque decision engines create supervisory blind spots. Financial stability now requires governance at the model architecture level. II. Digital Financial Sovereignty Defined Digital Financial Sovereignty refers to the structured capacity of a state to govern AI-enabled financial systems, maintain jurisdictional clarity over financial data, manage cross-border regulatory exposure, and preserve systemic resilience under geopolitical pressure. It is not isolationism. Rather, it is strategic autonomy within interdependence. Sovereignty in the digital financial era requires institutional capacity, cross-border intelligence, and resilient infrastructure architecture. III. D-FSRA™ Framework Overview The Digital Financial Sovereignty & AI Risk Architecture (D-FSRA™) is structured across five pillars: 1. AI Financial Model Governance 2. Data Sovereignty & Jurisdictional Exposure Mapping 3. Cross-Border Regulatory Risk Architecture 4. Cyber & Systemic Resilience Design 5. Institutional Governance Capacity & Oversight These pillars operate as an integrated system, ensuring that sovereign and institutional governance aligns with evolving AI deployment realities. IV. Sovereign Deployment Model At the sovereign level, D-FSRA™ supports ministries of finance, central banks, and digital economy authorities in designing AI governance frameworks, supervisory modernization pathways, and national digital financial risk maps. Implementation follows a three-phase model: Diagnostic Assessment, Architecture Design, and Institutional Integration."
  },
  {
    "title": "Integrating Artificial Intelligence into a Multimodal Learning Framework",
    "url": "https://doi.org/10.4324/9781003665472-7",
    "date": "2026-02-20",
    "content": "This chapter proposes and validates an AI-integrated Multimodal Learning Framework for ESL academic writing that blends linguistic, visual, audio, spatial, and gestural modes with AI support. Grounded in the New London Group’s multiliteracies, Sociocultural/Social Constructivist perspectives, and the Process Writing model, the study follows a Design–Development–Research (DDR Type 2) pathway across three phases: framework design, expert validation, and empirical verification. Using the Fuzzy Delphi Method with domain experts, the Integrating Artificial Intelligence (IAI) construct was endorsed (nine refined items retained), highlighting roles for adaptive guidance, analytics, virtual assistants, content recommendation, academic integrity checks, educator professional development, and ethical safeguards. A pilot with ESL foundation students and a subsequent large-scale Rasch analysis established reliability, item fit, and construct coherence, confirming the framework’s practicality for real classrooms. Focus-group data triangulated quantitative results, showing perceived gains in clarity, coherence, and self-regulated revision via timely AI feedback. The chapter offers implementable guidance for teachers, curriculum designers, and policymakers seeking sustainable, ethical AI adoption in writing instruction and sets a research agenda for adaptive pathways and capacity building."
  },
  {
    "title": "AI-Augmented Authenticity: Multimodal Artificial Intelligence and Trust Formation in Cultural Consumer Evaluation",
    "url": "https://doi.org/10.3390/world7020030",
    "date": "2026-02-20",
    "content": "This study examines how artificial intelligence (AI) contributes to contemporary processes of authenticity evaluation by functioning as a multimodal diagnostic cue in consumer decision-making. Drawing on survey data collected from 468 visitors at Terra Madre Salone del Gusto in Turin, Italy, the study tests a structural model comprising five latent constructs: Authenticity Trust, Perceived AI Usefulness and Diagnosticity, Multimodal Value, User Engagement, and Behavioural Intentions. The findings indicate that heritage-based and institutional authenticity cues remain foundational in consumers’ evaluations, but are increasingly associated with interaction with AI-supported information perceived as credible and diagnostically informative. Multimodal inputs—particularly the integration of textual, visual, and auditory narratives—are positively associated with perceived multimodal value and user engagement within AI-supported evaluation. Experiential enjoyment during interaction with the AI system is positively associated with behavioural intentions to adopt AI-supported evaluation tools, while behavioural intentions encompass both adoption readiness and a stated willingness to pay a premium for products perceived as authentic. Although the use of a convenience sample limits generalisability, the results highlight the broader potential of multimodal AI systems to enhance perceived diagnostic clarity and evaluative confidence in complex cultural and consumer environments. Conceptually, the study advances the notion of augmented authenticity, defined as a hybrid evaluative process in which tradition-based trust mechanisms are interpreted in relation to perceived AI diagnosticity and multimodal coherence. By situating AI within culturally embedded processes of meaning-making rather than purely instrumental evaluation, the findings contribute to interdisciplinary debates on technology-supported trust processes, consumer judgement, and the societal implications of AI-supported decision-making."
  },
  {
    "title": "From frustration to satisfaction: the effectiveness of chatbot communication styles in service recovery",
    "url": "https://doi.org/10.1108/jhtt-03-2025-0227",
    "date": "2026-02-20",
    "content": "Purpose Given the importance of chatbots in service failures recoveries, this paper aims to explore how chatbots, powered by advanced technologies to handle customer complaints with personalized and contextual responses. This study investigates the impact of artificial intelligence (AI) chatbot communication styles and blame attributions on customer perceptions of re-patronage intentions in airline service context. Design/methodology/approach A 2 (chatbot interaction style: social vs task-oriented) × 2 (blame attribution: high vs low) between-subject experimental design was conducted, data were collected from 382 participants (inspired by theories of social penetration, attribution and perceived justice). ANOVA and structural equation modeling was used to test the hypotheses. Findings This study demonstrates that blame attribution in chatbot-mediated service recovery critically shapes perceived service climate, whereas emoji implementation (social-oriented communication) provides limited value. Customer satisfaction areareth chatbot services is positively impacted by perceived justice, which also significantly increases re-patronage intentions. Furthermore, thoughtful chatbot design enhances service recovery efficacy, elevates satisfaction and strengthens loyalty, collectively optimizing customer experiences for airline operators. Originality/value This research pioneers a transformative paradigm in tourism service recovery, demonstrating that contextually calibrated communication substantially elevates perceived service climate across high and low blame-attribution failures. These findings deliver actionable frameworks for industry practitioners: Advanced chatbot systems optimize customer experiences, enhance operational efficiency and strengthen service recovery performance through dynamically calibrated communication strategies aligned with failure severity and attribution contexts."
  },
  {
    "title": "AGI Architecture Based on Self-Expanding Temporal Hypergraphs for Critical Structure Encoding",
    "url": "https://doi.org/10.5281/zenodo.18627168",
    "date": "2026-02-20",
    "content": "Building upon the structuralist framework established in the previous work, Reasoning as Structure-Preserving Transformation, this paper further explores the possibility of a semantics-independent reasoning architecture: Self-Expanding Temporal Hypergraphs for Critical Structure Encoding. We postulate that reasoning can be represented as a dynamical system akin to cellular automata or the evolution of identical particles, fundamentally characterized by information compression and structure-preserving transformations. The primary discussions of this paper include:(1) Axiomatic Representation: Referencing the ideas of category theory, reformulate formalized mathematics, and code logic into hypergraphs, where objects are recursively defined subgraphs;(2) Structural Invariants: Concretizing the concept of \"invariants\"—substructures that remain stable under permissible perturbations (e.g., reordering, local replacement) in the reasoning space—and discussing the feasibility of treating them as fundamental units of reasoning;(3) Reinforcement Learning: Investigating potential ways for applying reinforcement learning (such as AlphaZero-like algorithms) within continuously growing representation spaces, especially strategies for search and structure evaluation amidst the expansion of both embedding spaces and neural networks;(4) Unification of Philosophy and Application: Reflecting on the potential relationship between intelligence and cosmic evolution, and envisioning the application prospects of this architecture in Intermediate Representation (IR) layer code reconstruction, automated theorem discovery, and enhancing LLM reasoning. This study aims to provide preliminary philosophical support for connecting mathematics, physics, and artificial intelligence."
  },
  {
    "title": "AI-integrated smartwatch monitoring for early detection of stroke and hemorrhage: A systematic review",
    "url": "https://doi.org/10.1097/md.0000000000047775",
    "date": "2026-02-20",
    "content": "Background: Early detection of cerebrovascular events (subarachnoid hemorrhage [SAH], ischemic stroke, intracranial hemorrhage [ICH]) is critical for timely intervention. Emerging smartwatches with multiple sensors and artificial intelligence (AI) algorithms offer potential for real-time detection of acute neurological events. We performed a systematic review (2010–2025) per the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 guidelines to identify published studies on AI-enabled smartwatches for detecting SAH, ischemic stroke, and ICH. Methods: We searched multiple databases from January 2010 through May 2025 using combinations of smartwatch OR wearable , stroke OR SAH OR ICH , and AI OR machine learning OR deep learning . Risk of bias was assessed using the Quality Assessment of Diagnostic Accuracy Studies-2 tool for diagnostic accuracy studies. Results: The search yielded 3 eligible studies on ischemic stroke detection by wearable accelerometers with AI. The stroke studies (n = 3) used bilateral wrist/arm accelerometers to detect unilateral motor deficits. Deep learning models achieved high diagnostic accuracy (area under the receiver operating characteristic curve 0.95–0.99) for detecting acute stroke symptoms. One study reported a median detection time of 15 to 29 minutes after stroke onset, depending on the false alarm threshold. A feasibility trial (STROKE ALARM) using accelerometer bands and a smartphone app triggered frequent false alarms without observed strokes. Conclusion: Wearable technology with AI shows promise for ischemic stroke symptom detection, but there is a critical gap in SAH and ICH detection. Challenges include sensor accuracy, false alarms, and algorithm generalizability. We propose a conceptual multisensory model integrating heart rate (electrocardiogram/photoplethysmography), blood pressure surrogates, and motion data into an AI pipeline for future smartwatch systems, which can lead to the detection of stroke/SAH, might show promise in other brain pathologies."
  },
  {
    "title": "Ethical and Regulatory Aspects of AI in Industry 5.0",
    "url": "https://doi.org/10.1201/9781003596479-12",
    "date": "2026-02-20",
    "content": "Industry 5.0 extends beyond the automation-driven focus of Industry 4.0 by emphasizing a human-centered approach that integrates technology with sustainability, ethical responsibility, and worker well-being. Rather than solely prioritizing efficiency, Industry 5.0 seeks to harmonize artificial intelligence (AI) with human creativity, ensuring innovation benefits both society and the environment."
  },
  {
    "title": "HR strategies for Gen-Z and Millenial",
    "url": "https://doi.org/10.5281/zenodo.18711255",
    "date": "2026-02-20",
    "content": "Abstract The workforce is rapidly changing due to the global population being made up of more Generations Y and Z than Baby Boomers by 2030 (58% globally). Due to the demographic changes, there has been a shift to an employee-centred approach to management, which will cease being \"command and control\" and will have a flexible, purpose-based approach. Research has shown that organisations need to use intuitive digital tools and artificial intelligence to provide a digital native experience for the younger generations. Additionally, to remain engaged with younger workers, organisations must develop a culture of continuous learning and provide frequent and constructive feedback. Research has shown that the type of HR practices utilised by an organisation are a critical factor in the retention of younger workers, with motivation being an important mediating factor. Key areas of distinction between Generations Y and Z are that while Generation Y prefers collaborative brainstorming, Generation Z prefers to complete tasks independently and communicate directly. Ethical awareness and a focus on social responsibility are also important factors for these generations, as they want to work for organisations that align with their values. This provides organisations with a significant competitive advantage in attracting and retaining Generation Y and Z workers, as the success of an organisation in recruiting and retaining these workers will depend on its cultural authenticity and transparency in leadership. By adopting a modern workplace culture and the frameworks associated with it, organisations are positioned to bridge the generational divide within the workforce and will sustain long-term viability."
  },
  {
    "title": "Role of Artificial Intelligence (AI) in Public Health, Education and Research: An Analysis",
    "url": "https://doi.org/10.5281/zenodo.18707531",
    "date": "2026-02-20",
    "content": "Artificial intelligence is transforming scientific research across disciplines by automatic data analysis, hypothesis generation, experimentation, and scholarly communication. The twentieth century was an era in which industrialization and automation transformed the external structure of human life, but also profoundly affected its inner imagination, sensitivity, and creative consciousness. AI-driven tools now aid in tasks as diverse as discovering new materials, simulating physical systems and analysing social data. The rapid growth of AI has created a virtuous cycle between computational innovation and scientific discovery. Artificial intelligence is poised to transform science through ground breaking approaches with far reaching societal implications, while challenging scientific problems in turn push AI development."
  },
  {
    "title": "Thermodynamic Color Reasoning (TCR): A Chromatic Grammar for Thermodynamic and Post-Symbolic AI Systems",
    "url": "https://doi.org/10.5281/zenodo.18717197",
    "date": "2026-02-20",
    "content": "Thermodynamic Color Reasoning (TCR) formalizes color as a primary, non-symbolic semantic layer for reasoning, alignment and coordination across human cognition, artificial intelligence and world-scale systems. As part of the Ambient Era Canon, TCR defines chromatic states as thermodynamic operators governing agency, stability, transition and ambient integration beyond linguistic representations."
  },
  {
    "title": "Models for predicting the risk of developing postoperative complications and mortality in esophageal cancer surgery",
    "url": "https://doi.org/10.17116/onkolog20261501170",
    "date": "2026-02-20",
    "content": "Today, maximum effectiveness in treating patients with esophageal cancer is achieved by reducing surgical complications and mortality. Proper selection of patients for surgical treatment is the most promising way to reduce complications and mortality. A clear understanding of the relationship between various risk factors and postoperative complications in the future will allow for a more careful approach to patient selection, counseling, and ultimately, high-quality preparation for surgery. Over the past few decades, many scales have been proposed in general surgery and oncosurgery to predict mortality. Significant progress has been made in developing models through the improvement of mathematical algorithms, automation of information systems, and the use of artificial intelligence systems, but the ideal tool does not yet exist. In the presented literature review, a search was conducted for the most valid preoperative scales for predicting the risks of postoperative mortality in patients after esophagectomy."
  },
  {
    "title": "Anesthetic Management in Geriatric and Comorbid Patients: Clinical Challenges, Risk Stratification, and Outcome-Oriented Strategies",
    "url": "https://doi.org/10.5281/zenodo.18708603",
    "date": "2026-02-20",
    "content": "The increased life expectancy across the world has resulted in an increasing number of geriatric patients attending surgical procedures with a multitude of comorbidities. Advancing physiological changes, changes in pharmacodynamics and pharmacokinetics, cognitive frailty, and riskiness in the perioperative period are all linked to ageing. Additional complications such as multimorbidity and polypharmacy make these changes very difficult to deal with during anaesthetic management. These are the outcome-based factors of anaesthetic care in geriatric and comorbid patients that have been thoroughly presented in this article. It references anesthesiology, geriatrics, perioperative medicine, and public health based evidence in assessing age-related physiological alterations, risk stratification models, choice of anaesthetic technique, issues related to intraoperative management, and optimization of postoperative outcomes. The cognitive complications, frailty, and patient-centred outcomes are given a special emphasis. New opportunities of digital health, artificial intelligence, and precision medicine in geriatric anaesthesia are also explained. The article concludes that the outcome-oriented, multidisciplinary, and individualised approach to geriatric and comorbid patients perioperative care is safe and effective."
  }
]
