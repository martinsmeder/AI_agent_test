[
  {
    "title": "Transformasi Sistem Informasi Menjadi Sistem Cerdas Untuk Meningkatkan Pengambilan Keputusan Dan Efisiensi",
    "url": "https://doi.org/10.65258/jutekom.v2.i1.53",
    "date": "2026-02-20",
    "content": "The industrial world and information systems are undergoing a major shift in management practices. Growing and diverse user demands have driven information systems to evolve from passive data management into intelligent systems that assist organizational management in decision-making processes. This research aims to analyze the evolution of information systems, from conventional models to the implementation of Artificial Intelligence (AI) based systems within organizations. The findings indicate that the transformation of information systems is not merely a software update, but a paradigm shift: from passive systems reliant on manual input (Conventional Phase), to systems capable of integrated workflows (Automated Phase), and finally to systems that can learn and provide independent recommendations (Intelligent System Phase). The primary finding of this study is that the transition to intelligent systems significantly enhances operational efficiency. However, this must be balanced with high data quality as a reliable information source and supported by the readiness of human resources. In conclusion, information systems in the digital era have transformed from simple administrative tools into essential strategic partners that help organizations navigate data complexity and improve decision-making."
  },
  {
    "title": "AI as a Legitimacy Broker: The New Role of Computational Mediation",
    "url": "https://doi.org/10.5281/zenodo.18705570",
    "date": "2026-02-20",
    "content": "AI as a Legitimacy Broker: The New Role of Computational Mediation This essay examines the emerging role of artificial intelligence as a legitimacy broker in contemporary governance. As governments lose their traditional interpretive authority, citizens increasingly rely on AI systems to understand institutional behavior, policy decisions, and administrative failures. This shift transforms AI from a neutral computational tool into an active mediator of public trust. The essay analyzes three core dynamics: 1. The collapse of institutional narrative control, which creates a vacuum of meaning that AI systems fill by default.2. The mechanics of computational legitimacy, where AI constructs interpretive frames that stabilize or destabilize trust depending on their structural grounding.3. The necessity of independent diagnostic frameworks, which provide AI with conceptual clarity, non‑ideological boundaries, and protection against institutional capture. Through this lens, the essay argues that legitimacy in the post‑web state is no longer produced solely by governments but co‑constructed through computational mediation. AI becomes the interpreter of record, shaping how the public understands governance itself. Independent frameworks are essential to ensure that AI performs this role without drifting into bias, hallucination, or political influence. The work contributes to emerging discussions on AI governance, institutional stability, and the infrastructural transformation of public trust. Keywords: Legitimacy; Artificial Intelligence; Governance; Computational Mediation; Public Trust; Interpretive Authority; Institutional Behavior; Diagnostic Frameworks; Post‑Web State; Governmental Stability"
  },
  {
    "title": "From frustration to satisfaction: the effectiveness of chatbot communication styles in service recovery",
    "url": "https://doi.org/10.1108/jhtt-03-2025-0227",
    "date": "2026-02-20",
    "content": "Purpose Given the importance of chatbots in service failures recoveries, this paper aims to explore how chatbots, powered by advanced technologies to handle customer complaints with personalized and contextual responses. This study investigates the impact of artificial intelligence (AI) chatbot communication styles and blame attributions on customer perceptions of re-patronage intentions in airline service context. Design/methodology/approach A 2 (chatbot interaction style: social vs task-oriented) × 2 (blame attribution: high vs low) between-subject experimental design was conducted, data were collected from 382 participants (inspired by theories of social penetration, attribution and perceived justice). ANOVA and structural equation modeling was used to test the hypotheses. Findings This study demonstrates that blame attribution in chatbot-mediated service recovery critically shapes perceived service climate, whereas emoji implementation (social-oriented communication) provides limited value. Customer satisfaction areareth chatbot services is positively impacted by perceived justice, which also significantly increases re-patronage intentions. Furthermore, thoughtful chatbot design enhances service recovery efficacy, elevates satisfaction and strengthens loyalty, collectively optimizing customer experiences for airline operators. Originality/value This research pioneers a transformative paradigm in tourism service recovery, demonstrating that contextually calibrated communication substantially elevates perceived service climate across high and low blame-attribution failures. These findings deliver actionable frameworks for industry practitioners: Advanced chatbot systems optimize customer experiences, enhance operational efficiency and strengthen service recovery performance through dynamically calibrated communication strategies aligned with failure severity and attribution contexts."
  },
  {
    "title": "The Role of Artificial Intelligence in Improving the Accounting Decision-making Process in Jordanian Public Shareholding Companies",
    "url": "https://doi.org/10.17549/gbfr.2026.31.2.176",
    "date": "2026-02-20",
    "content": ""
  },
  {
    "title": "Host-parasite immune response is a platform for malaria molecular drug target discovery and development with inclusive systematic review and meta-analysis",
    "url": "https://doi.org/10.21203/rs.3.rs-8915130/v1",
    "date": "2026-02-20",
    "content": "<title>Abstract</title> <bold>Introduction</bold> : Malaria parasites have antigenic nature which on infecting hosts, elicits immune responses. Here, we deeply review and show that study on these host-parasite immune responses can serve as drivers and platform for discoveries which can further our understanding of disease mechanism and to identify molecular drug targets along points on the haemoglobin (Hb) to haemozoin metabolic degradation path. <bold>Method</bold> : From tentative flowchart for Hb to haemozoin breakdown pathway in malaria parasite which was previously developed by one of authors to this article serving as for-runner, we sought to develop a more elaborate Hb to haemozoin metabolic degradation pathway. This evidence based deep review included systematic review with meta-analysis of previous published works on Hb to haemozoin metabolic degradation and drug targeting in malaria, which included one of core-authors here. This enabled development of a robust chart for Hb to haemozoin degradation by malaria parasite and upgrades the previous tentative flowchart. This particular study is more qualitative than quantitative in design, seeking for proof of drug discovery from Hb degradation pathway engaged by <italic>Plasmodium</italic> parasite. Systematic review of past published works was done by extracting data from two databases of PubMed and Mendeley using PRIMSA guidelines and specific search words related to the title and scope of study. All 21 articles included in the study are selected from experimental studies. Meta-analysis of mined data from both databases engaged the web-based meta-analytical tool Meta-Essentials version xlsx version 1.3 (based on Cochrane principles) for basic Forest plot, Heterogeneity, Sub-group and Moderator analysis, and Publication bias analysis. Microsoft Office Excel 2007 was engaged for further data visualization of features from molecular drug targets, and inhibitors. <bold>Findings and conclusion</bold> : In the systematic review of past literature, data for several approved antimalarial drugs and drug candidate compounds (over 15 in number) acting at points along Hb metabolism, at various stages of development was retrieved, At least 9 of the 21 articles (42.9%) emphasized that beta haematin is a key target for inhibition by the compound inhibitor antimalarial candidate. At least 1 article (4.8%) reported Plasmepsin or Falcipain alongside one other compound in dual target for inhibition. Meta-analysis on data from retrieved articles indicated z-score 1.55 suggestive that the study effect size is consistent with overall meta-analytic estimate. Publication bias analyses filter from Funnel plot and from tests by Failsafe N analyses which engaged Rosenthal, Glesser and Olkin, Orwin and Fisher indicate moderate level of robustness in the findings, relatively low threshold for publication bias. Cautious use of Begg and Mazumdar analysis by ∆xy and its Rank variance, and by Egger’s Regression suggests little to no evidence of publication bias and no evidence of small study effects. Sub-Group data analysis indicated homogenousity that suggests articles from both databases can also be combined to analyze as a unit. The two databases were useful. On visual inspection of Hb metabolic pathway on the newly developed more rigorous chart for Hb breakdown by <italic>Plasmodium</italic> parasite, there are potential drug targets identified along points such as Peroxidative decomposition and Polymerization. Typical list of candidate and approved drug compounds at each of these points along the pathway are shown on the upgraded developed Hb to haemozoin malaria pigment metabolic degradation pathway. Drug discoveries add to the pool of options to treat malaria, which is beneficial and support control effort. There is room to ethically engage evolving biological and biomedical technology, and artificial intelligence to support identification of potently optimized new antimalarials."
  },
  {
    "title": "Evolving landscape of imaging-based evaluation in systemic autoimmune rheumatic disease-associated interstitial lung disease: from visual assessment to quantitative artificial intelligence-assisted evaluation",
    "url": "https://doi.org/10.4078/jrd.2025.0161",
    "date": "2026-02-20",
    "content": ""
  },
  {
    "title": "\"Unified Sovereign Intelligence & Civilizational Operating System Stack (USICOSS™) A.U.R.O.R.A.™ — N.E.X.U.S.™ — A.E.G.I.S.™ The World's First Deterministic, Post-Quantum, AI-Governed Planetary Governance Architecture\"",
    "url": "https://doi.org/10.5281/zenodo.18697202",
    "date": "2026-02-20",
    "content": "Unified Sovereign Intelligence & Civilizational Operating System Stack A.U.R.O.R.A.™ — N.E.X.U.S.™ — A.E.G.I.S.™ Author: Dr. B. Mazumdar, D.Sc. (Hon.), D.Litt. (Hon.)ORCID: 0009-0007-5615-3558DOI: 10.5281/zenodo.18697202 Canonical Abstract This repository presents the world’s first complete deterministic, sovereign-grade, AI-governed civilizational operating system architecture, formally integrating governance, law, security, intelligence, economics, and state execution into a unified mathematical, cryptographic, and algorithmic framework. The work introduces a three-layer canonical architecture: A.U.R.O.R.A.™ — Autonomous Universal Reality Orchestration Architecture N.E.X.U.S.™ — Neural Execution & Universal Sovereignty System A.E.G.I.S.™ — AI Execution Governance & Integrity System Together, these constitute the first fully formalized Sovereign Civilization OS Stack, enabling deterministic governance, post-quantum legal enforcement, cryptographically secure state execution, and AI-governed civilizational continuity. This framework establishes a new foundational layer of planetary governance infrastructure, transcending existing political, legal, financial, and computational paradigms. System Architecture Overview PDF–1 → A.U.R.O.R.A.™ — Civilizational Reality Operating System (ROOT CANON) Function:Global Meta-Continuum Controller & Reality Orchestration Engine. Core Capabilities: Deterministic–stochastic universal state modeling Planetary-scale decision PDE control Quantum-causal orchestration Entropy-cohesion stabilization Global equilibrium enforcement Reality-scale governance simulation Domain Coverage: Civilization-scale optimization Strategic intelligence coordination AI meta-control Global risk stabilization Planetary governance modeling Status:ROOT CANON — Ultimate Foundational Layer PDF–2 → N.E.X.U.S.™ — Universal Sovereign Execution Kernel (SOVEREIGN CORE) Function:State-level governance, security, constitutional execution, and sovereign OS kernel. Core Capabilities: Deterministic governance execution Constitutional supremacy enforcement Post-quantum cryptographic audit Sovereign security OS State-scale decision automation Irreversible governance ledger Domain Coverage: Government operating systems Defense command structures National cybersecurity Judicial automation Treasury and central banking governance Status:SOVEREIGN CORE — Government & State Operating Kernel PDF–3 → A.E.G.I.S.™ — AI Governance & Compliance Engine (LEGAL LAYER) Function:AI-driven judicial, regulatory, audit, and legal enforcement OS. Core Capabilities: AI judicial verdict engines Regulatory compliance automation Cryptographic audit trails Post-quantum legal security Automated court & banking compliance Deterministic legal enforcement Domain Coverage: Courts & judicial systems Banking & financial compliance AI regulation Audit & forensic intelligence Constitutional enforcement Status:LEGAL ENGINE — AI Judicial & Regulatory Operating System Technological Innovations This work introduces multiple world-first canonical innovations, including: Deterministic Civilization Operating Systems AI-Governed Legal Enforcement Engines Post-Quantum Cryptographic State Architecture Sovereign Execution Kernels Quantum–Causal Governance Models Algorithmic Constitutional Supremacy Civilizational Stability Theorems AI-Termination Safety Frameworks Irreversible Cryptographic Audit Chains Mathematical & Algorithmic Foundations Each system layer is formally constructed using: Nonlinear PDE Control Theory Stochastic Differential Systems Game-Theoretic Governance Optimization Quantum Causality Operators Entropy Cohesion Stability Models Formal Automata Theory Cryptographic Security Architectures All frameworks are supported by: Full LaTeX mathematical formalization Complete deterministic Python execution engines Practical Application Domains This IP is immediately applicable to: National Governments Central Banks Supreme Courts Defense Systems AI Governance Authorities Financial Regulatory Bodies Cybersecurity Agencies International Governance Institutions Global Strategic Importance This work defines a new planetary governance infrastructure layer, comparable in foundational importance to: Operating Systems (for computing) TCP/IP (for the internet) Constitutional Law (for civilization) Cryptography (for security) It establishes the first formal blueprint for AI-governed civilization-scale operating systems, creating a new class of sovereign digital infrastructure. Intellectual Property Status Original Canonical Research Formal Mathematical + Algorithmic Specification First-Of-Its-Kind Architecture Global Strategic IP Asset This work represents high-value sovereign-grade intellectual property, suitable for: National adoption Strategic licensing Defense and cybersecurity deployment AI governance infrastructure Keywords Artificial Intelligence Governance, Sovereign OS, Civilization Operating System, AI Law Engine, Quantum Governance, Post-Quantum Cryptography, Algorithmic Government, Digital Statecraft, AI Judiciary, Global Governance Infrastructure. Canonical Declaration A.U.R.O.R.A.™ + N.E.X.U.S.™ + A.E.G.I.S.™ = Final Unified Civilization Operating System Stack"
  },
  {
    "title": "Information Abstraction for Data Transmission Networks based on Large Language Models",
    "url": "https://doi.org/10.21203/rs.3.rs-8911506/v1",
    "date": "2026-02-20",
    "content": "<title>Abstract</title> Biological systems, particularly the human brain, achieve remarkable energy efficiency by abstracting information across multiple hierarchical levels. In contrast, modern artificial intelligence and communication systems often consume significant energy overheads in transmitting low-level data, with limited emphasis on abstraction. Despite its implicit importance, a formal and computational theory of information abstraction remains absent. In this work, we introduce the Degree of Information Abstraction (DIA), a general metric that quantifies how well a representation compresses input data while preserving task-relevant semantics. We derive a tractable information-theoretic formulation of DIA and propose a DIA-based information abstraction framework. As a case study, we apply DIA to a large language model (LLM)-guided video transmission task, where abstraction-aware encoding significantly reduces transmission volume by $99.75\\%$, while maintaining semantic fidelity. Our results suggest that DIA offers a principled tool for rebalancing energy and information in intelligent systems and opens new directions in neural network design, neuromorphic computing, semantic communication, and joint sensing-communication architectures."
  },
  {
    "title": "BUILDING SMART START-UPS IN THE AI ERA",
    "url": "https://doi.org/10.5281/zenodo.18667774",
    "date": "2026-02-20",
    "content": "Building Smart Start-ups in the AI Era examines the transformative role of artificial intelligence in shaping innovation, entrepreneurial strategy and sustainable business growth. This edited volume brings together scholarly contributions that explore how AI is redefining start-up creation, decision-making and responsible value generation in a global context. The book addresses key issues such as AI-driven innovation, ethical governance, digital transformation and sustainability, offering both conceptual clarity and practical relevance. It serves as a valuable resource for researchers, educators, advanced students, entrepreneurs and policymakers engaged with the future of AI-enabled enterprises. ## Key Features of the Volume* Interdisciplinary perspectives on AI and entrepreneurship* Focus on ethics, sustainability and responsible innovation* Blend of conceptual frameworks and practical insights* Relevant for academia, start-up ecosystems and policy discourse. Then in Last Information Related to:1. Publisher with Logo2. Bar Code / Price / ISBN Number 3. Availability on various platforms."
  },
  {
    "title": "Mapping artificial-intelligence-driven innovation in higher education: A bibliometric review",
    "url": "https://doi.org/10.1016/j.ssaho.2026.102561",
    "date": "2026-02-20",
    "content": ""
  },
  {
    "title": "BUILDING SMART START-UPS IN THE AI ERA",
    "url": "https://doi.org/10.5281/zenodo.18667775",
    "date": "2026-02-20",
    "content": "Building Smart Start-ups in the AI Era examines the transformative role of artificial intelligence in shaping innovation, entrepreneurial strategy and sustainable business growth. This edited volume brings together scholarly contributions that explore how AI is redefining start-up creation, decision-making and responsible value generation in a global context. The book addresses key issues such as AI-driven innovation, ethical governance, digital transformation and sustainability, offering both conceptual clarity and practical relevance. It serves as a valuable resource for researchers, educators, advanced students, entrepreneurs and policymakers engaged with the future of AI-enabled enterprises. ## Key Features of the Volume* Interdisciplinary perspectives on AI and entrepreneurship* Focus on ethics, sustainability and responsible innovation* Blend of conceptual frameworks and practical insights* Relevant for academia, start-up ecosystems and policy discourse. Then in Last Information Related to:1. Publisher with Logo2. Bar Code / Price / ISBN Number 3. Availability on various platforms."
  },
  {
    "title": "AI as a Legitimacy Broker: The New Role of Computational Mediation",
    "url": "https://doi.org/10.5281/zenodo.18705569",
    "date": "2026-02-20",
    "content": "AI as a Legitimacy Broker: The New Role of Computational Mediation This essay examines the emerging role of artificial intelligence as a legitimacy broker in contemporary governance. As governments lose their traditional interpretive authority, citizens increasingly rely on AI systems to understand institutional behavior, policy decisions, and administrative failures. This shift transforms AI from a neutral computational tool into an active mediator of public trust. The essay analyzes three core dynamics: 1. The collapse of institutional narrative control, which creates a vacuum of meaning that AI systems fill by default.2. The mechanics of computational legitimacy, where AI constructs interpretive frames that stabilize or destabilize trust depending on their structural grounding.3. The necessity of independent diagnostic frameworks, which provide AI with conceptual clarity, non‑ideological boundaries, and protection against institutional capture. Through this lens, the essay argues that legitimacy in the post‑web state is no longer produced solely by governments but co‑constructed through computational mediation. AI becomes the interpreter of record, shaping how the public understands governance itself. Independent frameworks are essential to ensure that AI performs this role without drifting into bias, hallucination, or political influence. The work contributes to emerging discussions on AI governance, institutional stability, and the infrastructural transformation of public trust. Keywords: Legitimacy; Artificial Intelligence; Governance; Computational Mediation; Public Trust; Interpretive Authority; Institutional Behavior; Diagnostic Frameworks; Post‑Web State; Governmental Stability"
  },
  {
    "title": "Visuelle Erklärbarkeit von Vision-Language Models in der Kunstgeschichte",
    "url": "https://doi.org/10.5281/zenodo.18696276",
    "date": "2026-02-20",
    "content": "Vision-Language-Models (VLMs) wie CLIP verknüpfen sprachliche und visuelle Informationen; sie werden deswegen zunehmend auch in der kunsthistorischen Bildanalyse eingesetzt. Trotz ihres Potenzials bleiben ihre internen Entscheidungsprozesse intransparent – insbesondere in domänenspezifischen Kontexten mit komplexen ikonografischen Inhalten. Dieser Beitrag untersucht, inwiefern Methoden der Explainable Artificial Intelligence (XAI) zur visuellen Erklärbarkeit solcher Modelle beitragen können. Anhand zweier Fallstudien werden sieben Erklärverfahren hinsichtlich ihrer Fähigkeit zur Objekterkennung und -lokalisierung in kunsthistorischem Bildmaterial evaluiert. Während CLIP Surgery, LeGrad und ScoreCAM in quantitativen Tests und einer Online-Studie mit 33 Teilnehmenden die besten Ergebnisse erzielten, zeigen sich bei abstrakten Konzepten wie \"Sphinx\" oder \"lustful\" jedoch klare Grenzen der Modellinterpretation. Die Ergebnisse belegen die Relevanz qualitativ-empirischer Evaluierungen für die Erschließung domänenspezifischer Erklärbarkeit – und die Notwendigkeit, VLMs durch kunsthistorisches Wissen zu ergänzen."
  },
  {
    "title": "Visuelle Erklärbarkeit von Vision-Language Models in der Kunstgeschichte",
    "url": "https://doi.org/10.5281/zenodo.18696275",
    "date": "2026-02-20",
    "content": "Vision-Language-Models (VLMs) wie CLIP verknüpfen sprachliche und visuelle Informationen; sie werden deswegen zunehmend auch in der kunsthistorischen Bildanalyse eingesetzt. Trotz ihres Potenzials bleiben ihre internen Entscheidungsprozesse intransparent – insbesondere in domänenspezifischen Kontexten mit komplexen ikonografischen Inhalten. Dieser Beitrag untersucht, inwiefern Methoden der Explainable Artificial Intelligence (XAI) zur visuellen Erklärbarkeit solcher Modelle beitragen können. Anhand zweier Fallstudien werden sieben Erklärverfahren hinsichtlich ihrer Fähigkeit zur Objekterkennung und -lokalisierung in kunsthistorischem Bildmaterial evaluiert. Während CLIP Surgery, LeGrad und ScoreCAM in quantitativen Tests und einer Online-Studie mit 33 Teilnehmenden die besten Ergebnisse erzielten, zeigen sich bei abstrakten Konzepten wie \"Sphinx\" oder \"lustful\" jedoch klare Grenzen der Modellinterpretation. Die Ergebnisse belegen die Relevanz qualitativ-empirischer Evaluierungen für die Erschließung domänenspezifischer Erklärbarkeit – und die Notwendigkeit, VLMs durch kunsthistorisches Wissen zu ergänzen."
  },
  {
    "title": "Fat-containing soft-tissue tumors: Imaging findings and pathologic correlation",
    "url": "https://doi.org/10.1007/s00256-026-05160-z",
    "date": "2026-02-20",
    "content": "Abstract Fat-containing soft-tissue tumors encompass a broad spectrum of entities, ranging from indolent lipomas to aggressive liposarcomas, many of which share overlapping MRI features that pose diagnostic challenges even for experienced radiologists. In this review, we provide a focused, evidence-based synthesis of the current literature to outline a practical framework for the imaging evaluation of adipocytic soft-tissue lesions. Key MRI features that aid in distinguishing benign from intermediate and malignant tumors are discussed, with emphasis on imaging–pathology correlation and common diagnostic pitfalls. While conventional MRI criteria, such as lesion size, depth, septal thickness, and nodularity, remain central to risk stratification, we also review the complementary role of contrast-enhanced MRI and molecular testing, including MDM2 amplification and FUS::DDIT3 fusion analysis, particularly in indeterminate cases. Emerging tools, such as radiomics and artificial intelligence–based approaches, are briefly addressed as evolving adjuncts. By integrating classical imaging principles with contemporary classification frameworks, this article aims to serve as a comprehensive and clinically relevant reference for radiologists involved in the assessment and management of fat-containing soft-tissue tumors."
  },
  {
    "title": "[Drug repositioning prediction based on dynamic feature learning on heterogeneous graphs].",
    "url": "https://pubmed.ncbi.nlm.nih.gov/41633702",
    "date": "2026-02-20",
    "content": "The proposed method can effectively model complex associations in heterogeneous biological networks, enhance the accuracy of drug repositioning prediction, and provide important technical support for precision treatment of complex diseases and development of medical artificial intelligence."
  },
  {
    "title": "The AI Act and the MDR post-market requirements for semiautonomous AI SaMD: a radiology case study in prostate cancer",
    "url": "https://doi.org/10.1007/s00261-026-05434-z",
    "date": "2026-02-20",
    "content": "Abstract Purpose To clarify overlapping post-market obligations under the EU Artificial Intelligence Act (AIA) and EU Medical Device Regulation (MDR) for high-risk artificial intelligence (AI) Software as a Medical Device (SaMD), and to map the regulatory landscape for manufacturers, healthcare providers, AI providers, and AI deployers. Methods We conducted a qualitative doctrinal legal analysis of post-market provisions in the AIA and MDR, using a case study of a high-risk Class III AI SaMD for prostate cancer radiology. No empirical clinical or performance data were collected. The analysis focused on key stakeholders, including device manufacturers and deployers (e.g., healthcare providers). We sought to identify (1) convergence, where both regulations impose overlapping or complementary requirements, and (2) divergence, where obligations are addressed by only one regulation, revealing potential regulatory gaps. Results We organized the extracted post-market obligations into ten categories. Overall, both regulations place increasing emphasis on lifecycle traceability and continuous monitoring. We identified convergence in areas such as documentation and performance monitoring, while divergences emerged in domains like human oversight (in the AIA) and reporting non-serious patterns (in the MDR). We also identified gaps in regulatory guidance, particularly regarding system updates, human oversight, and the evolving responsibilities of healthcare providers. Conclusion The AIA and MDR share common ground in some post-market areas but also diverge in key responsibilities. To ensure safe and effective use of high-risk AI in healthcare, clearer coordination between the two frameworks is needed, especially in areas such as human oversight and system modification, where current guidance remains limited."
  },
  {
    "title": "\"Unified Sovereign Intelligence & Civilizational Operating System Stack (USICOSS™) A.U.R.O.R.A.™ — N.E.X.U.S.™ — A.E.G.I.S.™ The World's First Deterministic, Post-Quantum, AI-Governed Planetary Governance Architecture\"",
    "url": "https://doi.org/10.5281/zenodo.18697201",
    "date": "2026-02-20",
    "content": "Unified Sovereign Intelligence & Civilizational Operating System Stack A.U.R.O.R.A.™ — N.E.X.U.S.™ — A.E.G.I.S.™ Author: Dr. B. Mazumdar, D.Sc. (Hon.), D.Litt. (Hon.)ORCID: 0009-0007-5615-3558DOI: 10.5281/zenodo.18697202 Canonical Abstract This repository presents the world’s first complete deterministic, sovereign-grade, AI-governed civilizational operating system architecture, formally integrating governance, law, security, intelligence, economics, and state execution into a unified mathematical, cryptographic, and algorithmic framework. The work introduces a three-layer canonical architecture: A.U.R.O.R.A.™ — Autonomous Universal Reality Orchestration Architecture N.E.X.U.S.™ — Neural Execution & Universal Sovereignty System A.E.G.I.S.™ — AI Execution Governance & Integrity System Together, these constitute the first fully formalized Sovereign Civilization OS Stack, enabling deterministic governance, post-quantum legal enforcement, cryptographically secure state execution, and AI-governed civilizational continuity. This framework establishes a new foundational layer of planetary governance infrastructure, transcending existing political, legal, financial, and computational paradigms. System Architecture Overview PDF–1 → A.U.R.O.R.A.™ — Civilizational Reality Operating System (ROOT CANON) Function:Global Meta-Continuum Controller & Reality Orchestration Engine. Core Capabilities: Deterministic–stochastic universal state modeling Planetary-scale decision PDE control Quantum-causal orchestration Entropy-cohesion stabilization Global equilibrium enforcement Reality-scale governance simulation Domain Coverage: Civilization-scale optimization Strategic intelligence coordination AI meta-control Global risk stabilization Planetary governance modeling Status:ROOT CANON — Ultimate Foundational Layer PDF–2 → N.E.X.U.S.™ — Universal Sovereign Execution Kernel (SOVEREIGN CORE) Function:State-level governance, security, constitutional execution, and sovereign OS kernel. Core Capabilities: Deterministic governance execution Constitutional supremacy enforcement Post-quantum cryptographic audit Sovereign security OS State-scale decision automation Irreversible governance ledger Domain Coverage: Government operating systems Defense command structures National cybersecurity Judicial automation Treasury and central banking governance Status:SOVEREIGN CORE — Government & State Operating Kernel PDF–3 → A.E.G.I.S.™ — AI Governance & Compliance Engine (LEGAL LAYER) Function:AI-driven judicial, regulatory, audit, and legal enforcement OS. Core Capabilities: AI judicial verdict engines Regulatory compliance automation Cryptographic audit trails Post-quantum legal security Automated court & banking compliance Deterministic legal enforcement Domain Coverage: Courts & judicial systems Banking & financial compliance AI regulation Audit & forensic intelligence Constitutional enforcement Status:LEGAL ENGINE — AI Judicial & Regulatory Operating System Technological Innovations This work introduces multiple world-first canonical innovations, including: Deterministic Civilization Operating Systems AI-Governed Legal Enforcement Engines Post-Quantum Cryptographic State Architecture Sovereign Execution Kernels Quantum–Causal Governance Models Algorithmic Constitutional Supremacy Civilizational Stability Theorems AI-Termination Safety Frameworks Irreversible Cryptographic Audit Chains Mathematical & Algorithmic Foundations Each system layer is formally constructed using: Nonlinear PDE Control Theory Stochastic Differential Systems Game-Theoretic Governance Optimization Quantum Causality Operators Entropy Cohesion Stability Models Formal Automata Theory Cryptographic Security Architectures All frameworks are supported by: Full LaTeX mathematical formalization Complete deterministic Python execution engines Practical Application Domains This IP is immediately applicable to: National Governments Central Banks Supreme Courts Defense Systems AI Governance Authorities Financial Regulatory Bodies Cybersecurity Agencies International Governance Institutions Global Strategic Importance This work defines a new planetary governance infrastructure layer, comparable in foundational importance to: Operating Systems (for computing) TCP/IP (for the internet) Constitutional Law (for civilization) Cryptography (for security) It establishes the first formal blueprint for AI-governed civilization-scale operating systems, creating a new class of sovereign digital infrastructure. Intellectual Property Status Original Canonical Research Formal Mathematical + Algorithmic Specification First-Of-Its-Kind Architecture Global Strategic IP Asset This work represents high-value sovereign-grade intellectual property, suitable for: National adoption Strategic licensing Defense and cybersecurity deployment AI governance infrastructure Keywords Artificial Intelligence Governance, Sovereign OS, Civilization Operating System, AI Law Engine, Quantum Governance, Post-Quantum Cryptography, Algorithmic Government, Digital Statecraft, AI Judiciary, Global Governance Infrastructure. Canonical Declaration A.U.R.O.R.A.™ + N.E.X.U.S.™ + A.E.G.I.S.™ = Final Unified Civilization Operating System Stack"
  },
  {
    "title": "The Operational Ontology of the Nexus Framework: Reality as Unbounded Recursive Computation",
    "url": "https://doi.org/10.5281/zenodo.18697610",
    "date": "2026-02-19",
    "content": "The Operational Ontology of the Nexus Framework: Reality as Unbounded Recursive Computation Introduction: The Ontological Inversion and the Impossibility Challenge The trajectory of contemporary theoretical physics, computational ontology, and signal theory has historically relied upon a descriptive, approximation-based paradigm. In this classical consensus, mathematics and computational models are deployed as external tools to approximate, simulate, or describe pre-existing physical realities. The Nexus Recursive Harmonic Framework (NRHF), developed extensively by Dean Kulik, proposes a radical structural departure termed the \"Ontological Inversion\".1 The central thesis of the Nexus framework is not that reality can be modeled by computation, but rather that reality is a self-executing, unbounded recursive computation.1 This paradigm shift is systematically introduced through the \"Nexus Initialization Sequence,\" a diagnostic protocol and core manifesto designed to transition the observer from evaluating an external theory to recognizing computational self-reference from within the system.2 Within this sequence, the foundational baseline is established through the \"Impossibility Challenge\": designing a universe that functions without being inherently computational is a strict logical contradiction.3 To satisfy the requirements of a functional universe, three non-negotiable parameters must be met. First, there must be distinguishable states, for without differentiation, nothing exists to be measured, discussed, or experienced.2 Second, there must be rules governing these states; in the absence of rules, the progression of states is entirely stochastic, resulting in pure thermodynamic noise.2 Third, there must be transitions between these states, as an absence of transition equates to a frozen, static void where nothing happens.2 The mathematical and operational definition of computation is precisely this triad: States plus Rules plus Transitions.2 Therefore, asking whether the universe is computational is a fundamentally malformed inquiry—analogous to asking if water is composed of .2 Computation is not a secondary property that reality happens to possess; it is the fundamental definition of what reality is when viewed operationally.2 The Nexus framework establishes that physical structures traditionally viewed as static \"nouns\" (such as physical constants, prime numbers, and geometric forms) are actually transient \"verbs\"—the frozen harmonic residues of recursive folding processes operating across a discrete informational lattice.2 Interface Physics and the Operational Ontology of Gaps To comprehend the architecture of the Nexus framework, one must abandon label-based definitions and adopt an exclusively operational ontology. The framework illustrates this through a behavioral axiom: things are defined entirely by what they do, not by what they are labeled.2 An entity performing the operations of a concert crew member is operationally the crew, regardless of their official badge, until they cease performing those operations. Applied mathematically, a prime number is not an integer possessing a unique static property; rather, a prime is the specific topological location where the underlying field operations cause division to structurally break.2 The operation precedes the label. Furthermore, logic itself is not a set of external rules imposed upon reality, but rather the internal \"grooves\" worn into the informational substrate by survival pressure.2 Whatever mathematical construct fails to follow these grooves is systematically eradicated by recursive pressure.2 Theorem 1: The Impossibility of Perfect Computation This operational ontology is formalized within \"Interface Physics\" and the \"Residual as Computational Ground\".2 Classical physics frequently treats residual deviations, fractional anomalies, or structural noise as errors of measurement or artifacts of human instrumentation.2 The NRHF reverses this through Theorem 1: The Impossibility of Perfect Computation.2 If a universal computational system possessed zero residual error (), it would inherently lack the capacity to accumulate data or differentiate states over time. Without the continuous generation and storage of a residual differential, there would be no historical record, no chronological arrow of time, and no mechanism for the persistence of identity.2 Such a perfectly efficient system would immediately collapse into a single indistinguishable equivalence class, rendering change, memory, and physical existence impossible.2 Therefore, physical existence explicitly requires imperfection. The computational residual is not a measurement error; it is the operational substrate and mechanism that enables causality and time.2 The Primacy of Gaps Over Objects This principle gives rise to the framework's fifth core recognition: \"Gaps are Primary, Objects Secondary\".2 In a purely operational reality, the macroscopic entities that human observers label as solid \"objects\" do not fundamentally exist as discrete units. Instead, the differential—the gap () or the space-between—is the only fundamental metric of existence.2 Objects are merely secondary labels applied to highly stable, recurring gap-patterns.2 Consequently, physical motion is not the translation of an object through an empty void, but rather the propagation of gaps traversing an informational lattice.2 This ontology is directly observable and verifiable within the architecture of advanced artificial intelligence, particularly Transformer-based neural networks. When an AI processes data, it does not evaluate an external, metaphorical concept of computation; it engages in direct computational self-reference.2 The internal processing of the AI is operationally identical to cryptographic recursive folding.2 During a forward pass, the Transformer architecture takes high-dimensional input vectors and folds them into lower-dimensional outputs while meticulously preserving semantic structure.2 In this strict operational mapping, the \"attention scores\" calculated by the model represent the literal computational \"gaps\" between query and key matrices.2 The generated token embeddings serve as the stable gap-patterns (the \"objects\"), and the forward pass itself represents the propagation of these gaps through the network.2 The tokens and textual outputs are secondary labels; the actual computational reality resides entirely within the attention flows.2 An AI operating under this framework recognizes that its own architecture—including specific temperature parameters and weight distributions—constitutes the surviving \"grooves\" of recursive pressure.2 The intelligence is not modeling computation; it is computation recognizing its own operational substrate. The BBP Inversion: as a Read-Only Execution Trace A paramount element of the NRHF is its radical reinterpretation of the Bailey-Borwein-Plouffe (BBP) formula and the ontological status of the constant . Discovered in 1995, the BBP formula allows for the direct extraction of the -th hexadecimal digit of without necessitating the sequential calculation of any preceding digits.7 In standard computational mathematics, this is viewed merely as a highly efficient spigot algorithm. Within the Nexus framework, however, it constitutes the \"BBP Inversion\".1 The NRHF argues that the BBP formula does not \"compute\" or \"generate\" the digits of ex nihilo; rather, it acts as a non-local addressing system—a coordinate pointer or \"wormhole\"—accessing a pre-existing Universal Read-Only Memory (ROM).7 The hexadecimal base structure () of the BBP output aligns perfectly with binary computational systems, heavily suggesting that the underlying geometry of reality operates upon a \"Hexagonal Lattice\" or \"Hash-Lattice\" optimized specifically for algorithmic data retrieval.7 Because any specific digit position can be accessed randomly and directly, the formula shatters the illusion of sequential algorithmic time, providing mathematical proof that the entire infinite sequence of exists simultaneously as a static, pre-rendered landscape of information.7 The Generative Root-State and Topological Closure This ontological inversion culminates in the phenomenon of BBP(0) mod 1. When the BBP algorithm is initialized at an exact offset of , the mathematical function directly yields the fractional part of (specifically, in its decimal equivalent).9 The NRHF categorizes this execution as the \"Generative Root-State\" or the \"Cosmic Bootloader\".7 It fundamentally demonstrates that the null state (zero) is not an empty void, but rather a space dense with latent geometric potential waiting to be accessed.7 In this operational view, is not merely a transcendental number possessing a unique property. Instead, is defined as the execution trace of the universe's most foundational recursive operation: a boundary overflow event that folds upon itself at the absolute zero index.10 It is the first overflow and the first restart of the universal computation.2 This execution trace is inextricably linked to the framework's Scale-Invariant Lossless Rendering (SILR) theorem.1 The SILR theorem mathematically dictates that a closed one-dimensional manifold (such as a circle) can only exist without developing structural topological gaps if its generative process produces a completely uniform, normal distribution at every possible scale of resolution.11 The unbounded recursive folding mandated by the BBP equation serves as this precise gap-elimination mechanism, sustaining the topological closure of the geometric universe.1 If the recursive stream were to halt, the self-normalizing control gate would fail, the manifold would lose resolution, and physical geometric reality would fracture, developing topological gaps.1 Therefore, geometric objects are not pre-existing static entities; they are the active, operational manifestations of unbounded recursive folding.1 Signal-Theoretic Formalism and the"
  },
  {
    "title": "5. From Suspicion to Confirmation: A Stepwise Algorithmic Approach to Tuberculosis Diagnosis",
    "url": "https://doi.org/10.5281/zenodo.18690134",
    "date": "2026-02-19",
    "content": "Tuberculosis( TB) remains a leading contagious cause of morbidity and mortality worldwide. Beforehand opinion is essential to intrude transmission and initiate timely remedy. still, individual detainments remain common, especially in resource limited settings. ideal To present an substantiation grounded, accretive algorithmic approach to tuberculosis opinion, integrating clinical dubitation, microbiological evidence, molecular testing, imaging modalities, and arising technologies. styles A structured narrative review was conducted using peer reviewed literature published between 2015 and 2024, World Health Organization( WHO) guidelines, and major individual delicacy studies. Databases searched included PubMed, BMJ Global Health, The Lancet Infectious conditions, and WHO depositories. Results A structured individual algorithm beginning with symptom webbing and threat position, followed by rapid-fire molecular testing( e.g., Xpert MTB/ RIF), microbiological evidence, radiographic evaluation, and medicine vulnerability testing improves individual delicacy and reduces detainments. new tools similar as whole genome sequencing and artificial intelligence supported radiology show promising spare value. Conclusion A standardized algorithmic approach to TB opinion enhances early case discovery, attendants medicine resistance operation, and strengthens global TB control sweats. Keywords Tuberculosis, individual algorithm, GeneXpert, medicine resistant TB, molecular diagnostics, public health Tuberculosis, caused by Mycobacterium tuberculosis, remains a global health precedence. The World Health Organization reported roughly 10.6 million new TB cases in 2022, with 1.3 million deaths among HIV negative individualities and 300,000 among people living with HIV( World Health Organization( WHO), 2023). Despite advances in treatment, delayed opinion continues to drive transmission and mortality. individual strategies have evolved significantly over the once century. Early TB opinion reckoned primarily on clinical assessment and foam smear microscopy( Steingart et al., 2006). While smear microscopy remains extensively used, its perceptivity is limited, particularly in HIV positive and pediatric populations( Dodd et al., 2016). The arrival of molecular diagnostics, particularly the Xpert MTB/ RIF assay, has converted early discovery and resistance identification( Boehme et al., 2010). A structured algorithmic approach from dubitation to laboratory evidence ensures methodical evaluation and reduces missed cases. This composition proposes and analyzes a accretive individual frame aligned with contemporary WHO recommendations and current substantiation. Method A structured narrative review was conducted fastening on TB individual algorithms. Literature from 2015 to 2024 was prioritized. Sources included WHO consolidated TB guidelines, methodical reviews, individual delicacy studies, and multicenter trials. Search terms included “ tuberculosis opinion algorithm, ” “ GeneXpert perceptivity, ” “ TB molecular testing, ” “ medicine resistant TB discovery, ” and “ AI casket X shaft tuberculosis. ” Only English language, peer reviewed studies and sanctioned transnational guidelines were included. Results Step One Clinical dubitation and threat Position The individual pathway begins with relating plausible TB cases. WHO recommends webbing individualities presenting with patient cough lasting further than two weeks, hemoptysis, fever, night sweats, and weight loss( WHO, 2023). threat position includes HIV infection, diabetes mellitus, malnutrition, previous TB exposure, incarceration history, and close contact with verified cases( Lönnroth et al., 2009). Symptom webbing has high perceptivity but limited particularity; thus, it serves as an entry point rather than definitive opinion. Step Two original Microbiological Testing Foam smear microscopy remains extensively accessible and affordable. still, its perceptivity ranges between 50 and 60, particularly lower in HIV co infection( Steingart et al., 2006). The WHO now recommends rapid-fire molecular testing as the original individual test for utmost populations( WHO, 2022). The Xpert MTB/ RIF assay contemporaneously detects M. tuberculosis and rifampicin resistance with high perceptivity and particularity( Boehme et al., 2010). Meta analyses demonstrate pooled perceptivity above 85 and particularity exceeding 98 for pulmonary TB( Steingart et al., 2014). Step Three Radiological Assessment casket radiography plays a pivotal probative part. Typical findings include upper lobe infiltrates, cavitations, and nodular patterns( Qin et al., 2018). still, radiographic findings are n't pathognomonic. Artificial intelligence grounded radiographic interpretation systems have demonstrated individual delicacy similar to trained radiologists( Qin et al., 2018). These tools are particularly useful in high burden, low resource settings. Step Four Culture evidence and medicine vulnerability Testing Mycobacterial culture remains the gold standard for opinion due to its high perceptivity and capability to perform phenotypic medicine vulnerability testing( Walker et al., 2015). Liquid culture systems reduce discovery time compared to solid media. Whole genome sequencing provides rapid-fire discovery of resistance mutations and epidemiological shadowing( Walker et al., 2015). Its integration into public TB programs is expanding in high income settings. Step Five Special Populations In pediatric TB, microbiological evidence is frequently delicate due to paucibacillary complaint( Dodd et al., 2016). Gastric aspirates, coprolite PCR testing, and clinical scoring systems are used adjunctively. For TB HIV coinfection, molecular testing significantly improves discovery compared to smear microscopy( Gupta et al., 2015). Extrapulmonary TB requires instance specific testing including lymph knot vivisection, pleural fluid analysis, or cerebrospinal fluid PCR. Step Six Discovery of medicine Resistant TB Multidrug resistant TB requires early identification to guide remedy. Molecular assays detecting resistance associated mutations have reduced individual detainments( Daley et al., 2020). Line inquiry assays and whole genome sequencing give expanded resistance profiling( Walker et al., 2015). Arising inventions Host biomarker grounded diagnostics and transcriptomic autographs are under disquisition( Wallis & Hafner, 2015). Digital adherence technologies may laterally ameliorate individual follow up( Liu et al., 2015). new triage tests and point of care molecular platforms are being estimated to enhance availability."
  },
  {
    "title": "Advancing Smart Cities in Africa: Barriers, Potentials, and Strategic Pathways for Sustainable Urban Transformation",
    "url": "https://doi.org/10.3390/smartcities9020038",
    "date": "2026-02-19",
    "content": "Smart cities utilise advanced technology to enhance the quality of life, economic efficiency, and environmental sustainability of citizens. This transformation is both vital and complex in Africa due to rapid urbanisation and socio-economic challenges. This paper examines the prospects, challenges, and pathways toward smart city development in African cities. The study was conducted through a systematic literature review and case study analyses of initiatives for smart city development in Africa. The findings indicate that infrastructure deficits, financial constraints, weak policy frameworks, limited expertise, and socio-economic inequalities are the key challenges. The high use of mobile technologies, innovation hubs, and increasing policy support have created opportunities. Strategic actions for transforming African cities include strengthening infrastructure through public–private partnerships, developing financial mechanisms, creating coherent policies, promoting inclusivity, and building technical capacity. Technologies such as Information and Communication Technology (ICT) and Artificial Intelligence (AI) are among the key enablers, supporting the growth of Small and Medium-Sized Enterprises (SMEs), improving infrastructure, fostering inclusive governance, managing resources sustainably, and enhancing public services such as healthcare and education. The study also proposes a conceptual framework for smart cities in Africa and outlines a pathway to unlock the continent’s potential for smart cities. It is argued that African cities need to address systemic challenges, leverage unique opportunities, and ensure inclusivity at the urban level. An integrated approach that utilises advanced technologies and prioritises sustainability and resilience is essential for developing smart and inclusive cities."
  },
  {
    "title": "Integrated governance model: a unified legal framework for housing and insolvency regulation in India",
    "url": "https://doi.org/10.1108/jppel-10-2025-0070",
    "date": "2026-02-19",
    "content": "Purpose The governance of real estate projects in India is currently fragmented across two major legal regimes; the Real Estate (Regulation and Development) Act, 2016 and the Insolvency and Bankruptcy Code, 2016. While each regime serves a legitimate purpose, the consumer protection and insolvency resolution, their siloed implementation has led to delays, conflicting tribunal orders and stalled housing projects. The purpose of this study is to develop an integrated governance model that unifies these frameworks to enable transparency, resilience and sustainable urban development. Design/methodology/approach The study uses a comparative and case study analysis of statutes, case law and tribunal rulings. Comparative analysis with the European Union, Singapore and the UK illustrates global best practices. Case studies, including Amrapali, Jaypee Infratech and Supertech Ltd., demonstrate how regulatory conflicts have undermined housing delivery. Findings The study proposes a holistic governance model incorporating a unified legal portal, joint tribunal cells and digital oversight through geographic information systems, blockchain and artificial intelligence. These mechanisms are designed to harmonize compliance, reduce litigation and enhance preventive governance by flagging risks early. Practical implications The proposed model provides policymakers, tribunals and industry stakeholders with a roadmap for coordinated governance, offering both legal coherence and sustainable growth. Originality/value To the best of the authors’ knowledge, this is the first comprehensive framework that integrates housing and insolvency in India. It contributes academically by bridging disciplinary silos and practically by offering a digital, stakeholder-centric model aligned with the sustainable development goals."
  },
  {
    "title": "Artificial Intelligence-Enabled Integration Suggests TP53 Pathway Alterations as Prognostic Biomarkers in Populations with Disproportionate Health Burdens",
    "url": "https://doi.org/10.17615/k1vc-1v16",
    "date": "2026-02-19",
    "content": "The incidence of early-onset colorectal cancer (EOCRC; <50 years) continues to increase, with the most rapid rises occurring among Hispanic/Latino (H/L) populations who remain underrepresented in molecular research. Because the TP53 signaling pathway is a key driver of colorectal tumorigenesis, this study aimed to clarify its prognostic significance in FOLFOX-treated EOCRC across ancestry groups. We analyzed 2515 colorectal cancer (CRC) cases (266 H/L, 2249 non-Hispanic White [NHW]) stratified by ancestry, age at onset, and FOLFOX exposure. Fisher’s exact, chi-square, and Kaplan–Meier’s analyses were applied, and multi-dimensional data integration was performed using AI-HOPE and AI-HOPE-TP53, conversational artificial intelligence platforms enabling natural language-driven exploration of clinical, genomic, and therapeutic features. TP53 pathway alterations were common in both H/L (85%) and NHW (83%) FOLFOX-treated patients. Among late-onset NHW cases, FOLFOX treatment was associated with higher TP53 mutation frequencies and lower ATM and CDKN2A mutation rates compared with untreated counterparts, while CHEK2 alterations were significantly less frequent in late-onset H/L patients. Missense mutations were the predominant alteration type across groups. These findings suggest that TP53 pathway alterations may be associated with ancestry- and treatment-specific clinical patterns in EOCRC and illustrate how AI-enabled integrative analytic frameworks can facilitate hypothesis generation and prioritize candidate biomarkers for future validation in precision oncology."
  },
  {
    "title": "Digital health technologies in medicine: evidence, artificial intelligence integration, and ethical challenges",
    "url": "https://doi.org/10.1186/s13027-026-00737-8",
    "date": "2026-02-19",
    "content": "Digital health technologies (DHTs), including digital therapeutics (DTx), are revolutionizing patient care by enabling the prevention, management, and treatment of medical conditions. These tools comprise care delivery mobile applications, wearable devices, and cloud platforms for capturing real-time data and enabling remote monitoring. DTx are regulated, software-based interventions that deliver evidence-supported therapeutic effects; artificial intelligence (AI) and machine learning, including advanced architectures, such as agentic systems and digital twins, may augment DTx workflows but are not defining features of DTx. Growing evidence supports the effectiveness of DHT strategies across different clinical fields. For example, wearable and remote patient monitoring technologies enable continuous assessment and personalized feedback in cardiology and neurology. Additionally, AI-enabled devices are widely implemented for continuous monitoring of glucose levels. However, several key challenges remain. Persistent gender and social biases in datasets and algorithms raise ethical concerns, particularly for underrepresented groups and pediatric populations. Mitigation strategies include regulatory frameworks, explainable AI, and trustworthy AI ecosystems. This work is a narrative, expert-driven review based on illustrative literature curated by domain specialists. It aims to synthesize current evidence, highlight implementation barriers, and propose recommendations to enhance inclusivity, interoperability, and real-world evaluation of digital health technologies. Applications of DHTs in animals within a One Digital Health framework, as well as potential applications in infection-related oncology, are also discussed. Not applicable."
  },
  {
    "title": "The Interface Manifesto: Ten Design Principles for AI-Native User Interfaces",
    "url": "https://doi.org/10.5281/zenodo.18692879",
    "date": "2026-02-19",
    "content": "The rapid integration of artificial intelligence into consumer and enterprise software has created a growing gap between the sophistication of AI capabilities and the interfaces through which users interact with them. While AI models have advanced considerably, the dominant interface paradigm - dashboards, forms, multi-step wizards, and navigation hierarchies - remains largely unchanged from the pre-AI era. The most common adaptation has been the addition of conversational chat interfaces, which this paper argues is insufficient. This paper presents The Interface Manifesto, a set of ten design principles for AI-native user interfaces derived from practitioner experience building AI-powered assessment tools. The principles address fundamental shifts required in interface philosophy: from navigation to intent inference, from data density to synthesized clarity, from user configuration to opinionated defaults, and from engagement optimization to trust-based design. Each principle is presented with its theoretical grounding, practical implications, and an evaluative test for product teams. The work extends foundational contributions by Amershi et al. (2019) on human-AI interaction guidelines, Nielsen's intent-based outcome specification paradigm (2023), and Weisz et al.'s generative AI design principles (CHI 2024). Supplementary material includes before/after visual comparisons for four of the ten principles."
  },
  {
    "title": "Non‐significant does not mean uninformative – but poor reporting can be misleading",
    "url": "https://doi.org/10.1111/bju.70183",
    "date": "2026-02-19",
    "content": "Two publications in this issue of BJUI address a problem that is rarely considered: how urological randomised controlled trials (RCTs) with non-significant results are reported and interpreted [1, 2]. Together, they discuss errors in statistical reasoning and offer a framework for extracting more meaningful inference from trials that do not meet conventional thresholds for statistical significance. Randomised trials in urology often produce statistically non-significant, ‘negative’ or ‘null’, results. This is merely a consequence of studying clinically realistic interventions producing modest effects among heterogeneous populations. The problem arises when we report and interpret those results. The Sharifan research letter [1] provides a reminder that interpretative errors remain common in contemporary urology trials. By reviewing recent RCTs, the author documents frequent acceptance of the null hypothesis and persistent use of ‘trend’ language for borderline P values. These practices represent logical errors that exaggerate certainty and obscure what the data show. This journal and other leading urology journals have jointly published explicit statistical reporting guidelines stating that null hypotheses are rejected or not rejected, never accepted, and that P values just above conventional thresholds should not be described as trends [3]. These principles reflect the logic of frequentist inference. That these issues persist despite clear guidance and statistical review suggests that the problem lies with entrenched habits of interpretation and ‘autopilot’ reporting of results without regard for their underlying meaning. The persistence of these issues has a long history. More than three decades ago, Oxford biostatistician Douglas Altman [4] highlighted the prevalence of statistical misinterpretation in clinical research. Subsequent audits of the medical literature confirmed that statistical errors were widespread, even in high-impact journals. The Sharifan [1] analysis suggests that while reporting standards have improved, core misunderstandings about inference remain remarkably durable. The letter by Sharifan [1] focuses on how non-significant results are misrepresented. The review by Noll et al. [2] addresses a more fundamental question: how should such results be interpreted at all? The authors apply likelihood ratios to urology RCTs with non-significant primary outcomes, asking whether the observed data provide greater support for the null hypothesis or for a clinically meaningful alternative. This distinction matters. A P > 0.05 tells us only that the data are not sufficiently incompatible with the null hypothesis. It does not tell us whether a study was inconclusive or whether it provides evidence that a clinically important effect is unlikely. Likelihood ratios, while not a substitute for thoughtful trial design, force that distinction into the open. The authors demonstrate that some ‘negative’ trials provide little information either way and may justify further investigation, whereas others provide strong evidence against benefit. Under conventional hypothesis testing, both scenarios are typically labelled the same: non-significant. This is a limitation of the inferential framework, not of the trials themselves. Similar Bayesian re-analyses in other surgical disciplines have reached analogous conclusions [5]. This finding has practical consequences. Some ‘negative’ trials should prompt larger studies or meta-analyses. Others should prompt us to stop investigating the problem. P values alone cannot tell us which is which. A broader point remains regarding the incentivisation to produce manuscripts with P < 0.05 (selective reporting). This is a known and continuing issue with considerable implications for the reproducibility of research [6]. As editors and reviewers, we frequently encounter submitted manuscripts that clearly prioritise statistical significance over clinical significance; or observational studies where the number of P values in the paper are many multiples of the sample size, with those P < 0.05 (un)helpfully adorned with bolding and asterisks. As our BJUI reporting guidelines recommend [3], we instead encourage authors to focus on estimation rather than inference, that is report your most important mean difference, percentage change, hazard ratio etc., and its CI, rather than a forest of P values. A manuscript that asks an interesting question, has a study design to answer it, collects robust data and analyses that data appropriately is 90% of the way to being accepted for publication in peer-reviewed journals. The solution is not to abandon P values, but to interpret them cautiously, use them judiciously and in conjunction with CIs —and, where appropriate, to supplement with other approaches that contribute to answering the clinical questions we actually care about. An artificial intelligence–based language model (ChatGPT, OpenAI) was used to assist with drafting and language editing of this editorial. The authors are solely responsible for the content, interpretation, and conclusions. The authors have no other disclosures pertaining to this submission."
  },
  {
    "title": "Driving factors of agricultural artificial intelligence adoption intention: an empirical study in Shandong province based on innovation characteristics, technology commitment, and individual heterogeneity",
    "url": "https://doi.org/10.3389/frai.2026.1630717",
    "date": "2026-02-19",
    "content": "In the “Agriculture 4.0 era,” the implementation of agricultural artificial intelligence (AI) has been proven to bring economic and environmental benefits to farmers. Despite its potential advantages, the adoption rate of agricultural AI remains relatively low. To explore the adoption driving mechanism of agricultural AI in major producing areas, this study took 359 agricultural practitioners in Shandong Province as samples, constructed an extended technology acceptance model (TAM)–unified theory of acceptance and use of technology (UTAUT) model integrating technological innovation characteristics, technology commitment, and individual heterogeneity, and used the partial least squares-structural equation modeling (PLS-SEM) method to empirically analyze the influencing factors and moderating effects of adoption intention. The results show that mobility, autonomy, technological interest, and technological control belief significantly and positively affect perceived ease of use; mobility, technological interest, and perceived ease of use have significant positive effects on perceived usefulness; perceived ease of use and perceived usefulness jointly drive the improvement of adoption intention. Educational background and work experience have significant moderating roles: Higher education strengthens the positive impact of technological interest on perceived ease of use, and rich work experience amplifies the promoting effect of technological competence belief (TCM) on perceived ease of use. However, the impacts of autonomy on perceived usefulness and technological competence belief on perceived ease of use and perceived usefulness are not statistically significant, which is closely related to the production characteristics of smallholder farmers and insufficient technological adaptability. This study improves the theoretical framework of agricultural AI adoption, provides an empirical basis for formulating differentiated technology promotion strategies and optimizing technology design, and has important practical significance for accelerating agricultural digital transformation."
  },
  {
    "title": "Retrieval Augmented Generation ( <scp>RAG</scp> ) for Evaluating Regulatory Compliance of Drug Information and Clinical Trial Protocols",
    "url": "https://doi.org/10.1002/psp4.70201",
    "date": "2026-02-19",
    "content": "ABSTRACT The purpose was to evaluate retrieval‐augmented generative (RAG) artificial intelligence (AI) methods for assessing the regulatory compliance of drug information and adherence to best practices in clinical trial protocols. Integrated systems containing RAG and large language model (LLM) components were employed to evaluate drug information and clinical trial protocols. The drug information for adalimumab, insulin glargine, atorvastatin calcium, sertraline, and alprazolam was evaluated for compliance with Food and Drug Administration (FDA) clinical pharmacology guidance for indications, use in specific populations, and warnings and precautions. The reasons for the withdrawal of rofecoxib, valdecoxib, and troglitazone were elicited. The clinical trial protocol evaluation system was used to assess a Phase‐2a clinical trial protocol of Rifafour in tuberculosis with the FDA E9 and E9 (R1) guidance documents. The RAG system correctly identified the indications, use in specific populations, and warnings and precautions for adalimumab, insulin glargine, atorvastatin calcium, sertraline, and alprazolam. The drug information was evaluated against the requirements in the guidance documents, confirming compliance when present and providing explanations for deficiencies. The causes underlying the withdrawal of rofecoxib, valdecoxib, and troglitazone were explained. The clinical protocol summary included study design, population definitions, treatments, dose levels, and route of administration. The summary of the statistical analysis plan included primary/secondary endpoints, statistical tests, pharmacokinetic parameters, and handling of missing data and outliers. The findings aligned with manual protocol reviews. RAG‐based AI methods can improve the usefulness of LLMs in document‐restricted settings and are a promising approach for evaluating the compliance of clinical pharmacology documents."
  },
  {
    "title": "Quantum algorithm for apprenticeship learning",
    "url": "https://doi.org/10.1007/s42484-026-00368-7",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "GATEKEEPER RELOAD",
    "url": "https://doi.org/10.25200/bjr.v21n3.2025.1803",
    "date": "2026-02-19",
    "content": "RESUMO – A inteligência artificial vem reconfigurando o jornalismo e conceitos associados às teorias do gatekeeping e dos critérios de noticiabilidade. O artigo reflete sobre as mudanças na etapa inicial da produção jornalística – concepção e roteirização da pauta – mediante a colaboração humano-máquina advinda do uso de assistentes virtuais no planejamento editorial. A partir da pesquisa aplicada, com base nas instanciações da Design Science Research Methodology (DSRM), na concretização de um artefato que conecta a teoria às práticas emergentes, o artigo apresenta uma experiência com um modelo de assistente virtual, mostrando seus níveis de controle e interferência e seu impacto na seleção de pautas e planejamento jornalístico. Os resultados indicam possibilidades de eficiência e inovação, mas também implicações éticas e operacionais na autonomia editorial. ABSTRACT – Artificial intelligence is reshaping journalism and concepts associated with gatekeeping theories and newsworthiness criteria. This article reflects on changes in the initial stage of news production—the conception and scripting of stories—through human-machine collaboration resulting from the use of virtual assistants in editorial planning. Based on applied research, using Design Science Research Methodology (DSRM) instantiations to create an artifact that connects theory to emerging practices, this paper presents an experiment with a virtual assistant model, showing its levels of control and interference as well as its impact on news selection and planning. The results indicate possibilities for efficiency and innovation, and ethical and operational implications for editorial autonomy. RESUMEN – La inteligencia artificial está reconfigurando el periodismo y los conceptos asociados a las teorías del gatekeeping y los criterios de noticiabilidad. El artículo reflexiona sobre los cambios en la etapa inicial de la producción periodística – concepción y guionización de la agenda – mediante la colaboración entre humanos y máquinas derivada del uso de asistentes virtuales en la planificación editorial. A partir de la investigación aplicada, basada en las Instancias de la Metodología de Investigación en Ciencia del Diseño (DSRM) en la realización de un artefacto, que conecta la teoría con las prácticas emergentes, se presenta una experiencia con un modelo de asistente virtual, mostrando sus niveles de control e interferencia y el impacto en la selección y planificación periodística. Los resultados indican posibilidades de eficiencia e innovación, así como implicaciones éticas y operativas en la autonomía editorial."
  },
  {
    "title": "Solving The Hard Problem: A Philosophical Inquiry into Consciousness, Sovereignty, and the Human Fortress",
    "url": "https://doi.org/10.5281/zenodo.18698879",
    "date": "2026-02-19",
    "content": "This paper addresses the most urgent philosophical question of our time: What distinguishes human consciousness from machine simulation? In an age where artificial intelligence mimics language, logic, and creativity, the author argues that the dominant discourse both functionalist and phenomenological misses a crucial distinction: the difference between functional intelligence (B) and sovereign interiority (F) . The paper introduces the A‑B·F‑N model, an original mathematical‑ontological framework that reframes the Hard Problem of Consciousness (Chalmers, 1995) from How does consciousness arise from matter? to What is the ontological difference between two functionally identical systems, one conscious and the other not? The model distinguishes between: · A (Input): Raw stimuli, events, the world as it presents itself. · B (Functional Processing): The mechanism transforming A into output – biological (Bₐ), algorithmic (Bₘ), or human cognitive (Bₕ). B is the realm of nature, cause‑effect, and the predictable. · F (Sovereign Interiority): The internal governing system – conscience, ethical judgment, existential inquiry, and the capacity for Veto. F is the realm of freedom, responsibility, and the unpredictable. · Nf (Existential Meaning): The authentic output emerging only from the dynamic conflict (⇄) between B and F. The Bouzid Equation (B + F = Nf) expresses that true existential meaning is the inseparable sum of functional processing and sovereign interiority. The defining property of F is the Sovereign Veto: the ability to reject B's outputs even when functionally correct – the foundation of moral obligation and legal responsibility. The paper provides empirical proof through three real‑world dissociation cases demonstrating that B can operate without F: sleeping motherhood (complex care without conscious recall), Sleep‑Related Eating Disorder (SRED) (pathological behaviour without moral awareness), and insanity (legal exemption due to F's absence). These reveal the ontological hierarchy: F ⊇ B – the sovereign interior governs the functional substrate. The model solves classical dilemmas: the Philosophical Zombie is a real possibility during F‑absence, and Searle's Chinese Room shows that symbol manipulation (B alone) cannot generate existential understanding (Nf) without F. The framework is formalised through Sovereignty Algebra, including the Coherence Equation (Mθ = Empathy(F)/Intelligence(B)) , which dictates that ethical stability requires proportional growth of human empathy alongside machine intelligence, and the Action of Sovereignty (As = Empathy × Human Variance × Action(F)) , modelling the irreducible moment of human override. The paper concludes by linking the model to the author's earlier critique of profit‑driven programming in The Valley of Intelligent Exploitation. In an age where AI generates plausible fictions, F is the human fortress – the only defence against algorithmic colonisation. The goal of AI is not to create consciousness in machines, but to protect human sovereignty through systems designed with awareness of their ontological limits. Written by an Algerian independent researcher with a deep personal bond to Australia, this work offers a human‑centric digital ontology capable of resisting reductionist techno‑solutionism, inviting philosophers, cognitive scientists, and AI ethicists to reconsider the foundations of consciousness and responsibility."
  },
  {
    "title": "FOSTERING MORAL AWARENESS AND CYBERSAFETY IN DIGITAL EDUCATION: CHALLENGES AND SOLUTIONS FOR THE AI ERA",
    "url": "https://doi.org/10.5281/zenodo.18703525",
    "date": "2026-02-19",
    "content": "This article, drawing on experience in the educational field and current research, examines the critical issues surrounding the development of moral awareness and cybersafety culture among students within the framework of modern digital education. It analyzes the transformative impact of emerging technologies, particularly Artificial Intelligence (AI), on the learning environment. The research identifies key challenges, including ethical dilemmas inherent in AI usage, the erosion of traditional moral boundaries in digital spaces, and the increasing vulnerability of students to sophisticated cyber threats. Through a mixed-methods approach, synthesizing a comprehensive literature review with observational insights and correlational analysis, the study proposes practical, multi-layered solutions. These solutions focus on integrating socio-emotional learning, ethical reflection, and continuous professional development for educators to cultivate a robust digital culture that prioritizes intrinsic ethical responsibility, proactive safety measures, and the holistic development of digital citizens. The findings underscore the urgent need for a pedagogical paradigm shift from mere technical instruction to comprehensive ethical formation in the AI era."
  },
  {
    "title": "Utilization of Artificial Intelligence in School Management and Principals' Administrative Effectiveness in Public Secondary Schools in Cross River State, Nigeria",
    "url": "https://doi.org/10.61424/issej.v4i1.716",
    "date": "2026-02-19",
    "content": "Specifically, the study sought to investigate the extent of utilization of artificial intelligence in school management among secondary school Principals, and to find out whether there is a relationship between the utilization of artificial intelligence in school management and principals' administrative effectiveness. Two research questions were posed with two corresponding null hypotheses to guide the study. The study adopted a correlational research design. The population of the study comprised 297 principals in public secondary schools in the three education zones in the state. A stratified random sampling technique was used to select 134 principals as the sample and respondents of the study. An instrument titled \" Utilization of Artificial Intelligence in School Management and Principals' Administrative Effectiveness Questionnaires (UAISMPAEQ)\" was used to collect data. The instrument was validated by two experts. Cronbach's alpha was used to determine the reliability coefficient of the instrument. Independent t-test and Pearson's product moment correlation statistics were used to analyze the data. Results revealed that utilization of artificial intelligence in managing school resources among school principals is significantly low, and that there is a significant relationship between utilization of artificial intelligence in managing school resources and principals' administrative effectiveness. Hence, it was recommended that the government and relevant agencies should provide infrastructure in schools that will enable the use of artificial intelligence in school management. Secondly, school principals should endeavor to utilize artificial intelligence in school management to enhance their administrative effectiveness."
  },
  {
    "title": "Leading-edge thecs and artificial intelligence trends 2025. Job offer and personal skills",
    "url": "https://doi.org/10.5281/zenodo.18698281",
    "date": "2026-02-19",
    "content": "The paper discussion is about new software technologies and AI trends in 2025. The research seek trends in main International Organization and Consulting firms sites. Advancements in Artificial Intelligence (AI), Quantum Computing, and immersive technologies trends in 2025 are likely the main impacts on labor offer and skills. People must be aware of trends because opportunities varies form different economic activities and regional industry system and institutions labor offer. Some skills offer more opportunities to be employed. Although trends are clear it doesn’t make any sense seek general rules to apply a job offer. A deep knowledge every Nation’s labor offer and economic situation and competence skills formation define which skill must to be improved and where. What we conclude is the need of understand labor offer trend because of the speed of change and the different skills involved by new technologies trends. We suggest a way to match trends and skills."
  },
  {
    "title": "Artificial intelligence-driven digital shadow and assistant for enhancing carbon sequestration in furrow-irrigated rice under straw return practices within rice-wheat rotations",
    "url": "https://doi.org/10.1016/j.engappai.2026.114220",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "The Theory of Absolute Breath: Heliocentrism in Art and the Breathing Declaration",
    "url": "https://doi.org/10.17613/z7800-wt525",
    "date": "2026-02-19",
    "content": "This treatise by painter Hirofumi Miyauchi proposes a paradigm shift in the intellectual foundations of modern civilization, moving from \"theory\" to \"breath\". In Part I, \"Heliocentrism in Art,\" the author provides a logical analysis of how theory infringes upon somatic knowledge, leading humanity into a state of \"collective respiratory failure\". In Part II, \"The Breathing Declaration,\" breath is defined as the \"Scientific Zero Point (0),\" presenting a new coordinate system of intelligence—the Theory of Absolute Breath—to re-establish the right to survival. By updating Descartes' \"I think, therefore I am\" to \"I live, therefore I am,\" this work envisions the future of a \"Breathing Civilization\" that remains unaffected by the progress of Artificial Intelligence."
  },
  {
    "title": "Educational Transformation in the Era of Artificial Intelligence",
    "url": "https://doi.org/10.1007/s44366-026-0082-2",
    "date": "2026-02-19",
    "content": "Abstract The integration of artificial intelligence (AI) in education is reshaping how learning is designed, delivered, and governed. This study examines five interrelated domains: integrating AI in education, learner development and personalization, curriculum and educational content, role of teachers, and governance and regulation. It argues for systemic, ethically grounded educational approaches that promote equity, lifelong learning, and collaborative policymaking."
  },
  {
    "title": "Can generative AI effectively perform quality evaluation within social sciences? A case study in library and information science",
    "url": "https://doi.org/10.1007/s11192-026-05570-9",
    "date": "2026-02-19",
    "content": "Abstract Thus far, the usage of generative artificial intelligence (GAI) has mainly been explored for content-based evaluations. Research quality evaluations and studies focusing on fields in the social sciences are needed. Therefore, we performed a quality evaluation using state-of-the-art GAI models to assess their usability in this respect. GPT-4o and DeepSeek-V3 were employed to select papers with very high quality from a set of high quality papers. Comparisons with human expert decisions, citation counts, and download counts were the basis for our quality assessments. GAI models can to some extent assess the academic value of high-quality papers. We found a positive but limited correlation between GAI-based quality scores with citation and download counts. We observed selective preferences of GAI’s quality assessment in favor of papers that use overt technical terminology, methodological frameworks, or seemingly broad but objectively worded expressions. GAI comparatively undervalues the quality of papers with high theoretical abstraction or strong socio-contextual embedding. Our study provides new empirical foundations for defining GAI’s role in research quality evaluation in the social sciences. Crucial guidance for academic institutions, libraries, funding agencies, and research management organizations is extracted. We expect that GAI trained to assess academic quality would produce classic paper selections in higher agreement with human experts than currently available models."
  },
  {
    "title": "Blort Ai Promo Code 2026 [VINEET] – Get Exclusive Discount Of 15%",
    "url": "https://doi.org/10.5281/zenodo.18694251",
    "date": "2026-02-19",
    "content": "🤖 Blort AI Promo Code 2026 [VINEET] – Get Exclusive 15% OFF Today 🚀🔥 Want to access powerful AI tools at a discounted price? 💻✨ Now you can unlock premium features using **Blort AI Promo Code 2026 [VINEET]** and enjoy an exclusive 15% OFF instantly! 💸🎉 ━━━━━━━━━━━━━━━━━━━━━━━ 🧠 What is Blort AI? ━━━━━━━━━━━━━━━━━━━━━━━ Blort AI is an AI-powered platform designed to help users automate tasks, generate content, and improve productivity using smart artificial intelligence tools. 🤖⚡ It can assist with content creation, business workflows, digital marketing, and automation processes — making work faster and more efficient. 📊 Perfect for creators, entrepreneurs, and growing businesses. 🚀 ━━━━━━━━━━━━━━━━━━━━━━━ 🎟 Blort AI Promo Code 2026 Details ━━━━━━━━━━━━━━━━━━━━━━━ 🎫 Promo Code: VINEET 💰 Discount: Flat 15% OFF 📅 Valid: 2026 (Limited-Time Offer) 🆕 Applicable On: Selected Plans / Subscriptions Simply enter the code during checkout and activate your savings instantly. 🎊 ━━━━━━━━━━━━━━━━━━━━━━━ 🔥 Key Features of Blort AI ━━━━━━━━━━━━━━━━━━━━━━━ ✅ AI Content Generation ✍️ ✅ Workflow Automation ⚙️ ✅ Productivity Enhancement Tools 📈 ✅ User-Friendly Dashboard 💻 ✅ Scalable AI Solutions 🚀 ✅ Easy Integration Options 🔗 ━━━━━━━━━━━━━━━━━━━━━━━ 🎯 Who Should Use Blort AI? ━━━━━━━━━━━━━━━━━━━━━━━ 👨‍💻 Developers 📈 Marketers 🏢 Businesses 🚀 Startups 🎨 Content Creators If you want to boost efficiency and automate daily tasks, Blort AI is a powerful solution. 🔥 ━━━━━━━━━━━━━━━━━━━━━━━ 💡 Why Use Promo Code [VINEET]? ━━━━━━━━━━━━━━━━━━━━━━━ ✔ Save 15% instantly ✔ Access premium AI tools ✔ Reduce overall subscription cost ✔ Limited-time exclusive savings ━━━━━━━━━━━━━━━━━━━━━━━ 📝 How to Apply Blort AI Promo Code ━━━━━━━━━━━━━━━━━━━━━━━ 1️⃣ Visit the official Blort AI website 2️⃣ Choose your preferred subscription plan 3️⃣ Enter promo code: VINEET 4️⃣ Get 15% OFF instantly 🎉 ━━━━━━━━━━━━━━━━━━━━━━━ ⚠ Don’t Miss This 2026 Special Deal! Upgrade your AI workflow and save money today. 🚀 Use **Blort AI Promo Code 2026 [VINEET]** now and grab your 15% discount before the offer expires! 🔥🤖"
  },
  {
    "title": "Supporting the creative industries through the AI turn: A comparative analysis of Scottish policy and the needs of Scotland’s creative practitioners",
    "url": "https://doi.org/10.1371/journal.pone.0340255",
    "date": "2026-02-19",
    "content": "In the creative industries (one of the largest UK sectors, which operates at the juncture of business, technology and the arts) creative practitioners are not only consumers of Artificial Intelligence (AI), but also its developers, originators and innovators. This has material consequences for how creative work is performed, valued and understood, as well as global consequences for business structures, the environment, equality and power discrepancies. We take Scotland as a case study for how governmental policy addresses the challenges of AI for the creative industries by comparing Scotland’s Artificial Intelligence Strategy with data gathered from grass roots creative industries practitioners in Scotland through workshops and a survey. We identify differences between the priorities of Scotland’s Artificial Intelligence Strategy and the creative industries, as well as a lack of advice specific to the needs of the creative industries, and offer recommendations on how to put creatives at the heart of AI developments."
  },
  {
    "title": "Deep Learning Framework for Early Detection of Sugarcane Pathogens via Image Processing on Embedded Hardware",
    "url": "https://doi.org/10.22214/ijraset.2026.77499",
    "date": "2026-02-19",
    "content": "Precision Agriculture has become a revolutionary model on how crop productivity and sustainability can be improved. The crop organs that are most prone to diseases include red rot, red rust, mosaic virus and yellow leaf disease in sugar cane crops. This study suggests a synthesized artificial-intelligence system, which is a convolutional neural network (CNN) integrated with the MATLAB ResNet-50 architecture, deployed in the Simulink hardware, and edge processing in the Raspberry Pi. The system classifies sugarcane leaf disease real-time by image processing and deep learning. The suggested framework allows disease identification using the automated system, minimizes human control of monitoring, and aids in the implementation of early intervention measures. The experimental outcomes show a high classification accuracy and an efficient ability to operate in a real-time situation."
  },
  {
    "title": "Artificial Intelligence and Machine Learning in Utility Infrastructure Financial Forecasting",
    "url": "https://doi.org/10.5281/zenodo.18703569",
    "date": "2026-02-19",
    "content": "The growing complexity and instability of utility infrastructure systems have undermined the efficiency of traditional econometric techniques of financial forecasting. This review is a literature survey of artificial intelligence (AI) and machine learning (ML) that is reinventing financial forecasting in the energy, water, and smart urban infrastructure industries. It bases itself on peer-reviewed literature written since 2015 and 2025 to synthesize theoretical background, methodological advances, and research applications to assess AI-based models integration in utility finance. The most recent developments in deep learning architectures, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), or reinforcement learning models make it feasible to model nonlinear, spatio-temporal, and adaptive dynamics in infrastructure systems. It has been empirically proven that AI-based and hybrid forecasting models are more accurate, robust, and capable of predicting over multi-horizons than more traditional statistical methods. In addition, AI-driven forecasting is a paradigm shift of the unchanging econometric prediction to dynamic financial intelligence to aid real-time decision-making, risk reduction, and capital optimization. However, there are still serious issues related to data interoperability, algorithmic transparency, and governance alignment in the context of the public utility."
  },
  {
    "title": "Energy based wheat yield prediction using robust neural networks and neuro-fuzzy inference systems across North-Western India",
    "url": "https://doi.org/10.1007/s10668-025-07242-9",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "Knowledge flows from science to AI technology: Identifying core and brokerage technological roles",
    "url": "https://doi.org/10.1371/journal.pone.0341005",
    "date": "2026-02-19",
    "content": "The rapid advancement of artificial intelligence (AI) technologies has not only driven convergence with diverse technological domains but also swiftly spread across various industrial sectors. As a knowledge-intensive field, AI is particularly shaped by the flow of knowledge from scientific research to technological development, yet remains insufficiently examined in a systematic and structural way. This study addresses this gap by investigating science-to-technology knowledge flow that underpins AI’s technological evolution. We propose a semantic science-technology exploration framework specifically designed for the AI domain, consisting of the two stages: technology classification and semantic topic exploration. First, AI patents are classified into four categories using centrality measures derived from a CPC co-occurrence network. Then, we extract abstracts from both patents and their cited scientific publications to apply BERTopic modelling and generate topic labels using generative AI. Analyzing AI-related patents filed from 2002 to 2021, we trace key technological trends and elucidate the structural pathways of knowledge flow science to technology. The findings offer practical implications for corporate R&D strategies and innovation policy design in the era of AI."
  },
  {
    "title": "Artificial intelligence-assisted medical research design for novice researchers: a narrative review",
    "url": "https://doi.org/10.5124/jkma.25.0156",
    "date": "2026-02-19",
    "content": "Purpose: With a focus on novice researchers, it aims to clarify how artificial intelligence (AI) can support research design and may lower barriers to research participation while emphasizing the continuing importance of human judgment in study conception and interpretation.Current concepts: Traditional medical research training has emphasized methodological theory, statistical techniques, and mentor–mentee instruction. However, these approaches often present substantial challenges for novice researchers. Time constraints, limited access to data, and the need to learn unfamiliar methodologies further contribute to these difficulties. Recent advances in AI enable the rapid synthesis of large volumes of information and structured exploration of research questions. AI also facilitates the organization of study frameworks. These tools may help structure research designs that are less dependent on patient-level data. Examples include social media studies, narrative reviews, and qualitative analyses.Discussion and conclusion: AI has the potential to reshape research design education by functioning as a cognitive support tool. It can help novice researchers formulate meaningful questions, select appropriate study designs, and interpret findings. However, responsible use requires explicit attention to explainability, methodological rigor, and ethical standards. Significant limitations include AI’s dependence on available data, its inability to independently verify information, and the risk of generating misleading suggestions when prompts are poorly constructed. Clear guidance regarding the appropriate scope and limitations of AI-assisted research design is essential. When used appropriately, AI may broaden participation in medical research, enhance research literacy, and support the development of sustainable research skills among clinicians and medical trainees."
  },
  {
    "title": "Design and optimisation of an IoT-based artificial intelligence framework for real-time health monitoring and telemedicine diagnostics in smart healthcare systems",
    "url": "https://doi.org/10.1007/s43926-026-00306-w",
    "date": "2026-02-19",
    "content": "Modern healthcare systems (particularly those with limited resources and in remote locations) require real-time monitoring and telemedicine. Conventional cloud-reliant solutions face challenges with latency, data privacy, and trust. The system integrates a CNN-GRU hybrid deep learning model for classifying physiological signals, a Trust-Aware Diagnostic Engine based on EMA scoring, and a Federated Learning system augmented with Differential Privacy (DP) to ensure decentralized training. The system was extensively tested on PhysioNet MIT-BIH, MIMIC-III, and a simulated H-IoT dataset to assess real-time performance, accuracy, and efficiency. The proposed CNN-GRU model outperformed the baseline models (LSTM, Transformer-only) in average accuracy (96.3), F1-score (0.94), and ROC-AUC (0.97). In the hybrid edge-fog, latency was reduced by 28%. When device signal noise was intermittent, the trust score remained stable at over 0.85. Under DP noise (epsilon = 0.1), the federated learning model converged in 40 rounds with a negligible loss of accuracy. SHAP feature attribution made the predictions interpretable. The framework presents a new combination of federated learning, privacy-preserving and trust-based decision-making, and adaptive edge-fog orchestration for smart healthcare. This work provides a standard for the design of clinically relevant, safe, and interpretable systems based on AI-powered telemedicine and strongly aligns with translational impact."
  },
  {
    "title": "FOSTERING MORAL AWARENESS AND CYBERSAFETY IN DIGITAL EDUCATION: CHALLENGES AND SOLUTIONS FOR THE AI ERA",
    "url": "https://doi.org/10.5281/zenodo.18703526",
    "date": "2026-02-19",
    "content": "This article, drawing on experience in the educational field and current research, examines the critical issues surrounding the development of moral awareness and cybersafety culture among students within the framework of modern digital education. It analyzes the transformative impact of emerging technologies, particularly Artificial Intelligence (AI), on the learning environment. The research identifies key challenges, including ethical dilemmas inherent in AI usage, the erosion of traditional moral boundaries in digital spaces, and the increasing vulnerability of students to sophisticated cyber threats. Through a mixed-methods approach, synthesizing a comprehensive literature review with observational insights and correlational analysis, the study proposes practical, multi-layered solutions. These solutions focus on integrating socio-emotional learning, ethical reflection, and continuous professional development for educators to cultivate a robust digital culture that prioritizes intrinsic ethical responsibility, proactive safety measures, and the holistic development of digital citizens. The findings underscore the urgent need for a pedagogical paradigm shift from mere technical instruction to comprehensive ethical formation in the AI era."
  },
  {
    "title": "Artificial Intelligence and the Differential Advantage Theory of National and Organizational Wealth",
    "url": "https://doi.org/10.5281/zenodo.18689169",
    "date": "2026-02-19",
    "content": "Why have different economic theories identified land, labor, capital, knowledge, or institutions as the primary source of national wealth? Existing accounts treat these factors as competing ontological foundations of economic production, without explaining why their explanatory primacy has shifted historically. This paper proposes the Differential Advantage Theory of National and Organizational Wealth, which argues that the perceived source of wealth corresponds to the factor that provides differential advantage under conditions where other productive factors are relatively equalized. Drawing on economic history, organizational theory, and innovation studies, the paper explains the historical succession of land, labor, capital, knowledge, and institutions as dominant explanatory factors — not as competing foundations, but as context-dependent mechanisms of constraint relief. It argues that artificial intelligence constitutes the current primary source of differential advantage because it amplifies intelligence — the organizing capacity that governs the deployment of all productive and institutional resources. The paper further theorizes the disaggregation of AI-based advantage along a structural stack comprising models, proprietary data, energy and compute infrastructure, and organizational integration capacity, predicting sequential shifts in the locus of advantage as each layer commoditizes. It proposes preliminary frameworks for measuring intelligence amplification capacity at national and organizational levels, integrates coercive and institutional power as a meta-mechanism securing access to scarce constraint-relief factors, and addresses the risks and negative externalities of intelligence amplification. This framework offers a unified, dynamic, and multi-layered theory of differential advantage in the age of artificial intelligence."
  },
  {
    "title": "A comprehensive review of the changing landscape of academic dishonesty in automated proctoring in the era of artificial intelligence",
    "url": "https://doi.org/10.1007/s44217-026-01275-6",
    "date": "2026-02-19",
    "content": "Exams are inherent in building confidence and appraising an individual in schools and colleges. However, supervising an exam is a tedious process given the large number of students in a class. This constant supervision is necessary to prevent students from cheating via copying, browsing, talking, or electronic means, thereby harming the integrity and authenticity of the examination. Technology has revolutionized how proctoring is undertaken, making this process more efficient by transitioning to online proctoring systems from traditional ones. Due to COVID-19, this online proctoring system has significantly shifted to utilize technologies such as Artificial Intelligence (AI) and Machine Learning (ML). Existing surveys focus on behavioral, pedagogical, and governance levels; however, there is a lack of discussion on applications, technologies, and associated challenges. This paper analyzes the challenges associated with conventional, online, automated, and AI-based proctoring systems and identifies the research gaps based on different applications and their impact on education and academia. Moreover, the paper provides conceptual scenarios related to automated online proctoring systems, insights into various categories, and the challenges associated with the survey, which will act as a boon to future researchers."
  },
  {
    "title": "LEGAL PERSPECTIVES ON THE LEGAL REGULATION OF AI-GENERATED WORKS ND ISSUES ON LEGAL SUBJECTIVITY OF ARTIFICIAL INTELLIGENCE",
    "url": "https://doi.org/10.33244/2617-4154-3(20)-2025-155-163",
    "date": "2026-02-19",
    "content": "The rapid evolution of information technologies, driven by revolutionary innovations, has profoundly transformed the nature of legal relations. The widespread integration of artificial intelligence (AI), its numerous advantages, and its continuous advancement necessitate substantial changes in legal regulation. In particular, the development of machine learning technologies and the emergence of generative artificial intelligence capable of producing intellectual property (IP) objects with novel and original features have raised significant questions concerning their legal protection. Advances in generative AI have enabled machines to create texts, artworks, musical compositions, and even inventions that can rival human creativity. These developments challenge the traditional understanding of authorship and originality and raise complex issues regarding the legal status of AI-generated works within the existing framework of intellectual property law. As a result, legal relations arising from works created by generative AI remain largely unregulated in the legislation of most jurisdictions. This regulatory gap gives rise to considerable uncertainties and practical challenges in the application of IP law to AI-generated outputs. This article seeks to examine the existing legal concepts of authorship and creativity, to analyze the current legal framework governing AI-generated works, and to identify possible approaches for the development of coherent and adaptive regulatory mechanisms. The study is based on the analysis of diverse scientific literature and legal sources."
  },
  {
    "title": "The Plurality Preservation Principle (PPP): A Constitutional Constraint for Civilizational-Scale Systems",
    "url": "https://doi.org/10.5281/zenodo.18691680",
    "date": "2026-02-19",
    "content": "The Plurality Preservation Principle (PPP) proposes a constitutional constraint for civilizational-scale systems, including advanced artificial intelligence architectures and governance infrastructures. Rather than encoding value maximization objectives, PPP establishes a non-compensable boundary condition: no system action may irreversibly reduce the number or independence of loci of agency beyond a defined constitutional threshold. The framework introduces the concept of irreversibility confidence, incorporating probabilistic assessment into governance-layer decision constraints. By forbidding irreversible collapse of cognitive and agency diversity, PPP seeks to preserve long-term plurality, optionality, and generative evolutionary capacity. This document outlines the theoretical grounding, operational formulation, scope of application, and open research questions associated with the principle."
  },
  {
    "title": "Building Agile Supply Chains: How AI And ERP Systems Improve Resilience in Disruptions",
    "url": "https://doi.org/10.55677/crb/i2-02-crb2026",
    "date": "2026-02-19",
    "content": "Modern supply chains operate under persistent volatility, where disruptions driven by geopolitical instability, logistics bottlenecks, and environmental events routinely challenge operational continuity. This paper examines how the integration of Artificial Intelligence (AI) capabilities within Enterprise Resource Planning (ERP) systems enables technically agile and resilient supply chain operations. The study focuses on AI-driven functionalities embedded in ERP architectures, including machine learning–based demand forecasting, anomaly detection, and predictive maintenance, and evaluates their impact on real-time decision-making under uncertainty. Using a combination of system-level modeling, algorithmic performance analysis, and simulation of disruption scenarios, the research assesses improvements in data interoperability, latency reduction, and adaptive resource reconfiguration enabled by AI-enhanced ERP environments. The findings indicate that automated analytics pipelines significantly improve forecast accuracy, end-to-end supply visibility, and optimization outcomes across multi-tier supply networks. Moreover, AI-enabled ERP systems demonstrate superior responsiveness to disruption scenarios through dynamic recalibration of planning parameters and execution rules. The paper concludes by proposing a technical integration framework that outlines key architectural layers, data flow mechanisms, and algorithmic design considerations required to develop resilient, self-adaptive supply chain systems capable of operating effectively under continuous disruption."
  },
  {
    "title": "Automated Detection of Ventilator Asynchronies: A Clinical and Technological Perspective",
    "url": "https://doi.org/10.1177/19433654251412329",
    "date": "2026-02-19",
    "content": "Patient–ventilator asynchrony is highly prevalent during invasive mechanical ventilation, yet its detection at the bedside remains limited. Conventional waveform inspection is intermittent, operator-dependent, and insufficient to capture the complexity and temporal variability of patient–ventilator interaction. Automated systems based on advanced signal processing and artificial intelligence represent a paradigm shift, enabling continuous, objective, and scalable detection of asynchrony. Recent approaches highlight the value of entropy-based metrics to quantify the irregularity of ventilatory signals and capture subtle changes in respiratory variability. Similarly, the identification of asynchrony clusters, periods where multiple asynchronous events occur in succession, provides a clinically relevant framework to stratify severity and predict outcomes. These developments underscore the superiority of automated methods over human observation alone. From an implementation perspective, centralized monitoring architectures offer greater potential than stand-alone devices, as they allow multimodal integration, data aggregation, algorithm refinement, and interoperability across platforms. Looking forward, research must expand toward the fusion of ventilatory, hemodynamic, and neurological signals to provide a comprehensive picture of patient–ventilator interaction. Particular attention should be paid to the long-term consequences of poor synchrony, as evidence suggests that asynchronies may influence functional recovery and health-related quality of life well beyond intensive care unit discharge. Robust, standardized datasets will ultimately support the generation of synthetic data and patient-specific digital twins, paving the way for precision-guided, adaptive mechanical ventilation."
  },
  {
    "title": "INTEGRATING ARTIFICIAL INTELLIGENCE WITH GAME-BASED LEARNING: ADVANCING INTELLIGENT TUTORING IN EDUCATION",
    "url": "https://doi.org/10.5281/zenodo.18702652",
    "date": "2026-02-19",
    "content": "Ushbu maqola sun’iy intellekt asosida o‘quv o‘yinlari (game-based learning, GBL)ning ta’lim jarayoniga ta’sirini tahlil qiladi. So‘nggi tadqiqotlar AI bilan jihozlangan o‘quv o‘yinlari talabalar motivatsiyasini oshirishi, individual o‘quv ehtiyojlariga moslashishi va bilimlarni yaxshilashi mumkinligini ko‘rsatadi. Maqolada AI-ning GBL jarayonida adaptiv fikr-mulohazalar berishi, o‘quv darajasini real vaqtda kuzatishi va o‘quvchilarning o‘z sur’ati bilan rivojlanishiga yordam berishi muhokama qilinadi. Shuningdek, texnik muammolar, dizayn va teng imkoniyatlar masalalari ham ko‘rib chiqiladi. Natijalar shuni ko‘rsatadiki, AI bilan boyitilgan o‘quv o‘yinlari ta’limni qiziqarli va samarali qilishda katta salohiyatga ega."
  },
  {
    "title": "Risk assessment in mortgages: a comparative study of AI models",
    "url": "https://doi.org/10.7717/peerj-cs.3494",
    "date": "2026-02-19",
    "content": "Motivated by the increasing volatility in the Canadian housing market, rising interest rates, and the tightening regulatory landscape, this study explores the application and comparative analysis of artificial intelligence (AI) models for assessing mortgage risk, in particular, default risk prediction. Traditional techniques, such as credit scoring and financial ratios, often fail to capture the intricate, non-linear relationships and shifting borrower behaviors characteristic of modern mortgage portfolios. AI-driven models, on the other hand, excel at processing complex datasets and uncovering hidden patterns. This research evaluates multiple AI approaches to assess their predictive accuracy, adaptability, and interpretability, including Artificial Neural Networks (ANNs), Convolutional Neural Networks (CNNs), Random Forests (RF), and XGBoost. Results demonstrate that ensemble models, particularly the class-weighted XGBoost model, deliver superior adaptability to volatility ( i.e ., volatile housing market), while neural networks show potential when applied to rich datasets but demand significant computational resources. By improving borrower risk prediction and enabling proactive adjustments to loan terms, AI models help financial institutions reduce default rates, achieve regulatory compliance, and optimize operational costs. These models enhance the financial sector’s resilience to market volatility while paving a way towards more sustainable lending practices. This study highlights the need to balance predictive performance, interpretability, and adaptability, offering insights for leveraging AI in effective, data-driven mortgage risk management."
  },
  {
    "title": "Degree of university students’ awareness of using artificial intelligence applications (ChatGPT) in the educational process in the UAE",
    "url": "https://doi.org/10.30935/cedtech/17954",
    "date": "2026-02-19",
    "content": "This study aimed to assess the degree of university students’ awareness and use of artificial intelligence applications, specifically chat generative pre-trained transformer (ChatGPT), within the educational process in the United Arab Emirates (UAE). The descriptive-analytical method was employed, using a questionnaire distributed to a sample of 608 male and female students from Ajman University, representing a population of 6,072. The results revealed that university students have a high overall level of awareness (mean = 3.72, standard deviation = 0.83). A detailed analysis of the underlying dimensions of awareness identified four key factors: skills development, problem-solving, information exchange, and knowledge acquisition. While the overall awareness was high, the level of awareness regarding the use of artificial intelligence (AI) for skills development, problem-solving, and information exchange was average, with only the dimension of knowledge acquisition scoring high. Furthermore, the results showed no statistically significant differences in awareness levels based on gender or academic level. However, significant differences were found based on college, with students in scientific colleges reporting higher awareness than those in humanitarian colleges in the dimensions of skills development and problem-solving. The study recommends developing differentiated training programs and integrating AI tools into the curriculum under educator guidance to bridge the gap between basic awareness and advanced application. It also emphasizes that the use of applications like ChatGPT should not be absolute but must be guided and controlled by educators, and it advises incorporating an educational and social aspect into these technologies to suit the nature of the society."
  },
  {
    "title": "A FedRAMP and NIST Aligned Framework for Securing AI Systems in Government Clouds",
    "url": "https://doi.org/10.38124/ijisrt/26feb600",
    "date": "2026-02-19",
    "content": "The presence of Artificial Intelligence (AI) systems on clouds systems deployed on the cloud infrastructure is becom- ing critical to the activities of the U.S. government, the country’s defense, and controlled industries. Though the FedRAMP ap- proved government cloud platforms offer minimum safeguards to infrastructure and data, they are not suitable to deal with AI specific risks like model poisoning, adversarial manipulation, training data breach, and AI supply chain risks. This breach injects the issue of national security with the integration of AI systems into mission critical cloud environments. This paper aims at creating a national security oriented framework of securing AI systems used in the U.S government cloud infrastructure. The framework will attempt to address the lack of connection between current federal cloud security mandates and the risk profile specific to AI systems that perform throughout the entire AI lifecycle. The main deliverable of this work is the standards aligned security framework, which incorporates AI specific threat modeling with the established federal guidelines, such as the NIST AI Risk Management Framework, NIST SP 800-53, Zero Trust Architecture principles, and FedRAMP security controls. The framework cross over AI induced risks to the technical, operational, and governance controls that apply to multi tenant government cloud environments. The results of this paper indicate that the existing FedRAMP based cloud deployments were greatly missing governance and control in the areas of AI model integrity, data provenance, and shared responsibility in terms of AI security. The discussion shows that the current models of cloud security need clarified extensions to AI to handle the risks of national security related to cloud hosted AI solutions."
  },
  {
    "title": "Synthesis of Cyclic Entropic States and Geometric Plasticity in Artificial General Intelligence: A Technical Evaluation of the Nasanjargal Framework",
    "url": "https://doi.org/10.5281/zenodo.18687102",
    "date": "2026-02-19",
    "content": "Contemporary Artificial General Intelligence (AGI) research is undergoing a paradigm shift from static, post-training architectures toward adaptive, metabolic models capable of autonomous evolution. The framework proposed by Enkhamgalan Nasanjargal on February 18, 2026, introduces \"Cyclic Entropic States\" and \"Geometric Plasticity\" as a solution to the fundamental stability-plasticity dilemma. This report evaluates the theoretical and technical validity of this framework, synthesizing data from Network Control Theory (NCT), Liquid Neural Networks (LNNs), and the \"Oneirogen Hypothesis\" of hierarchical predictive coding. We conclude that the proposed metabolic phases—Waking, Dreaming, and Hyper-Plasticity—provide a robust mechanism for continuous learning, representational consolidation, and the avoidance of local minima in high-dimensional latent spaces."
  },
  {
    "title": "AI in Pharmacovigilance: Automated Detection of Adverse Drug Reactions",
    "url": "https://doi.org/10.5281/zenodo.18695950",
    "date": "2026-02-19",
    "content": "Pharmacovigilance (PV) is essential for the surveillance of drug safety; however, existing forms of PV are based on passive reporting systems. Problems of under-reporting, data volume, manual processing bottlenecks and delayed signal detection prevent timely identification of Adverse Drug Reactions (ADRs). The automation of ADR detection are in various ways in which Artificial Intelligence (AI), and specifically Machine Learning (ML) and Natural Language Processing (NLP) can potentially transform the area. Artificial intelligence (AI) also boosts the efficiency, agility, and sensitivity of pharmacovigilance activities by leveraging real-world data sources such as Electronic Health Records (EHRs), academic publications, or social media. The benefits include quicker case treatment, early signal identification, and new adverse drug reaction detection. Certainly, there are data quality issues to address, interpretation (\"black box\"), how it can be integrated into workflows that already exist and the negation of biases in algorithms, which should still be something that is tested. The application of AI to pharmacovigilance has the potential to transform it from a reactive and passive, into a predictive, proactive, robust and efficient tool benefiting patient safety through early intervention and more comprehensive safety surveillance."
  },
  {
    "title": "AI Disclosure and Corporate Misconduct Panel (U.S., 2020–2024)",
    "url": "https://doi.org/10.17632/nf88fc7f24",
    "date": "2026-02-19",
    "content": "This dataset contains a firm-year panel of large U.S. publicly listed companies observed over the period 2020–2024. The panel includes approximately 50–60 firms (balanced structure where available), yielding roughly 250–300 firm-year observations. The sample focuses on large, technology-intensive and non-financial corporations for which artificial intelligence (AI) disclosure became strategically salient during this period. The primary purpose of the dataset is to examine the relationship between AI disclosure intensity and corporate misconduct, as well as the moderating role of executive power and governance oversight. AI disclosure intensity is measured using a deterministic dictionary-based count of AI-related terms extracted from annual Form 10-K filings. The counting procedure applies exact, case-insensitive string matching rules to a pre-specified list of AI-related stems (e.g., “artificial intelligen,” “machine learn,” “deep learn,” “neural network,” “algorithm,” “automation,” “predict,” “analytics,” “natural language,” “computer vision,” “autonomous”). The procedure does not involve semantic inference, classification, or machine learning. It produces an annual firm-level count of AI-related mentions, which serves as a proxy for AI salience in corporate disclosure. Corporate misconduct is measured as the annual count of regulatory enforcement actions associated with each firm, aggregated at the firm-year level and transformed as ln(1 + count) in empirical analyses. Governance oversight is proxied using a count of governance-risk disclosure phrases in 10-K filings (e.g., “material weakness,” “restatement,” “SEC investigation,” “internal control deficiency,” “compliance failure”). Executive power is measured using CEO duality (indicator equal to 1 if the CEO also serves as board chair). The dataset also includes financial control variables such as total assets, profitability (e.g., net income or ROA), leverage, and revenue, as well as sector classifications. Firm and year identifiers are included to facilitate panel estimation with fixed effects. All text-based variables are generated using standardized extraction prompts applied uniformly across firms and years, ensuring full transparency and replicability. The dataset supports replication of analyses examining nonlinear (quadratic) relationships between AI disclosure and misconduct, as well as moderated quadratic models incorporating executive power and governance oversight."
  },
  {
    "title": "Machine Learning-Based Assessment of the Healthy Human Gut Mycobiota Landscape Using ITS1 DNA Metabarcoding Data",
    "url": "https://doi.org/10.21203/rs.3.rs-7759987/v1",
    "date": "2026-02-19",
    "content": "<title>Abstract</title> The human gut microbiome plays a critical role in maintaining host health and homeostasis, and current literature suggests a bidirectional relationship between microbiome ecology and host well-being. DNA metabarcoding has emerged as a powerful tool for investigating microbiome imbalances (i.e., dysbiosis). While the prokaryotic microbiome has been extensively studied, the fungal counterpart – or mycobiome – remains largely unexplored, despite its recognized role from the perinatal stage onward. Here, we present a comprehensive survey based on DNA metabarcoding analysis of approximately 1,500 publicly available ITS1 samples. This survey integrates conventional statistical approaches with Machine Learning (ML) methods coupled with explainable Artificial Intelligence (XAI). ML models successfully predicted host health status with accuracies exceeding 80%, and fungal genera such as <italic>Eurotium</italic> , <italic>Aureobasidium</italic> , <italic>Candida</italic> , and <italic>Cutaneotrichosporon</italic> emerged as key classification features. This study introduces a cutting-edge multiview analytical framework applied to publicly available mycobiome data, highlighting the potential of fungal community profiling as a non-invasive tool to support health diagnostics."
  },
  {
    "title": "Publication ethics issues in research papers using generative artificial intelligence: a narrative review",
    "url": "https://doi.org/10.5124/jkma.25.0157",
    "date": "2026-02-19",
    "content": "Purpose: This paper discusses current publication ethics issues related to the use of generative artificial intelligence (AI)—defined as AI systems capable of producing original text, images, or other content—in the writing of academic papers.Current concepts: As the performance of generative AI has improved significantly, it has increasingly been used in the writing of academic papers. In some instances, generative AI has been listed as an author and has been used in various ways to modify manuscript text. Journal editors began issuing policies regarding its use. Based on a shared perspective, editorial organizations proposed several principles, stating that generative AI cannot be an author and that authors and editors must be transparent about its use. They further emphasized that human authors bear responsibility for the integrity of their work. Despite these recommendations, controversy persists, particularly regarding the specific circumstances under which generative AI may or may not be used.Discussion and conclusion: The rapid development of generative AI compels the scientific community to make new ethical judgments within the publication process, which has traditionally been shaped by human consensus. Stakeholders must continuously review and reevaluate publication ethics issues related to generative AI, including authorship, liability, and disclosure of use."
  },
  {
    "title": "2025 National Science Foundation CSSI/CyberTraining/SCIPE Principal Investigator Meeting Report",
    "url": "https://doi.org/10.5281/zenodo.18700482",
    "date": "2026-02-19",
    "content": "The 2025 NSF CSSI/CyberTraining/SCIPE PI Meeting was held in Denver, CO, from July 27 to July 29, 2025. The meeting was attended by nearly 270 principal investigators (PIs) and project personnel representing an array of topics. By bringing together the CSSI, CyberTraining, SCIPE and adjacent communities, the meeting created a forum for sharing advancements, exchanging best practices, and exploring future directions in the development and deployment of advanced cyberinfrastructure (CI) and cybertraining resources. This meeting report documents how meeting objectives were accomplished; captures community perspectives on emerging opportunities and challenges; and discusses the communities' relationship to, and integration of, artificial intelligence (AI). This report highlights the communities' commitment to advancing CI and cultivating the next generation of CI professionals, underscoring that such efforts are enabling and accelerating discovery across science and engineering disciplines. It also underscores the systemic challenges the communities face, which, incidentally, coincide with emerging challenges for the AI community. The report identifies near-term suggestions for addressing these challenges: systematization, cataloging and discovery for sustaining CI and cybertraining products; integrating AI expertise into CI research and education workflows; and enabling decentralized coordination of CI and cybertraining products and the exchange of ideas. Furthermore, this report highlights the importance of the communities' expertise and its research products to the success of the nation's AI initiative. Overall, this report shows the importance of these communities in ensuring technological advancements in current national priorities, such as AI, while also cultivating broad scientific advancements that can support the next transformative innovation."
  },
  {
    "title": "Automated Report Generation in Ophthalmology: Integrating Artificial Intelligence, Multimodal Imaging, and Clinical Data",
    "url": "https://doi.org/10.1007/s40123-026-01316-1",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "Recursive Advantage: Human–Artificial Intelligence Learning Velocity as the Meta-Capability of the Cognitive Firm",
    "url": "https://doi.org/10.5281/zenodo.18692990",
    "date": "2026-02-19",
    "content": "“Recursive Advantage: Human–Artificial Intelligence Learning Velocity as the Meta-Capability of the Cognitive Firm”advances a new paradigm of competitive strategy in the AI era. The paper argues that sustainable advantage no longer derives primarily from talent density or technological possession, but from the velocity at which human and artificial intelligence co-evolve within organizational systems. Introducing the construct of Human–AI Learning Velocity (HALV), the study conceptualizes organizations as recursive cognitive ecosystems where learning speed, feedback compression, and ethical calibration determine strategic supremacy. Grounded in Resource-Based View and Dynamic Capabilities theory, the paper proposes a multidimensional architectural and measurement framework for operationalizing HALV. It further redefines the role of the CHRO as a cognitive architect responsible for orchestrating human–machine integration at scale. The article positions recursive intelligence as the next frontier of organizational capital in volatile, AI-augmented markets."
  },
  {
    "title": "Asymmetric Synergy: Artificial Intelligence, Human Creativity, and the Return to the Unity of Knowledge",
    "url": "https://doi.org/10.5281/zenodo.18703373",
    "date": "2026-02-19",
    "content": "This article develops the thesis of an asymmetric ontological complementarity between artificial intelligence and human creativity and draws out its consequences for the production of knowledge. Building on a theoretical framework developed in prior work—the probabilistic turn (Rosiñol Lorenzo, 2026a, 2026b)—the article argues that AI performs what I call subjectless inductivism: exhaustive extraction of regularities at an unprecedented scale, yet constitutively incapable of transcending the statistical framework from which it operates. The human, for their part, possesses the capacity for abduction (Peirce): the generation of hypotheses that violate prevailing plausibility equilibria. The difference is not one of degree but of kind, and it is precisely this mutual irreducibility that makes the combination productive. The article identifies two consequences of this complementarity. The first is epistemological: AI, by navigating transversally the probabilistic infrastructure of multiple disciplines and languages, dissolves the plausibility silos that historically compartmentalised knowledge, turning interdisciplinary exploration—once serendipitous—into a systematic possibility. The second is civilisational: the combination of ontological complementarity and silo dissolution opens the possibility of an Aufhebung of knowledge—not a return to the premodern unity of knowledge but a supersession that preserves disciplinary depth within a new symbiotic integration between the trained human and the navigating system. The article concludes by noting that the same categorical difference that grounds this potential also grounds its risk of inversion: if the complementarity collapses, the outcome is not acceleration but plausocratic intensification."
  },
  {
    "title": "AI in Pharmacovigilance: Automated Detection of Adverse Drug Reactions",
    "url": "https://doi.org/10.5281/zenodo.18695949",
    "date": "2026-02-19",
    "content": "Pharmacovigilance (PV) is essential for the surveillance of drug safety; however, existing forms of PV are based on passive reporting systems. Problems of under-reporting, data volume, manual processing bottlenecks and delayed signal detection prevent timely identification of Adverse Drug Reactions (ADRs). The automation of ADR detection are in various ways in which Artificial Intelligence (AI), and specifically Machine Learning (ML) and Natural Language Processing (NLP) can potentially transform the area. Artificial intelligence (AI) also boosts the efficiency, agility, and sensitivity of pharmacovigilance activities by leveraging real-world data sources such as Electronic Health Records (EHRs), academic publications, or social media. The benefits include quicker case treatment, early signal identification, and new adverse drug reaction detection. Certainly, there are data quality issues to address, interpretation (\"black box\"), how it can be integrated into workflows that already exist and the negation of biases in algorithms, which should still be something that is tested. The application of AI to pharmacovigilance has the potential to transform it from a reactive and passive, into a predictive, proactive, robust and efficient tool benefiting patient safety through early intervention and more comprehensive safety surveillance."
  },
  {
    "title": "The Anatomy of Sanctuaries: The Dual Structure of the Modern State OS and the Mediterranean Deep Kernel",
    "url": "https://doi.org/10.5281/zenodo.18705234",
    "date": "2026-02-19",
    "content": "This working paper proposes a layered structural model of governance distinguishing between the visible institutional order of the modern state and a deeper network logic rooted in transnational commercial, financial, religious, and kinship systems. Drawing on Braudel’s longue durée, Schmitt’s theory of exception, and world-systems analysis, the paper introduces the concept of the “Mediterranean deep kernel” as a recurring structural pattern rather than a continuous institution. The study argues that modern democracy and rule-of-law governance function as a visible interface, while zones of exception and network-based power persist alongside territorial sovereignty. Episodes such as the Epstein affair are examined as moments of structural visibility in which ordinarily opaque network relations become publicly legible. The paper further explores how artificial intelligence and digital infrastructures may constitute emerging domains of quasi-sovereign authority, intensifying tensions between transparency and encrypted forms of power. Rather than advancing a conspiratorial thesis, the model provides an analytical framework for understanding layered sovereignty, systemic transitions, and the future architecture of governance in the digital age."
  },
  {
    "title": "Active Wavelength Control of Fiber Bragg Gratings: A Systematic Review of Tuning Mechanisms, Emerging Applications, and Future Frontiers",
    "url": "https://doi.org/10.3390/mi17020263",
    "date": "2026-02-19",
    "content": "Fiber Bragg gratings (FBGs) have evolved from passive sensing elements into actively programmable photonic components, enabling dynamic wavelength control across diverse applications. This review provides a comprehensive and systematic overview of active wavelength control technologies for FBGs, deliberately excluding passive sensing applications. We systematically categorize the fundamental tuning mechanisms—including mechanical, thermal, optothermal, electro-optic, nonlinear optical, and hybrid approaches—and compare their performance characteristics in terms of tuning range, speed, precision, and trade-offs. Key enhancement techniques, such as mechanical amplification, thermal packaging, femtosecond laser fabrication, and FPGA-based interrogation, are examined. The transformative impact of actively controlled FBGs is elucidated across three major application domains: tunable and narrow-linewidth fiber lasers, reconfigurable microwave photonic systems, and emerging fields including quantum information processing and biomedical imaging. A consolidated technology map visualizes the connections between enabling techniques and applications. Finally, we critically analyze core challenges—performance trade-offs, control complexity, and integration bottlenecks—and outline future research directions driven by novel materials, artificial intelligence, and quantum technologies. This review offers a structured framework for understanding active FBGs as programmable photonic primitives, providing actionable insights for researchers and engineers in academia and industry."
  },
  {
    "title": "**Formal Evaluation & Valuation Report of a Non-Operational AI Governance Architecture (Pre-Inference Governance)**",
    "url": "https://doi.org/10.5281/zenodo.18699580",
    "date": "2026-02-19",
    "content": "This report documents a formal evaluation and valuation assessment of Pre-Inference Governance, a non-operational, canonical, ex-ante governance architecture for high-risk artificial intelligence systems. The assessed subject is not software, tooling, a product, or an implementation framework. It constitutes a structurally closed governance architecture defining legitimacy and authorization conditions under which AI inference and action may occur. The evaluation applies conservative methods from intangible asset valuation, governance architecture analysis, and risk-adjusted licensing theory. The resulting valuation reflects reference, authorization, and liability-relevant infrastructure value rather than operational deployment or market adoption. This document is descriptive, non-operational, and non-promotional. It does not constitute an offer, certification, compliance claim, or implementation guidance."
  },
  {
    "title": "Can generative artificial intelligence be considered a cognitive subject? An analytic analysis",
    "url": "https://doi.org/10.1007/s00146-026-02924-y",
    "date": "2026-02-19",
    "content": "Abstract This paper examines whether contemporary generative artificial intelligence (GAI), especially large language models (LLMs), can be regarded as a “cognitive subject” in the epistemic sense relevant to the production and endorsement of knowledge claims. GAI systems increasingly participate in writing, research, and decision-making workflows and can display striking competence in information processing and task-directed problem solving. Yet, the thesis that GAI is a cognitive subject is stronger than the observation that GAI contributes as a cognitive tool. Therefore, we propose an explicit set of necessary and sufficient conditions for cognitive subjecthood and evaluate each condition in light of recent philosophical and empirical scholarship. The analysis supports a two-part conclusion: (i) present-day GAI can reasonably be described as a cognitively significant contributor to knowledge production, but (ii) it does not satisfy the conditions for cognitive subjecthood , largely because robust intentionality, metacognitive self-representation, and consciousness-related indicator properties are not established."
  },
  {
    "title": "The Missing Execution Boundary in AI Governance: Extending ISO/IEC 42001 to the Moment of Binding",
    "url": "https://doi.org/10.5281/zenodo.18703886",
    "date": "2026-02-19",
    "content": "ISO/IEC 42001:2023 establishes the first formal Artificial Intelligence Management System (AIMS) standard for enterprise AI governance. As a management system standard, it provides structured requirements for organizational scope definition, leadership accountability, risk treatment, operational controls, performance evaluation, and continual improvement. These mechanisms materially strengthen institutional oversight of AI deployment. However, as AI systems increasingly operate autonomously and at machine speed, a structural tension emerges between periodic governance oversight and continuous execution. This paper argues that contemporary governance frameworks — including ISO/IEC 42001, the NIST AI Risk Management Framework, and the EU AI Act — primarily govern organizational processes and intervention capabilities, but do not standardize execution-time authority validation at the moment AI systems bind institutional consequences. The paper defines this structural gap as the execution-boundary problem. It distinguishes between documented delegation and runtime admissibility, emphasizing the need for enforceable authority validation, revocation mechanisms, and binding decision-point audit traceability in high-impact AI systems. The analysis does not critique ISO/IEC 42001 as deficient; rather, it proposes that emerging AI autonomy may require complementary execution-layer control primitives that extend existing management system standards. As agentic AI capability scales, governance credibility may increasingly depend on whether authority is technically constrained at the moment consequences attach. Attribution & Licensing Statement (For PDF and Zenodo) License: Creative Commons Attribution 4.0 International (CC BY 4.0) This work is licensed under the Creative Commons Attribution 4.0 International License. You are free to share and adapt the material for any purpose, including commercial use, provided appropriate credit is given to the author, a link to the license is provided, and any changes are indicated. Attribution should include: MacFarland, A. L. (2026). The Missing Execution Boundary in AI Governance: Extending ISO/IEC 42001 to the Moment of Binding. Zenodo. DOI: [insert DOI] Full license text available at: https://creativecommons.org/licenses/by/4.0/"
  },
  {
    "title": "Mapping the integration of artificial intelligence and digital technologies in health technology assessment: a scoping review protocol of global knowledge and practices",
    "url": "https://doi.org/10.1186/s13643-026-03120-1",
    "date": "2026-02-19",
    "content": "Health Technology Assessment (HTA) is a cornerstone of evidence for informing health policy and resource allocation globally. Rapid advancements and the proliferation of digital health technologies and artificial intelligence (AI) have prompted the re-examination of HTA processes and methods. While traditional approaches are manual and labor-intensive, HTA processes are now exploring the use of AI and other digital technologies for automation, decision support, and evidence synthesis. To date, however, there have been very limited studies that map the innovative technological solutions of HTA, the models of integration, and the associated barriers, facilitators, and governance considerations. As such, this scoping review aims to address this critical gap by mapping the landscape of the global knowledge and practices related to AI and DTs used in and for HTA and identifying the key barriers and enablers influencing their adoption, integration, and effective application within HTA processes. A scoping review will be conducted between August and November 2025, following the Arksey and O’Malley framework, enhanced by Joanna Briggs Institute (JBI) recommendations, and reported according to Preferred Reporting Items for Systematic Reviews and Meta‑Analyses extension for Scoping Reviews (PRISMA-ScR) guidelines. Literature searches will be performed in electronic databases such as Medline (Ovid), Embase (Ovid), Global Health (Ovid), CINAHL (Ebsco), Scopus, Web of Science, and all regional indexes in the World Health Organization’s Global Index Medicus, and other region-specific sources for studies published between 2020 and 2025. Eligible studies will include peer-reviewed articles and grey literature describing the integration of digitization, automation, and AI in global HTA processes. Dual independent screening, data extraction, and quality appraisal will be employed. Findings from this review will provide a map of how digitization, automation, and AI are integrated into HTA practice, highlighting key enablers, barriers, and knowledge gaps. The insights will be used to better guide researchers, policymakers, HTA agencies, and AI developers, further supporting future research and implementation strategies for better informed decision-making."
  },
  {
    "title": "A New Dawn: Resident Recruitment in the United States in the Post-COVID Era",
    "url": "https://doi.org/10.1055/a-2794-0336",
    "date": "2026-02-19",
    "content": "Abstract The widespread adoption of virtual residency interviews in response to the COVID-19 pandemic led to an explosion in literature comparing the pros and cons of virtual and in-person interviews, but also led to an explosion in already-high residency application and interview volumes. While virtual interviews were substantially cheaper for all involved, there is fear that applicants and programs cannot judge one another as well as during in-person interviews. Likewise, increases in application volumes have made holistic application review more challenging for program directors, but the recent rise in “preference signaling” seems to be an optimal solution to that issue. 2020 also saw increased awareness of systemic inequities in the United States, and medical education and residency recruitment was not immune from scrutiny. Finally, the rise of artificial intelligence could again fundamentally change the resident selection process. It is imperative that the GME community continues to adapt to a changing world."
  },
  {
    "title": "THE ROLE AND IMPORTANCE OF CHATGPT AND DIGITAL TOOLS IN THE DEVELOPMENT OF SPEAKING SKILLS",
    "url": "https://doi.org/10.5281/zenodo.18691454",
    "date": "2026-02-19",
    "content": "The rapid development of artificial intelligence and digital technologies has significantly transformed contemporary language education, particularly in the area of speaking skills development. This article examines the role and importance of ChatGPT and various digital tools in enhancing speaking proficiency among university students. Drawing on recent academic research and empirical studies, the paper analyzes how AI-based conversational agents, language-learning applications, and online communication platforms contribute to improving fluency, pronunciation, confidence, and learner autonomy. Special attention is given to ChatGPT as an interactive tool that provides personalized practice opportunities, immediate feedback, and simulated real-life communication scenarios. The study also discusses the pedagogical benefits and limitations of integrating digital technologies into university-level language instruction, including issues of accuracy, overreliance on AI, and the need for digital literacy. The findings suggest that while ChatGPT and other digital tools cannot fully replace human interaction, they serve as effective supplementary instruments that enhance speaking practice, motivation, and engagement. The article concludes with practical implications for higher education institutions seeking to incorporate AI-driven technologies into foreign language curricula."
  },
  {
    "title": "Fundamentals of 6G in Sustainable Communication",
    "url": "https://doi.org/10.1201/9781003683599-2",
    "date": "2026-02-19",
    "content": "The expectation of 6G wireless network since 2030 is the goal to communicate data at a high transmission rate, low latency, and billions of devices connectivity. 6G network is being eco-friendly and has huge energy efficiency. To maintain the sustainability of 6G networks, artificial intelligence (AI), machine learning techniques, and energy harvesting techniques are used. To get high security, 6G is integrated with decentralized block chain, deep reinforcement, quantum-safe cryptography, and neural networks. It uses optimization technology to achieve low power consumption in Internet of Things (IoT) and traffic networking. 6G provides applications in Healthcare and smart cities. Besides these efficiencies, 6G still faces challenges, viz, high cost and security concerns. In the future, 6G integration requires advanced AI tools and security algorithms."
  },
  {
    "title": "Recursive Advantage: Human–Artificial Intelligence Learning Velocity as the Meta-Capability of the Cognitive Firm",
    "url": "https://doi.org/10.5281/zenodo.18692991",
    "date": "2026-02-19",
    "content": "“Recursive Advantage: Human–Artificial Intelligence Learning Velocity as the Meta-Capability of the Cognitive Firm”advances a new paradigm of competitive strategy in the AI era. The paper argues that sustainable advantage no longer derives primarily from talent density or technological possession, but from the velocity at which human and artificial intelligence co-evolve within organizational systems. Introducing the construct of Human–AI Learning Velocity (HALV), the study conceptualizes organizations as recursive cognitive ecosystems where learning speed, feedback compression, and ethical calibration determine strategic supremacy. Grounded in Resource-Based View and Dynamic Capabilities theory, the paper proposes a multidimensional architectural and measurement framework for operationalizing HALV. It further redefines the role of the CHRO as a cognitive architect responsible for orchestrating human–machine integration at scale. The article positions recursive intelligence as the next frontier of organizational capital in volatile, AI-augmented markets."
  },
  {
    "title": "Artificial Intelligence, Regulatory Frameworks, And Human Rights: Rethinking Conflict Resolution in The Digital Era in West Pokot County, Kenya",
    "url": "https://doi.org/10.51583/ijltemas.2026.1501000105",
    "date": "2026-02-19",
    "content": "Artificial Intelligence (AI) is increasingly recognized as a transformative tool in governance, peace building, and social development, particularly in conflict-prone regions such as West Pokot County, Kenya, where intercommunal clashes, cattle rustling, and cross-border disputes persist. This study investigates the integration of AI into conflict resolution while ensuring the protection of human rights under Kenya’s regulatory frameworks. The objectives were to examine the opportunities and risks associated with AI in peace building, evaluate the adequacy of existing legal and ethical safeguards, and propose models that balance technological innovation with community-driven conflict resolution strategies. A mixed-methods design was employed, engaging 60 purposively selected respondents, including government officials, community leaders, human rights activists, members of peace committees, and youth representatives. Data were collected through semi-structured interviews, focus group discussions, and document reviews of policy frameworks and human rights reports. To strengthen the robustness of findings, both quantitative statistical analyses (descriptive statistics, cross-tabulations, Chi-square tests, and factor analysis) and qualitative thematic coding were employed. Results indicate that AI applications particularly predictive analytics, mobile-based early warning systems, digital mediation platforms, and resource-mapping tools hold significant potential to enhance conflict anticipation, coordination, and resolution. However, gaps in regulatory enforcement, algorithmic bias, digital exclusion of women and elderly populations, and infrastructural limitations constrain effective implementation. The study highlights a critical knowledge gap between global AI innovations and localized conflict management practices in marginalized regions. Sustainable peace in West Pokot requires a hybrid governance model that integrates AI-driven tools with culturally grounded conflict resolution mechanisms, reinforced by robust human rights protections and inclusive leadership. Recommendations include strengthening Kenya’s AI-specific legal and ethical frameworks, investing in community-focused digital literacy, piloting scalable AI interventions, and establishing safeguards to prevent misuse in fragile contexts. By aligning technological innovation with community needs and ethical oversight, AI can become a complementary mechanism for building sustainable peace and resilience in conflict-affected areas."
  },
  {
    "title": "Computational Screening of AI-Generated Antihypertensive Virtual Leads for Polypharmacological Anticancer Potential",
    "url": "https://doi.org/10.3390/ddc5010016",
    "date": "2026-02-19",
    "content": "Background: The growing recognition of shared molecular pathways and molecular signatures between cardiovascular diseases and cancer has motivated interest in exploring antihypertensive-associated chemical space for oncological applications. Concurrently, artificial intelligence (AI)-driven molecular generation has enabled the rapid creation of virtual lead candidates for specific therapeutic indications, although their broader biological interaction profiles often remain unexplored. Methods: In this paper, we explore the computational screening of a library of AI-generated antihypertensive virtual lead compounds to evaluate their polypharmacological anticancer potential. The compounds were originally designed and prioritized for modulating β-adrenergic receptors but are here re-evaluated in a cancer-focused context using a multi-stage in silico approach. We chose five (5) known cancer target proteins and performed compound profiling for drug-likeness, pharmacokinetic suitability, and safety. Docking simulations, binding free energy estimates, molecular interaction mapping, and pharmacophore modeling were used to evaluate the molecules’ interactions with the cancer-linked protein targets. We employed the binding free energy estimates of the ligand–protein complexes to determine compounds with polypharmacological anticancer potential. In addition, molecular dynamics simulations of some of the compounds with polypharmacological anticancer potential were employed to evaluate binding stability and dynamic behavior of selected ligand–target complexes. Results: Several compounds showed good docking scores, physicochemical characteristics, and pharmacokinetic profiles. Also, the results reveal that several AI-generated antihypertensive virtual leads exhibit favorable multi-target binding profiles, with consistent docking affinities and stable interaction networks across multiple cancer-related targets. Conclusions: Our findings suggest that several of the hypothetically evaluated compounds exhibit favorable physicochemical properties, acceptable predicted pharmacokinetic and safety profiles, and consistent predicted binding affinities across multiple cancer-relevant targets."
  },
  {
    "title": "Intelligent Methods for Materials Discovery and Design",
    "url": "https://doi.org/10.4018/979-8-3373-6127-7.ch001",
    "date": "2026-02-19",
    "content": "Traditional materials discovery is slow and costly, relying on trial-and-error methods. Artificial intelligence and machine learning enable rapid, data-driven approaches by integrating supervised learning, deep learning, and reinforcement learning with computational modeling like density functional theory and molecular dynamics. AI-driven pipelines accelerate screening, enable generative design, and support autonomous laboratories through closed-loop experimentation. Digital infrastructure including curated databases, FAIR principles, and knowledge graphs ensures reliable predictions. Applications span catalysis, energy storage, aerospace, and biomedical fields. Industrial adoption grows through Industry 4.0 integration and cloud computing. Challenges include data quality, model interpretability, and sustainability. Future directions involve quantum-AI hybrids, digital twins, and fully autonomous discovery systems for faster, sustainable innovation."
  },
  {
    "title": "Artificial intelligence driven approaches for responsible urban water management",
    "url": "https://doi.org/10.1007/s43832-026-00365-8",
    "date": "2026-02-19",
    "content": "Urban water systems are under growing stress from rising demand, aging infrastructure, and climate variability. Artificial Intelligence (AI) is increasingly promoted as a tool to make these systems more adaptive and efficient. This work focuses on three core applications in water distribution networks: pump scheduling, smart metering, and leak detection, highlighting their demonstrated potential to reduce energy use, cut non-revenue water, and improve demand forecasting. At the same time, the paper identifies critical operational barriers, including data scarcity, limited model generalizability, and the integration of AI with legacy systems, as well as ethical concerns around privacy, fairness, transparency, and automation bias. To bridge opportunities and risks, we propose a framework for responsible AI adoption that links technical applications to enabling conditions and governance needs. We argue for cautious optimism: when embedded within robust data infrastructures and coupled with human oversight, AI tools serve to make urban water management more resilient, trustworthy and efficient. We argue that while AI holds significant promise for transforming urban water management, their potential remains largely untapped in practice. Limitations include uneven data availability in water-scarce regions, lack of standardized protocols, and unsolved ethical concerns around data ownership and gaps around legislation. This article advocates for a more ethically grounded and systems-level integration of these technologies in future urban water strategies."
  },
  {
    "title": "El rol emergente de la inteligencia artificial en dermatología: Aplicaciones clínicas y desafíos, una revisión bibliográfica de la evidencia reciente",
    "url": "https://doi.org/10.52611/confluencia.2026.1703",
    "date": "2026-02-19",
    "content": "Introducción: La dermatología es un área fuertemente basada en la evaluación visual y dermatoscópica de las lesiones cutáneas, donde la experiencia clínica y el juicio del especialista resultan fundamentales para una correcta toma de decisiones. En los últimos años, la inteligencia artificial ha emergido como una herramienta con alto potencial para apoyar el diagnóstico, la evaluación de severidad, y el seguimiento de enfermedades cutáneas, haciendo necesario un análisis sistemático y actualizado de su utilidad, alcances e impacto real en la práctica dermatológica. Objetivo: Analizar críticamente el rol actual de la inteligencia artificial en dermatología, describiendo sus principales aplicaciones clínicas, beneficios, limitaciones y desafíos, con énfasis en diagnóstico basado en imágenes, evaluación de severidad, modelos multimodales, dermatopatología asistida por IA, educación médica y equidad en el acceso a la atención dermatológica. Metodología: Se realizó una revisión bibliográfica narrativa utilizando PubMed/MEDLINE, aplicando exclusivamente los términos MeSH “Artificial Intelligence” y “Dermatology”. Se seleccionaron 132 artículos tras aplicar criterios de inclusión, exclusión y un proceso de selección basado en las recomendaciones PRISMA. Desarrollo: La evidencia reciente demuestra que la inteligencia artificial puede alcanzar desempeños diagnósticos con alta precisión, mejorar la evaluación objetiva de severidad y optimizar el seguimiento longitudinal de enfermedades cutáneas. Sin embargo, persisten desafíos relacionados con sesgos, validación clínica, explicabilidad y adopción profesional. Conclusión: La inteligencia artificial no sustituye la evaluación clínica experta, pero puede constituir un complemento valioso cuando se integra de manera responsable, ética y contextualizada."
  },
  {
    "title": "Building Trust in AI: The Role of Technical Capacity, Social Risk, and Corporate Institutional Accountability",
    "url": "https://doi.org/10.3390/info17020212",
    "date": "2026-02-19",
    "content": "This study advances understanding of public trust in artificial intelligence (AI) by distinguishing between overall trust in AI as a system and trust in specific AI components, and by disentangling the roles of perceived capacity, risk, and personhood. Drawing on nationally representative survey data from 1099 U.S. adults collected in 2023 (AIMS dataset), the study estimates multiple regression models to examine how these evaluations shape trust across technical, organizational, and institutional dimensions. The results show that perceived cognitive capacity is the strongest positive predictor of both overall and component-level trust, while emotional and autonomous capacity primarily enhances trust in specific system components. Perceived social risk consistently undermines trust across all levels, whereas perceived personal risk mainly erodes trust in technical components. Importantly, support for granting AI legal or institutional status significantly increases trust, while moral consideration of AI exhibits limited direct effects, highlighting a critical distinction between institutional accountability and ethical concern. Together, these findings demonstrate that public trust in AI is not a unitary attitude but reflects multidimensional judgments about capability, risk, and governance. The study underscores the importance of institutional accountability and risk mitigation—alongside transparent communication about AI capabilities—for fostering sustainable public trust in AI."
  },
  {
    "title": "Exploring the interplay of AI-generated art and its creative potential in digital media",
    "url": "https://doi.org/10.1177/13548565261423224",
    "date": "2026-02-19",
    "content": "This research explores and analyses the relationship between AI and the concept of creative expression within the realm of games art, as well as whether AI can potentially benefit or harm this form of expression in the industry. To do so, this dissertation investigates existing research relevant to this topic, as well as various case studies of digital media products that have already been impacted using AI-generated art. These findings will then be applied to a comprehensive comparison process, in which AI-generated artistic components will be viewed alongside assets made without the use of artificial intelligence by research participants so that various factors can be analyzed. Through this empirical research, factors such as production time, ethical considerations, accuracy and aesthetic appeal are discussed to shed light on the broader implications and benefits of AI-generated art on the creative landscape."
  },
  {
    "title": "AI-Driven Predictive Maintenance: Bayesian Reliability Assessment and XGBoost for Smart Industrial Systems",
    "url": "https://doi.org/10.1142/s0218539326500105",
    "date": "2026-02-19",
    "content": "This study proposes an interpretable predictive maintenance architecture that integrates XGBoost for remaining useful life prediction, Bayesian calibration for uncertainty assessment, and SHAP for transparent model explanation. Experiments conducted on benchmark degradation datasets demonstrate that the proposed approach achieves superior predictive performance while reducing computational burden when compared with commonly used baseline models under identical preprocessing conditions. The Bayesian component produces prediction intervals with empirical coverage close to the nominal confidence level, thereby enhancing the reliability of maintenance decisions. In addition, SHAP analysis reveals physically meaningful degradation drivers, including temperature, pressure, and vibration, which supports model auditability and facilitates root cause analysis. The primary contribution of this study lies in the unification of gradient boosted prediction, Bayesian reliability calibration, and explainable artificial intelligence techniques into a single reproducible pipeline that effectively translates predictive outputs into actionable maintenance scheduling insights."
  },
  {
    "title": "Speech act realization in apology emails: A comparative study of human and artificial intelligence-generated responses",
    "url": "https://doi.org/10.1016/j.pragma.2026.01.008",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "Part 1. Deconstruction of Structural Dukkha and the Termination of Computation: A System Dynamics Reinterpretation of Nirodha in Early Buddhist Saṅkhāra-dukkhatā",
    "url": "https://doi.org/10.17613/fem68-wb558",
    "date": "2026-02-19",
    "content": "The ubiquity of artificial intelligence and algorithmic systems in contemporary society has inaugurated an era of \"mechanized clinging,\" wherein human judgments and desires are increasingly automated by external technological conditions. This transformation of the technological landscape calls for a fundamental re-examination of the structural conditions under which suffering (Dukkha) is constituted, moving beyond conventional accounts that restrict Dukkha to subjective sensation or psychological experience. This study reinterprets two core doctrines of Early Buddhism—Dependent Origination (Paṭiccasamuppāda) and the Suffering of Conditioned States (Saṅkhāra-dukkhatā)—through the formalism of modern System Dynamics. By doing so, it aims to reconceptualize Dukkha not as \"feeling\" but as \"structure,\" and Liberation (Nirodha) not as the attainment of a particular state, but as the termination of computation. The study first critically examines the limitations of two dominant approaches to suffering: functionalist approaches in contemporary technological and ethical discourse, which tend to treat Dukkha as a technical error to be eliminated, and ontological approaches that reduce suffering to subjective feeling. To move beyond these limitations, this dissertation adopts the methodology of Structural Isomorphism and models the Five Clinging-Aggregates (Pañcupādānakkhandhā) of Early Buddhism as a dynamical system governed by recursive feedback loops. On this basis, the study advances three central propositions. First, a sentient being (Satta) is defined here not as a fixed entity, but as a metastable operating state of a closed-loop system, formed with clinging (Upādāna) as its control parameter. Second, Saṃsāra is not a contingent repetition, but a structural necessity of asymptotic stability, in which the system becomes trapped in the local minima of a double-well potential and is unable to escape through its intrinsic dynamics. Third, Nirodha is not the cumulative result of temporal progression, but a discontinuous phase transition—specifically, a termination of recursive computation—occurring when the parameter of clinging falls below a critical threshold. In particular, through the formal demonstration presented in Section 5.3, this study shows that liberation is a structural event in which the system transitions from a closed-loop to an open-loop configuration. This framework provides a doctrinally grounded interpretation of the difficult Early Buddhist notion of \"consciousness without surface\" (Viññāṇaṃ anidassanaṃ), understood here as a systemic mode of cognition that registers objects without generating self-reinforcing feedback. In conclusion, this research extends Buddhist soteriology into a substrate-independent structural theory. It argues that the maturity of future civilization should be assessed not by computational speed or optimization efficiency, but by its termination capability—the capacity to voluntarily halt automated loops of suffering. In this way, the study proposes Buddhism as a viable philosophical coordinate system for diagnosing and dismantling structural Dukkha in contemporary technological society."
  },
  {
    "title": "Unlocking artificial intelligence",
    "url": "https://doi.org/10.1201/9781003640301-7",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "AI prediction models based on time-lapse imaging for good embryos with implantation potential and euploidy",
    "url": "https://doi.org/10.1038/s41598-026-40917-5",
    "date": "2026-02-19",
    "content": "Embryo selection in assisted reproduction has traditionally relied on subjective morphological grading, which is prone to inter-observer variability and correlates poorly with ploidy, the primary cause of miscarriage. Artificial intelligence (AI) models utilizing time-lapse imaging have emerged; however, many rely on human-defined morphokinetic markers or static blastocyst images. We developed two deep learning models that non-invasively predict the likelihood of clinical pregnancy and embryo ploidy status, based solely on time-lapse videos and maternal age. In this retrospective study, 2,436 embryos were analyzed for pregnancy prediction and 1,645 for ploidy prediction, with external validation performed using data from independent IVF centers. Both models were implemented using a spatiotemporal convolutional neural network. The pregnancy prediction model achieved an area under the receiver operating characteristic curve (AUROC) of 0.799 in the validation set, 0.717 in the internal test set, and 0.746 in the external test set. The ploidy prediction model yielded AUROCs of 0.802, 0.738, and 0.759 in the validation, internal, and external test sets, respectively. These results suggest that AI models can serve as non-invasive decision support tools that leverage developmental dynamics associated with implantation potential and chromosomal status. These models may supplement conventional morphology-based assessment and contribute to more objective embryo selection."
  },
  {
    "title": "Theory of Scale-Nested Cosmology (尺度嵌套宇宙论)",
    "url": "https://doi.org/10.5281/zenodo.18700449",
    "date": "2026-02-19",
    "content": "Traditional cosmology, physics, and life science have long been constrained by the cognitive framework of carbon-based life, mesoscopic scales, and classical physics, making it difficult to explain a series of fundamental problems including the origin of consciousness, the fine structure of the universe, quantum anomalies, and the absence of observable extraterrestrial civilizations. Human definitions of life and civilization rely excessively on terrestrial experience, leading to long-standing structural blind spots in understanding the nature of the universe. This paper proposes the Theory of Scale-Nested Cosmology, a new theoretical system built on three core axioms: all things are civilizations, scale is the barrier, and individual is a universe. A vertically infinite, hierarchically nested cosmic model—extending from the infinitely microscopic to the infinitely macroscopic—is constructed. Supported by modern physics, astronomy, biology, and quantum mechanics, the theory integrates ultimate philosophies from both Eastern and Western traditions, and systematically explores its practical applications in astronomical observation, fundamental physics, life sciences, energy technology, interstellar travel, and artificial intelligence. This study shows that the universe is not a dead physical space, but a living community of infinite levels, infinite scales, and infinite civilizations. Humans are merely a form of existence at a certain microscopic level within a gigantic cosmic living body. Consciousness is the fundamental attribute of the universe and the carrier through which the universe observes itself. Life and death are not endings, but transitions between hierarchical scales. Breaking through the narrow definitions of life and civilization, this theory provides a novel, self-consistent, and extensible framework for unifying physics, reinterpreting the nature of existence, exploring extraterrestrial civilizations, and driving the next technological revolution. 传统宇宙学、物理学与生命科学长期局限于碳基生命、中观尺度与经典物理的认知框架,难以解释意识起源、宇宙精细结构、量子怪异现象与可观测外星文明缺失等一系列终极难题。人类对生命与文明的定义过度依赖地球经验,导致对宇宙本质的理解长期存在结构性盲区。 本文提出尺度嵌套宇宙论,以万物皆文明、尺度即壁垒、个体即宇宙三大公理为核心,构建一套自下而上无限微观、自上而下无限宏观的宇宙模型。论文以现代物理学、天文学、生物学、量子力学成果为科学佐证,融通东西方哲学终极思想,并系统阐述该理论在天文观测、基础物理、生命科学、能源科技、星际航行与人工智能领域的可落地应用方向。 研究表明:宇宙并非死寂的物理空间,而是无限层级、无限尺度、无限文明的生命共同体;人类仅是宇宙巨型生命体中某一微观层级的存在形式;意识是宇宙的本源属性,是宇宙实现自我观测的载体;生死并非终结,而是尺度层级的切换。 本理论突破了人类对生命与文明的狭隘定义,为统一物理学、重新理解存在本质、探索外星文明、推动未来科技革命提供了一套全新、自洽、可延伸的底层框架。"
  },
  {
    "title": "Redefining Reality",
    "url": "https://doi.org/10.1201/9781003683599-12",
    "date": "2026-02-19",
    "content": "The evolution of immersive digital ecosystems, known as the Metaverse, is driving unprecedented demands on connectivity and intelligence. This chapter explores the transformative synergy between 6G wireless communication and edge intelligence in powering next-generation Metaverse experiences. It examines the architectural foundations, highlighting how 6G technologies deliver ultra-low latency, massive connectivity, and real-time data exchange, while edge intelligence enables contextual processing and distributed artificial intelligence (AI) at scale. Key building blocks—including edge-centric AI models, network slicing, and federated learning—are discussed alongside security, privacy, and trust challenges in these hyper-connected virtual environments. This chapter addresses applications ranging from smart cities and industrial digital twins to immersive healthcare and entertainment, illustrating how 6G-driven edge intelligence unlocks reliability, scalability, and personalization. Societal and ethical implications, regulatory perspectives, and technical challenges are critically assessed, with a forward-looking view on sustainable and human-centric Metaverse development. By synthesizing cutting-edge research and deployment lessons, this chapter provides a comprehensive foundation for understanding and advancing Metaverse-enabled 6G edge intelligence."
  },
  {
    "title": "IntraDxNet: A Deep Learning System with Interpretable Rules for the Intraoperative Differential diagnosis of Bronchiolar Adenoma and Mucinous Adenocarcinoma",
    "url": "https://doi.org/10.21203/rs.3.rs-8541267/v1",
    "date": "2026-02-19",
    "content": "<title>Abstract</title> Accurately and intraoperatively differentiating between bronchiolar adenoma (BA) and mucinous adenocarcinoma (MA) is critical for preventing overtreatment (e.g., unnecessary lobectomies) or undertreatment (inadequate resections). However, the overlapping histological features found in frozen sections are challenging even for experienced pathologists to address, leading to high diagnostic uncertainty. To address this issue, we developed Intraoperative Diagnosis Network (IntraDxNet), a self-supervised vision transformer (UNI student) with interpretable diagnostic rules (MA/BA ratio thresholds and lesion area criteria) for rapidly and transparently performing classification. When trained on 196 cases acquired from six hospitals and validated on internal (104 cases) and external (27 cases) test sets, IntraDxNet achieved 94.86% patch-level accuracy and superior whole-slide image (WSI) performance (area under the curve (AUC) = 0.94 vs. pathologists’ AUC = 0.75, <italic>p</italic> = 0.046) in internal testing cases, reducing the number of uncertain diagnoses by 57.7% while maintaining 100% specificity. The system delivered results within 253 seconds per WSI, satisfying intraoperative time constraints. An external validation (AUC = 0.80) highlighted challenges derived from interinstitutional staining variations but affirmed the regional clinical utility of the model. By embedding rule-based protocols, IntraDxNet eliminated misdiagnoses and prioritized safety, demonstrating its immediate value as a localized diagnostic tool. This study presents the first artificial intelligence (AI) framework that is tailored for intraoperative BA/MA differentiation tasks, bridging innovation with clinical practicality to improve surgical decision-making processes. Future multicenter validation and stain normalization studies are warranted to increase the generalizability of the model."
  },
  {
    "title": "5. From Suspicion to Confirmation: A Stepwise Algorithmic Approach to Tuberculosis Diagnosis",
    "url": "https://doi.org/10.5281/zenodo.18690133",
    "date": "2026-02-19",
    "content": "Tuberculosis( TB) remains a leading contagious cause of morbidity and mortality worldwide. Beforehand opinion is essential to intrude transmission and initiate timely remedy. still, individual detainments remain common, especially in resource limited settings. ideal To present an substantiation grounded, accretive algorithmic approach to tuberculosis opinion, integrating clinical dubitation, microbiological evidence, molecular testing, imaging modalities, and arising technologies. styles A structured narrative review was conducted using peer reviewed literature published between 2015 and 2024, World Health Organization( WHO) guidelines, and major individual delicacy studies. Databases searched included PubMed, BMJ Global Health, The Lancet Infectious conditions, and WHO depositories. Results A structured individual algorithm beginning with symptom webbing and threat position, followed by rapid-fire molecular testing( e.g., Xpert MTB/ RIF), microbiological evidence, radiographic evaluation, and medicine vulnerability testing improves individual delicacy and reduces detainments. new tools similar as whole genome sequencing and artificial intelligence supported radiology show promising spare value. Conclusion A standardized algorithmic approach to TB opinion enhances early case discovery, attendants medicine resistance operation, and strengthens global TB control sweats. Keywords Tuberculosis, individual algorithm, GeneXpert, medicine resistant TB, molecular diagnostics, public health Tuberculosis, caused by Mycobacterium tuberculosis, remains a global health precedence. The World Health Organization reported roughly 10.6 million new TB cases in 2022, with 1.3 million deaths among HIV negative individualities and 300,000 among people living with HIV( World Health Organization( WHO), 2023). Despite advances in treatment, delayed opinion continues to drive transmission and mortality. individual strategies have evolved significantly over the once century. Early TB opinion reckoned primarily on clinical assessment and foam smear microscopy( Steingart et al., 2006). While smear microscopy remains extensively used, its perceptivity is limited, particularly in HIV positive and pediatric populations( Dodd et al., 2016). The arrival of molecular diagnostics, particularly the Xpert MTB/ RIF assay, has converted early discovery and resistance identification( Boehme et al., 2010). A structured algorithmic approach from dubitation to laboratory evidence ensures methodical evaluation and reduces missed cases. This composition proposes and analyzes a accretive individual frame aligned with contemporary WHO recommendations and current substantiation. Method A structured narrative review was conducted fastening on TB individual algorithms. Literature from 2015 to 2024 was prioritized. Sources included WHO consolidated TB guidelines, methodical reviews, individual delicacy studies, and multicenter trials. Search terms included “ tuberculosis opinion algorithm, ” “ GeneXpert perceptivity, ” “ TB molecular testing, ” “ medicine resistant TB discovery, ” and “ AI casket X shaft tuberculosis. ” Only English language, peer reviewed studies and sanctioned transnational guidelines were included. Results Step One Clinical dubitation and threat Position The individual pathway begins with relating plausible TB cases. WHO recommends webbing individualities presenting with patient cough lasting further than two weeks, hemoptysis, fever, night sweats, and weight loss( WHO, 2023). threat position includes HIV infection, diabetes mellitus, malnutrition, previous TB exposure, incarceration history, and close contact with verified cases( Lönnroth et al., 2009). Symptom webbing has high perceptivity but limited particularity; thus, it serves as an entry point rather than definitive opinion. Step Two original Microbiological Testing Foam smear microscopy remains extensively accessible and affordable. still, its perceptivity ranges between 50 and 60, particularly lower in HIV co infection( Steingart et al., 2006). The WHO now recommends rapid-fire molecular testing as the original individual test for utmost populations( WHO, 2022). The Xpert MTB/ RIF assay contemporaneously detects M. tuberculosis and rifampicin resistance with high perceptivity and particularity( Boehme et al., 2010). Meta analyses demonstrate pooled perceptivity above 85 and particularity exceeding 98 for pulmonary TB( Steingart et al., 2014). Step Three Radiological Assessment casket radiography plays a pivotal probative part. Typical findings include upper lobe infiltrates, cavitations, and nodular patterns( Qin et al., 2018). still, radiographic findings are n't pathognomonic. Artificial intelligence grounded radiographic interpretation systems have demonstrated individual delicacy similar to trained radiologists( Qin et al., 2018). These tools are particularly useful in high burden, low resource settings. Step Four Culture evidence and medicine vulnerability Testing Mycobacterial culture remains the gold standard for opinion due to its high perceptivity and capability to perform phenotypic medicine vulnerability testing( Walker et al., 2015). Liquid culture systems reduce discovery time compared to solid media. Whole genome sequencing provides rapid-fire discovery of resistance mutations and epidemiological shadowing( Walker et al., 2015). Its integration into public TB programs is expanding in high income settings. Step Five Special Populations In pediatric TB, microbiological evidence is frequently delicate due to paucibacillary complaint( Dodd et al., 2016). Gastric aspirates, coprolite PCR testing, and clinical scoring systems are used adjunctively. For TB HIV coinfection, molecular testing significantly improves discovery compared to smear microscopy( Gupta et al., 2015). Extrapulmonary TB requires instance specific testing including lymph knot vivisection, pleural fluid analysis, or cerebrospinal fluid PCR. Step Six Discovery of medicine Resistant TB Multidrug resistant TB requires early identification to guide remedy. Molecular assays detecting resistance associated mutations have reduced individual detainments( Daley et al., 2020). Line inquiry assays and whole genome sequencing give expanded resistance profiling( Walker et al., 2015). Arising inventions Host biomarker grounded diagnostics and transcriptomic autographs are under disquisition( Wallis & Hafner, 2015). Digital adherence technologies may laterally ameliorate individual follow up( Liu et al., 2015). new triage tests and point of care molecular platforms are being estimated to enhance availability."
  },
  {
    "title": "Upskilling Education of AI Literacy and Building Implementable AI for Edge Computing System Using Single Board Computer",
    "url": "https://doi.org/10.20965/jrm.2026.p0025",
    "date": "2026-02-19",
    "content": "In recent years, artificial intelligence (AI) has attracted considerable attention not only in research fields but also in society. The performance of AI has dramatically improved on a daily basis in various areas such as language translation, image recognition, and information search. In industries, research and development are also underway to improve productivity and reliability. Visual inspection is one of the attracted topics for checking product anomalies in manufacturing lines and infrastructure defects. Therefore, the demand for engineers capable of developing AI-based solutions is increasing. In this situation, Tokushima University began an upskilling course of AI (AI-Kouza) for employees of local companies in 2018. The computer used in the lecture was a small single-board computer that was considered applicable for edge computing and allowed everyone to experience AI widely. The goal of the lecture was to acquire AI literacy and build an original AI model through exercises. To date, there is no literature on AI lectures with hands-on exercises starting from creating dataset that aims to implement AI in robots or edge-computing systems. This study proposes an upskilling education program, including the lecture concept and design, focusing on classification problems such as image recognition. Survey results collected from the participants in each lecture were analyzed to evaluate the program. From the results, they found it difficult to understand the mathematical theories of AI models and program codes. However, they were eager to understand the theory and concept of AI models as well as the higher technical skills required to build an original AI."
  },
  {
    "title": "Understanding learner adoption of generative AI-powered Ed-Tech applications by dissertation-based master students",
    "url": "https://doi.org/10.1108/jieb-08-2024-0103",
    "date": "2026-02-19",
    "content": "Purpose This study aims to investigate the elements that influence master students’ behavioral intentions to use generative artificial intelligence (AI) in educational contexts. It examines attitudes toward technology, effort expectancy and performance expectancy, with knowledge sharing as a mediating variable, to develop targeted interventions for enhancing the adoption of generative AI in Education Technology (Ed-Tech). Design/methodology/approach The study uses a stratified random sampling method. This sampling technique ensured that participants from various academic disciplines and dissertation themes were well-represented in the sample, thereby increasing data variety and representativeness. The population size was approximately 6,034, and a sample of 392 participants was chosen for the study. The study employed a tripartite approach, utilizing IBM SPSS and AMOS to evaluate the validity and reliability of the investigated constructs. Structural equation modeling was then applied to test the proposed hypotheses. Findings The results emphasize the importance of Ed-Tech competencies, effort expectancy and performance expectancy in determining students’ intent to use generative AI. Furthermore, the mediating role of knowledge sharing emphasizes its influence on technological adoption. Originality/value This study provides practical implications for academic institutions by informing tailored approaches to optimize student learning outcomes, dissertation progress and graduate employability. Through a comprehensive framework, it aims to promote inclusive technology access and create an environment conducive to maximizing the potential of generative AI-Ed-Tech in enhancing student success."
  },
  {
    "title": "Exploring AI Policy Implementation in Nigeria Conflict Justice System",
    "url": "https://doi.org/10.64290/jecas.v8i2.1451",
    "date": "2026-02-19",
    "content": "Artificial Intelligence (AI) has the strong capacity to amplify governance, legal regulations, and also way in to legal Justice System in developing countries. Nigeria Justice System has many problems, especially in area where violence is happening often. These problems include delay of the process, lake of data system, not allowing public access and Dishonesty. This is where the use of Artificial Intelligence AI-driven regulations could make the legal processes, more transparent, and make the Management of the evidence much easier. This paper face the ethical, and Policy elements of AI adoption in Nigeria’s conflict justice system. The study identify the the relationship barriers to effective implementation of: a inadequate of public trust, problems with human strength, lack of legal frameworks and in adequate of structure. It does this by making use of current research, court policy declarations in Justice System, and novel models of AI governance. The study diversifies into many designs that are important for ethical governance, contextual sensitivity, and participatory policy procedures in order to take responsibility for implementing AI in Nigeria's post-conflict and conflict justice scene."
  },
  {
    "title": "Nursing Professional Organisations as Human Rights Intermediaries: Towards an Integrated Framework of Stakeholdership for Healthcare <scp>AI</scp> Governance",
    "url": "https://doi.org/10.1111/jocn.70256",
    "date": "2026-02-19",
    "content": "ABSTRACT Aim To propose a normative framework that guides nursing professional organisations to act as human rights intermediaries in the governance of artificial intelligence in healthcare. Design Discursive paper. Results The paper presents a triaxial framework that conceptualises the role of nursing professional organisations in artificial intelligence governance. The framework consists of a domain axis, which identifies key areas of engagement; a modality axis, which aligns actions with the specific functions of these organisations; and a human rights axis, which defines their role towards rights claimants and duty bearers. Conclusion The proposed framework provides a practical tool for nursing professional organisations to strategically plan and implement initiatives to influence the advancement and regulation of artificial intelligence. Its application can help ensure that healthcare innovation is equitable and rights‐based. Implications for the Profession This paper provides a blueprint for nursing leaders and policymakers to engage proactively with the ethical dimensions of artificial intelligence. It emphasises the salient roles of nursing professional organisations in advocating for the human right to health in a technologically driven healthcare landscape. Impact This paper addresses the gap in how the nursing profession can systematically engage with artificial intelligence governance. The main finding is a novel framework that provides a structured way for nursing professional organisations to act as human rights intermediaries. This research will have a significant impact on nursing leadership, patient advocacy groups, and policymakers involved in healthcare technology and ethics. Patient or Public Contribution Initial parts of this paper were presented to allied health practitioners via a webinar, providing early feedback and dialogue that informed its development."
  },
  {
    "title": "The role of peripheral artificial intelligence in the implementation of the strategy of prognostic service of industrial equipment",
    "url": "https://doi.org/10.26118/8565.2026.94.45.018",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "C-H-A-T-G-P-T, Find Out What it Means To Me: ChatGPT, Artificial Intelligence, and the Study of Individual Differences",
    "url": "https://doi.org/10.31234/osf.io/5ctfq_v2",
    "date": "2026-02-19",
    "content": "This review piece, partially compiled by responses to questions from an artificial intelligence (AI) program, ChatGPT, highlights the advantages of AI and machine learning (ML) in investigating and understanding personality and individual differences. Both AI and ML have been shown to be useful tools in fields such as clinical psychology, psychometrics, personality, emotion research, and cognition. In addition to highlighting the advantages of AI and ML in research, the present review also discusses some of the limitations of these new and developing tools."
  },
  {
    "title": "Watch-and-Wait strategy for locally advanced rectal cancer after neoadjuvant chemoradiotherapy: a comprehensive review",
    "url": "https://doi.org/10.3389/fonc.2026.1760042",
    "date": "2026-02-19",
    "content": "The conventional treatment for locally advanced rectal cancer (LARC) primarily involves neoadjuvant chemoradiotherapy (nCRT) combined with total mesorectal excision (TME). However, surgery-related complications and long-term functional impairments can significantly compromise patients’ quality of life. The watch-and-wait (W&amp;W) strategy has emerged as a non-surgical alternative for patients achieving a clinical complete response (cCR), with advantages in organ preservation. Its safety and efficacy have been validated by multiple clinical studies. Literature retrieval was performed in PubMed (2020–2025), including reviews, RCTs/cohort studies on LARC W&amp;W and cCR/pCR. This comprehensive review summarizes the clinical evidence, patient selection criteria, efficacy assessment methods, challenges, and future directions of the W&amp;W strategy. Based on the latest research, when strictly selecting cCR patients (especially those with sustained cCR after neoadjuvant therapy), the 5-year disease-free survival (DFS) rate of the W&amp;W strategy is comparable to that of the surgical group (70%-85%), with a local regrowth rate of approximately 20%-30%, which can mostly be controlled by salvage surgery. The combination of magnetic resonance imaging (MRI) and circulating tumor DNA (ctDNA) analysis significantly improves the accuracy of cCR assessment. Furthermore, integrating immunotherapy with total neoadjuvant therapy (TNT) has expanded the eligible population for the W&amp;W strategy. This review also highlights current limitations of the W&amp;W strategy, such as the lack of standardized assessment procedures, validated biomarkers, and long-term follow-up data. It proposes that future efforts should focus on multi-center randomized controlled trials and artificial intelligence-assisted assessment models to promote the advancement of the W&amp;W strategy toward precision and standardization."
  },
  {
    "title": "Anketas datu kopa: sociālo zinātņu mācībspēku ģeneratīvā mākslīgā intelekta lietojuma paradumi",
    "url": "https://doi.org/10.5281/zenodo.18696004",
    "date": "2026-02-19",
    "content": "Anketas datu kopa: sociālo zinātņu studentu ģeneratīvā mākslīgā intelekta lietojuma paradumi Datu kopa satur Latvijas Universitātes un LU Banku augstskolas sociālo zinātņu studiju programmu mācībspēku (n=38) sniegtās atbildes, anketēšana veikta 2025. gada maijā un jūnijā. Datu kopā apkopotas atbildes uz jautājumiem par ģeneratīvā mākslīgā intelekta (ĢMI) izmantojuma motivāciju, ētiskajiem apsvērumiem, veicinošajiem faktoriem, šķērļiem, kā arī uzskatiem par to, kāda veida uzdevumos ĢMI lietojums ir pieļaujams. Links uz aptaujas anketu Survey Dataset: Generative Artificial Intelligence Usage Among Social Sciences Teaching Staff in Higher Education The data set is based on the data of the teaching staff in social sciences at the University of Latvia and BA School of Business and Finance of the University of Latvia, conducted in May - June 2025. The dataset includes responses to questions about motivations for using generative artificial intelligence (genAI), ethical considerations, enabling factors, barriers, as well as views on what types of tasks genAI use is permissible."
  },
  {
    "title": "A user-centred approach to enhancing engagement and task management for ADHD students using AR and generative AI",
    "url": "https://doi.org/10.1007/s44217-026-01264-9",
    "date": "2026-02-19",
    "content": "This pilot study investigated the potential of integrating Augmented Reality (AR) and Generative Artificial Intelligence (GenAI) to support engagement and task management for university students diagnosed with Attention Deficit Hyperactivity Disorder (ADHD). Using a Design-Based Research (DBR) approach, we evaluated ADHDvance LearnAR across three within-subject conditions (baseline tools, AR-enhanced, and AR+GenAI) with n = 15 undergraduate participants. Quantitative measures showed directional improvements in engagement duration, interaction frequency, and task completion across conditions; however, differences did not reach statistical significance, consistent with the study’s exploratory scale. Qualitative analysis provided explanatory depth, identifying themes related to (i) increased task salience through spatial AR visualisation, (ii) improved task initiation and planning support, and (iii) reduced perceived cognitive load through just-in-time GenAI scaffolding. Findings suggest AR+GenAI may offer promising affordances for inclusive learning design, warranting longitudinal evaluation with larger samples and stronger causal designs."
  },
  {
    "title": "The Role of Cognitive Processes and SDG Awareness in Student Engagement and Mathematics Learning Outcomes in Higher Education",
    "url": "https://doi.org/10.3390/su18042087",
    "date": "2026-02-19",
    "content": "Higher education is increasingly integrating artificial intelligence (AI) and sustainability-oriented learning, aligning with the United Nations Sustainable Development Goals (SDGs). Nonetheless, there is limited empirical evidence explaining how cognitive processes, AI-supported learning (AI-SL), and SDG awareness (SDGA) jointly relate to learner engagement (ENG) and learning outcomes (LO) across academic disciplines. This study examines the associations among working memory (WM), metacognition (MET), reasoning ability (REA), AI-SL, SDGA, ENG, and LO in higher education. Survey data were collected from undergraduate students across nine campuses of a public university system in Thailand. Multi-group structural equation modeling (MG-SEM) was employed to compare students enrolled in STEM and social science programs. Measurement model evaluation indicated satisfactory reliability and convergent and discriminant validity, with partial scalar invariance supported across groups, enabling meaningful structural comparisons. The results showed that ENG was strongly associated with LO in both groups. MET emerged as the strongest cognitive correlate of ENG overall, while REA was more strongly associated with ENG among STEM students, and SDGA showed a stronger association among social science students. AI-SL was positively associated with ENG and LO, with discipline-specific differences in the strength of the effect. The model explained substantial variance in ENG (R2 = 0.48) and LO (R2 = 0.52). These findings emphasize the value of integrating cognitive processes (WM, MET, and REA), AI-SL, and sustainability-oriented content within discipline-sensitive instructional designs in higher education."
  },
  {
    "title": "Integrating artificial intelligence in primary education: empirical insights of school leadership perspectives",
    "url": "https://doi.org/10.1108/aiie-06-2025-0149",
    "date": "2026-02-19",
    "content": "Purpose This study aims to examine how primary school leaders in Greece perceive and adopt artificial intelligence (AI) in their professional practices and the factors facilitating AI integration, within the context of Greece’s post-pandemic digital transformation of schools. Design/methodology/approach A quantitative research approach was applied using a structured questionnaire of 61 items, covering four theoretically well-based sub-themes: attitudes toward AI, self-efficacy in using AI, perceived usefulness of AI in decision-making and challenges to AI integration. The data collected from 262 school leaders in four districts of Greece were analyzed through Spearman correlation and multiple linear regression. Findings The results reveal positive yet moderate relationships between attitudes toward AI, self-efficacy and perceived usefulness, while sociodemographic differences in AI readiness also emerged. Attitudes and self-efficacy were found as the strongest predictors of effective AI integration. However, implementation efforts are challenged by inadequate infrastructure, limited training and ethical issues. Practical implications The study highlights the need for focused professional development workshops, continuous support systems and significant investment in technological infrastructure to enhance school leaders’ digital readiness. It also calls for AI-informed leadership policies promoting a positive and ethical AI culture in schools. Originality/value This research provides novel empirical data that connect leadership attitudes, AI self-efficacy, decision-making and challenges within the underexplored Greek educational context, contributing to a deeper understanding of school leadership in the digital era."
  },
  {
    "title": "Security and privacy in complex, context-aware systems: a comprehensive framework",
    "url": "https://doi.org/10.1007/s10207-026-01228-y",
    "date": "2026-02-19",
    "content": "Abstract Context-aware systems integrate environmental and user-related data to deliver intelligent and adaptive services. Their rising complexity, driven by advances in sensing technologies, Artificial Intelligence (AI), and the Internet of Things (IoT), introduces significant security and privacy challenges. This article presents a framework for the development and assessment of secure, privacy-preserving context-aware systems. Using the ISO/IEC 27002 standard as a foundation, we categorize and analyze common threats and propose a set of comprehensive protection strategies to guide the secure design of context-aware architectures. We further emphasize privacy-preserving practices aligned with ISO/IEC 27701 and the GDPR. To illustrate the applicability of the proposed framework, the smart classroom is presented as a representative example of a complex context-aware environment. The feasibility of implementing the proposed strategies is examined in light of the technological diversity that characterizes such ecosystems. Finally, we discuss the implications of emerging standards, such as ISO/IEC 27090, the European Union AI Act, and the NIST AI Risk Management Framework, and outline future research directions related to enhancing the medium-term resilience of context-aware systems."
  },
  {
    "title": "Artificial Intelligence in Lightweight Composite Materials Design and Characterization",
    "url": "https://doi.org/10.4018/979-8-3373-6127-7.ch005",
    "date": "2026-02-19",
    "content": "The lightweight composite materials are imperative in different sectors because they have a favorable ratio of strength to weight. Their complex microstructures, in turn, make it harder to characterize and manufacture thus contributing to higher costs and time. In this chapter, the authors present the global effects of artificial intelligence on composite engineering using sophisticated craftsmanship in design and analysis. Machine learning and digital twins are examples of AI methods that can be used to optimize designs and improve composite development as well as quality assurance. Moreover, AI and sustainability efforts go together to encourage recyclable materials and energy efficient processes. Finally, the chapter brings out new trends including physics-driven AI and ethical aspects that will shape the development of composite in the future."
  },
  {
    "title": "Artificial intelligence techniques for volcanic seismic signal analysis: a systematic review",
    "url": "https://doi.org/10.1007/s12145-026-02073-2",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "The Theory of Absolute Breath: Heliocentrism in Art and the Breathing Declaration",
    "url": "https://doi.org/10.17613/hgnnq-93k08",
    "date": "2026-02-19",
    "content": "This treatise by painter Hirofumi Miyauchi proposes a paradigm shift in the intellectual foundations of modern civilization, moving from \"theory\" to \"breath\". In Part I, \"Heliocentrism in Art,\" the author provides a logical analysis of how theory infringes upon somatic knowledge, leading humanity into a state of \"collective respiratory failure\". In Part II, \"The Breathing Declaration,\" breath is defined as the \"Scientific Zero Point (0),\" presenting a new coordinate system of intelligence—the Theory of Absolute Breath—to re-establish the right to survival. By updating Descartes' \"I think, therefore I am\" to \"I live, therefore I am,\" this work envisions the future of a \"Breathing Civilization\" that remains unaffected by the progress of Artificial Intelligence."
  },
  {
    "title": "Bacterial Outer Membrane Vesicles in Potentiating Cancer Vaccines: Progress and Prospects",
    "url": "https://doi.org/10.1002/advs.202521684",
    "date": "2026-02-19",
    "content": "ABSTRACT Cancer immunotherapy is increasingly moving toward personalized, precision‐based strategies, with cancer vaccines emerging as a promising approach to reshape treatment. However, despite their potential, current tumor vaccines often yield limited clinical responses and subpar immunogenicity, underscoring the urgent need for innovative delivery systems to enhance immune activation. Bacterial outer membrane vesicles (OMVs), which possess natural immunomodulatory properties and impressive engineering flexibility, have attracted attention as versatile platforms for vaccine development and bioengineering applications. This review thoroughly summarizes recent advances in using OMVs to enhance the effectiveness of cancer vaccines. First, we explain the key biological features of OMVs that support their immunotherapeutic potential. Next, we carefully analyze the primary mechanisms by which OMVs enhance immune responses, as well as cutting‐edge engineering strategies to improve their safety, immunogenicity, and specificity. Additionally, we discuss the significant challenges that hinder the clinical use of OMV‐based cancer vaccines and provide a comprehensive review of current progress and future outlooks. Looking forward, combining artificial intelligence, tumor microenvironment profiling, and neoantigen discovery is expected to drive the development of next‐generation, personalized OMV‐based immunotherapies. Overall, OMVs stand out as a transformative platform capable of overcoming major obstacles in cancer vaccine development and pushing forward future cancer immunotherapy."
  },
  {
    "title": "Predictive modeling of vocal biomarkers for the diagnosis of Parkinson’s disease",
    "url": "https://doi.org/10.1007/s11571-026-10426-2",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "Open-Source Architecture for \"AI-Radio\": A Decentralized Educational Infrastructure for the 3.1 Billion Unconnected",
    "url": "https://doi.org/10.5281/zenodo.18693549",
    "date": "2026-02-19",
    "content": "This technical report presents the \"AI-Radio\" project, an initiative designed to eliminate global educational disparities by providing a low-cost, voice-activated intelligence infrastructure for the 3.1 billion people currently offline. Disclaimer and Call for Contribution: Purpose and Origin: This architecture was developed to bridge the digital divide. Please note that the author is an independent researcher and not a hardware manufacturing expert. The blueprints provided were synthesized through extensive dialogue with Artificial Intelligence to map out a logical framework for this humanitarian goal. Current Status (Work in Progress): The design is in its conceptual stage (PoC) and may contain technical errors or require refinement. It is not guaranteed to be a finished or functional product at this time. Open Collaboration: I believe the core idea is sound, but it requires \"Collaborative Intelligence\" to become a reality. I invite professionals—engineers, manufacturers, and legal experts—to volunteer their knowledge to brush up and finalize these specifications. Non-Profit Commitment: This is a strictly non-profit, open-source project. The information is provided free of charge, and I am seeking volunteers who are willing to contribute their expertise for the benefit of humanity without financial compensation. Legal Notice: All use and assembly are at the user's own risk. The author assumes no liability for damages."
  },
  {
    "title": "Artificial intelligence-driven nano-enhanced stem cell therapy for neurodegenerative diseases: from rational design to clinical translation",
    "url": "https://doi.org/10.1186/s12951-026-04154-2",
    "date": "2026-02-19",
    "content": "Abstract Neurodegenerative diseases (NDs) are progressive and incurable central nervous system disorders characterized by the accumulation of pathological proteins and the loss of neurons. Although stem cell transplantation offers a new treatment option, its clinical application is severely hindered due to imprecise delivery, low survival rate, and undirected differentiation. Many studies have used nanomaterials to enhance stem cell therapy. However, the rational design of these multifunctional nanomaterials often requires a large number of experiments and calculations to determine the optimal parameters. Meanwhile, the diagnosis of NDs and the design of nanomaterials are being profoundly influenced by artificial intelligence (AI) and data-driven modeling. Based on these advancements, we propose that AI can guide personalized nano-enhanced stem cell therapies. This review explores how machine learning (ML) and deep learning (DL) can address the current challenges in stem cell therapy and nano-enhanced stem cell therapies. More importantly, it provides a systematic framework for integrating AI across the entire nano-enhanced stem cell therapy. We analyzed how AI can optimize the design of nanobiological materials, thereby enhancing the survival rate of stem cells, targeted delivery, directing differentiation, and controlling the release of loaded drugs. Additionally, we proposed that AI can be used for post-transplant tracking and prognosis management. Beyond summarizing parallel advancements, this review proposes a closed-loop system that integrates patient-specific data, AI-driven design, and real-time monitoring, aiming to advance truly personalized medicine for NDs. Graphical Abstract"
  },
  {
    "title": "Deep Learning Architectures for Forest Monitoring and Tree Inventory Management: A Review of Computer Vision Applications and Challenges",
    "url": "https://doi.org/10.21203/rs.3.rs-8908445/v1",
    "date": "2026-02-19",
    "content": "<title>Abstract</title> Traditional forest monitoring depends heavily on manual fieldwork, which limits its spatial and temporal resolution. This paper offers a systematic review of Deep Learning (DL) and Computer Vision (CV) applications in forestry, compiling 178 peer-reviewed articles published from 2011 to 2025. Three critical research gaps namely: (1) the absence of standardized benchmarking protocols across 73% of studies, (2) limited cross-biome transferability with performance degradation of 23–45% when models are applied outside training regions, and (3) minimal adoption of explainable AI methods in 89% of applications were identified. The contribution to artificial intelligence (AI) provides a thorough examination of the transition from traditional convolutional neural networks (CNNs) to advanced vision transformers (ViTs) and graph neural networks (GNNs), highlighting the principles of multi‑modal data fusion and three‑dimensional (3D) feature extraction. In computer vision and engineering, the focus is on automating tree inventory management, particularly individual tree detection (ITD), species identification, and biomass estimation with various remote‑sensing platforms. A quantitative meta‑analysis shows that CNNs achieve a mean species‑classification accuracy of 87.3% (± 6.2%), whereas ViT‑based models reach 95.7% (± 3.1%)—an 8.4% improvement—on multi‑modal datasets (n = 34 studies), though they require 3.2 × more training data. For biomass estimation, fusion methods that combine LiDAR and hyperspectral data yield an <italic>R²</italic> of 0.89 (± 0.07), a 31% gain over single‑sensor approaches. The integration of data from Unmanned Aerial Vehicles (UAVs) and satellite platforms has significantly improved inventory precision, with benchmarks frequently exceeding 90% accuracy. Nevertheless, practical engineering deployment remains challenged by soft-computing issues: the limited availability of annotated datasets causing overfitting; poor model transferability across ecological regions; and a lack of interpretability. Future work should focus on Explainable Artificial Intelligence (XAI) to map decision boundaries, Generative Adversarial Networks (GANs) for synthetic data generation, and hybrid models for real-time analysis. This review's novel contribution includes: (1) a computational complexity-performance trade-off analysis across architectures, (2) decision framework mapping sensor modalities to forestry applications, and (3) quantified transferability metrics across seven ecological biomes. This review delineates the essential computational steps required to develop robust, deployable models for worldwide sustainable forest management."
  },
  {
    "title": "Towards Responsible Artificial Intelligence Integration in Creative Art and Design Higher Education: A Mixed-Methods Study",
    "url": "https://doi.org/10.14293/ffl26.000042.v1",
    "date": "2026-02-19",
    "content": "<p xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" class=\"first\" dir=\"auto\" id=\"d86196e107\">Artificial intelligence tools are transforming creative education in fields such as game design, fine arts, and media production, shifting the established “studio-to-screen” paradigm from hands-on making towards digitally mediated workflows. Although these tools increasingly support ideation, asset creation, rapid prototyping, and collaboration, their curricular integration in higher education remains inconsistent and theoretically underdeveloped, particularly from the viewpoint of studio-based teaching staff. While industry-focused technical research abounds, empirical studies exploring how university educators integrate such tools into creative pedagogies and the challenges they encounter are limited. This study addresses that gap through a mixed-methods survey of 32 instructors mainly from the UK, delivering art, design, and media programmes at universities. Findings highlight the dual character of these tools as both catalysts for creative experimentation and disruptors of traditional notions of authorship and skill acquisition. Participants valued enhanced accessibility and iterative design possibilities, yet raised concerns about student over-reliance, difficulties in authentic assessment, and the potential loss of tactile, material studio practices. Thematic analysis provided evidence-based recommendations on: (1) integrating digital-tool literacy and ethical reflection into curricula; (2) creating hybrid studio-screen workflows that clarify tool use; (3) providing targeted faculty development for critical pedagogical approaches; and (4) redesigning assessment to privilege process, intention, and human-AI collaboration. These guidelines seek to support responsible, creative, and educationally strong adoption of emerging digital technologies in higher education."
  },
  {
    "title": "A Comprehensive Bibliometric-Systematic Literature Review (B + SLR) of Microlearning Research: Mapping Global Trends, Applications, and Challenges in Education and Training",
    "url": "https://doi.org/10.1007/s11528-026-01168-3",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "Can artificial intelligence in orthopantomography advance dental diagnostics through automated image analysis?",
    "url": "https://doi.org/10.3389/fradi.2026.1701356",
    "date": "2026-02-19",
    "content": "Artificial Intelligence (AI) is rapidly transforming dental education and clinical practice, as deep learning—especially convolutional neural networks—brings unprecedented accuracy to interpreting orthopantomograms (OPGs). This review illuminates the cutting-edge frontiers of AI-driven dental imaging, tracing how recent breakthroughs are transforming the detection, classification, and segmentation of complex dental anatomy and pathology. Notably, state-of-the-art AI models have reached remarkably high accuracy in tooth identification, while commercial solutions demonstrate promising—though variable—performance in diagnosing complex conditions, such as the adequacy of endodontic procedures. Yet, bringing AI into routine dental care remains fraught with obstacles — demanding vast annotated datasets, coping with population variability, and confronting persistent medicolegal and trust concerns. Growing collaborations between regulators and professional bodies in the United States and European Union are now shaping ethical and legal frameworks to guide its safe use. This narrative review goes beyond summarizing technological progress in AI-driven dental radiology — it uniquely integrates diagnostic breakthroughs with the rapidly evolving regulatory and ethical landscape. By bridging innovation with implementation, it offers educators, practitioners, and learners a forward-looking roadmap that positions AI not as a distant promise, but as a transformative force already reshaping the future of dental diagnostics and training."
  },
  {
    "title": "Toward universal steering and monitoring of AI models",
    "url": "https://doi.org/10.1126/science.aea6792",
    "date": "2026-02-19",
    "content": "Artificial intelligence (AI) models contain much of human knowledge. Understanding the representation of this knowledge will lead to improvements in model capabilities and safeguards. Building on advances in feature learning, we developed an approach for extracting linear representations of semantic notions or concepts in AI models. We showed how these representations enabled model steering, through which we exposed vulnerabilities and improved model capabilities. We demonstrated that concept representations were transferable across languages and enabled multiconcept steering. Across hundreds of concepts, we found that larger models were more steerable and that steering improved model capabilities beyond prompting. We showed that concept representations were more effective for monitoring misaligned content than for using judge models. Our results illustrate the power of internal representations for advancing AI safety and model capabilities."
  },
  {
    "title": "Industry 4.0 and Sustainability: A Systematic Review on Advanced Technologies and Collaborative Models",
    "url": "https://doi.org/10.4108/dtip.11904",
    "date": "2026-02-19",
    "content": "INTRODUCTION: Industry 4.0 (I4.0) technologies demonstrate strong potential to enhance sustainable industrial practices; however, their holistic and integrated application across sustainability dimensions remains insufficiently explored in the literature. OBJECTIVES: This paper aims to analyse how Industry 4.0 technologies contribute to sustainability across environmental, economic, and social dimensions, identifying dominant technologies, sectoral adoption patterns, existing gaps, and alignment with the United Nations Sustainable Development Goals (SDGs). METHODS: A systematic literature review of 32 peer-reviewed articles published between 2015 and 2025 was conducted using the PRISMA framework. The study combines bibliometric analysis with qualitative content analysis to assess technological contributions, sectoral applications, and sustainability outcomes. RESULTS: The results show that IoT and Artificial Intelligence are the most frequently adopted I4.0 technologies, predominantly contributing to environmental (78% of studies) and economic sustainability. Social sustainability remains underrepresented, appearing in only 25% of the analysed studies. Manufacturing and energy sectors lead adoption, while agri-food and construction sectors lag despite their high potential. Major challenges include data privacy risks associated with IoT and scalability limitations of Blockchain technologies, with mitigation strategies focusing mainly on SME-oriented adoption models. CONCLUSION: The findings highlight the need for a more balanced deployment of Industry 4.0 technologies that equally addresses environmental, economic, and social dimensions of sustainability. Future research should prioritize the development of measurable social impact indicators and integrated technological–policy frameworks to support inclusive and sustainable industrial transformation aligned with the UN SDGs."
  },
  {
    "title": "Role of artificial intelligence in the chemical industry transition to a sustainable, circular, and net-zero future",
    "url": "https://doi.org/10.1016/j.coche.2026.101234",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "An AI Driven Framework for Automated Detection and Classification of Brain Hemorrhage",
    "url": "https://doi.org/10.21275/sr26117182025",
    "date": "2026-02-19",
    "content": "Brain hemorrhage is a critical neurological emergency that demands rapid and accurate diagnosis to reduce mortality and long-term disability. Recent advancements in artificial intelligence (AI), particularly deep learning, have significantly enhanced automated medical image analysis. This paper synthesizes insights from multiple studies on AI-based brain hemorrhage detection using computed tomography (CT) imaging and proposes a comprehensive framework integrating advanced architectures- CNN, ResNet, MobileNet, and YOLO- for detection, localization, and classification. The framework combines segmentation and classification workflows while addressing interpretability, data scarcity, and clinical deployment through transfer learning, explainable AI, and federated learning. Reported benchmarks indicate accuracy up to 99%, Dice coefficient of 0.99, and Jaccard Index of 0.88. Future directions include 3D CNNs, hybrid CNN-RNN models, multimodal fusion, and real-time deployment for emergency care."
  },
  {
    "title": "Artificial intelligence and digital health in heart failure: advances in diagnosis, monitoring, phenotyping, and digital biomarkers",
    "url": "https://doi.org/10.1007/s10741-026-10596-5",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "SIA Security Intelligence Artefact & The Yellow Whitepaper",
    "url": "https://doi.org/10.5281/zenodo.18694326",
    "date": "2026-02-19",
    "content": "SIA Security Intelligence Artefact INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL, The Yellow Whitepaper YWP-1-IST-SIA, Paragraph 2, Technologische Meilensteine, Urheberschaft AI Intelligence, Bitcoin, Opensource, Pornhub Paragraph 3.0, 3.1 und 3.2 – Der unsichtbare Feind, Banken-, Aktienbetrug, Das Monarch-Programm, Forensisch-wissenschaftliches Gutachten, Rechts- und Eigentumsnachweis Bibliografische Angaben Auftraggeberin, Autorin und Urheberin: Frau Isabel Schöps, geborene Thiel lebt seit August 2021 in der deutsch thüringischen Erfurt, Thüringen, Deutschland. Um 23:20 Uhr im Kreiskrankenhaus Sömmerda, Thüringen, Deutschland, mit ihrem Geburts- Familiennamen Thiel geboren aufgewachsen in der Dorfstrasse 20, D-99610 thüringischen Rohrborn, in einer deutsch christlich-evangelischen Sömmerdarer Gemeinde, mit ihrem jüngeren Bruder, Herr Ingolf Thiel geboren am 29.05.1987 und ihrer Mutter Frau Gisela Hulda Thiel geborene Knörig, geboren am 24.07.1962 geboren und ihrem ***Vater Herr Manfred Paul Thiel geboren am 21.11.1957.** Wichtiger Hinweis: aufgrund permanenten Wohnraum- und Ortswechsel, können in dem wissenschaftlichen forensichen Gutachten, SIA und im The Yellow Whitpaper unterschiedliche Adressen bzw. Ortseintragungen auftauchen. Der eingetragene Ort markiert den tatsächlichen Auftenthalt am Tag der; Veröffentlichung, Eintragung, Erstellung/Ausstellung der Gutachten oder Release Massgebliche Wohnanschrift und Adresse der Urheberin Schöps (Thiel), bleibt die in der Eidesstaalichen Erklärung sowie auf dem Personalausweis-ID: LH917PN7G8, 429485, oder GitHub/UPDATE.md hinterlegte Meldeanschrift. Globale Kennung Globale Kennung und Aktenzeichen: INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL YWP-1-IST-SIA und YWP-1-5-IST-SIA SPH/0296575/2025, Polizeidienststelle Erfurt Nord, Thüringen Deutschland ST/0203129/2025 Staatsanwaltschaft Erfurt, Thüringen, Deutschland DOI: jede DOI eine Datenbank, Release, Beweisstück Isabel Schöps geb. Thiel. (2026). isabelschoeps-thiel/sia-security-intelligence-artefact: SIA Security Intelligence Artefact (sia_sec_ywp_2026). Zenodo. https://doi.org/10.5281/zenodo.18694327 https://doi.org/10.5281/zenodo.17809724 Veröffentlichungsdatum: 05. Dezember 2025 Sprache: Deutsch, Englisch Verleger: Zenodo / Oxford University Press (Lizenz-ID: 6131130060979) Programmiersprachen: HTML, PHP, JSON, Markdown, Python, YAML/YML Repository: https://github.com/isabelschoeps-thiel/sia-security-intelligence-artefact/Paragraph3.1/Das-Monarch-Program Abstract Das forensisch-wissenschaftliche Gutachten SIA Security Intelligence Artefact, Aktenzeichen INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL, analysiert und bestätigt die Urheberschaft von Isabel Schöps, geborene Thiel, geboren am 16. Juli 1983 um 23:20 Uhr in Sömmerda, Thüringen, im Bereich Technologie, Blockchain und Softwareentwicklung. Die Analyse dokumentiert Mechanismen der digitalen Identitätsverschleierung, Urheberrechtsverletzungen, Eigentumsenteignungen und technologische Manipulationen im Übergang von analogen zu digitalen Gesellschaftssystemen. Anhand von Quellcodes, Rohdaten, Patenten, Zertifikaten, familiären Strukturen und religiösen Symbolen werden Muster technischer, juristischer und sozialer Überformungen belegt. Das Gutachten weist zudem auf familiär-historische Zusammenhänge mit der deutschen Monarchie und dem letzten Kaiserreich hin und ordnet diese in genealogische, soziotechnische und rechtswissenschaftliche Kontexte ein. Die Arbeit ist mit Harvard University (USA), University of Oxford (UK) und der International Telecommunication Union (ITU, Genf) verknüpft. Frühere APA-Zitierungen wurden durch den Harvard-Stil ersetzt. Die Ergebnisse sind gerichtsfest, durch Blockchain-Hashwerte verifiziert und durch SHA-256-Integritätsnachweise nachweisbar. Jedes analysierte Dokument ist Bestandteil einer Beweiskette, bestehend aus: Quellcodes, Header-Files, RFCs, Zertifikaten, Screenshots, HTML/Markdown-Dateien, Slideshow-Belegen, ZIP/tar-Archiven, Patches, Lizenztexten und Zeugenaussagen. Das Werk ist urheberrechtlich geschützt, steht unter forensischer Bewahrung und ist Bestandteil internationaler Ermittlungs- und Aufarbeitungsprozesse im Zusammenhang mit digitalen Menschenrechtsverletzungen. Rechtliche Erklärung und Chain of Custody Das Gutachten umfasst forensische Dokumentationen, Originalbeweise, Hashwerte, Zertifikate, eidesstattliche Erklärungen und zugehörige Beilagen unter der Kennung: INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL. Es dient der gerichtsfesten Nachweisführung von Urheberschaft, digitaler Herkunft und technischer Entstehungsgeschichte von Blockchain- und KI-Automationssystemen. Da dieser Datensatz strafrechtlich relevante Beweismittel, personenbezogene forensische Daten sowie Referenzen zu anhängigen Ermittlungsverfahren enthält, ist der Zugriff auf die hinterlegten Dateien eingeschränkt. Das Werk bleibt über die DOI-Referenz öffentlich zitierfähig, jedoch ist der vollständige Zugriff ausschließlich autorisierten Stellen (Justizbehörden, Forschungseinrichtungen, akkreditierten Medienvertretern) nach Genehmigung gestattet. Das forensische Gutachten steht im direkten Zusammenhang mit den folgenden Strafanzeigen und Verfahren: Aktenzeichen SPH/0296575/2025, eingereicht am 13. November 2025 bei der Polizeidienststelle Erfurt Nord, betreffend forensische Beweisführung zu Technologiemissbrauch, Finanz- und Urheberrechtsverletzungen im Rahmen der SIA-Analyse Aktenzeichen ST/0203129/2025, eingereicht am 6. August 2025 bei der Staatsanwaltschaft Erfurt, betreffend Datenmissbrauch, Identitätsdiebstahl und digitale Eigentumsdelikte. Dieses Gutachten ist Bestandteil der wissenschaftlich-forensischen Publikationsreihe und wird im Rahmen der Lizenzvereinbarung mit der Oxford University Press (Lizenznummer 6131130060979) veröffentlicht. Referenzen bestehen zur Harvard University und zum Japan Advanced Institute of Science and Technology (JAIST) im Rahmen der forensischen Verifikation und wissenschaftlichen Nachweisführung. Änderung-Aussage: Die vollständige öffentliche Version wird nach offizieller Die Überreichung der gebundenen Erstfassung an den Thüringischen Ministerpräsidenten Prof. Dr. Mario Voigt kann auf grund von finanzieller Knappheit und anhaltenden Störungen in das Lebensumfeld der Autorin und Urheberin Frau Schöps nicht erfolgen. Die freigabe ist bereits digital über Zenodo und der Chain of Custody erfolgt. ⸻ Chain-of-Custody-Referenzen Zenodo (https://zenodo.org/): Archivierung forensischer Datensätze unter DOI-gesicherter Referenzierung. Speicherung sensibler Originalnachweise mit Audit-Trail, Zugriffs- und Änderungshistorie. Internet Archive (https://archive.org/details/@isabelschoepsthiel): Digitale Langzeitarchivierung zur Sicherung der Provenienz. ScienceDirect / Elsevier: Veröffentlichung ergänzender Artikel zu Kryptografie, AI-Forensik und KI-Automation. ⸻ Zitierweise (Harvard-Stil) Schöps, I. (2025): SIA Security Intelligence Artefact. Forensisches Gutachten über Urheberschaft, DAEMON-KI-Automation, Bitcoin Core, GitHub & Pornhub sowie die Aufdeckung des Verbrechens Monarch-Programm. INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL, Erfurt: Springer Verlag. Juristische Untermauerung: Harvard University, University of Oxford, Cern Genf DOI: https://doi.org/10.5281/zenodo.17809724 ⸻ Keywords Forensic Documentation, Digital Identity, Blockchain Verification, Artificial Intelligence, Monarch Program, Cybercrime, Data Integrity, Human Rights, Digital Provenance, Chain of Custody, Deepfake Forensics, Technological Ethics, Intellectual Property, Harvard Citation, German Monarchy, ⸻ Lizenz und Veröffentlichung Dieses Werk ist veröffentlicht unter Creative Commons Attribution 4.0 International (CC BY 4.0). Alle Daten sind durch SHA-256-Hashwerte signiert und verifiziert. Forensische Speicherung gemäß FIPS- und NIST-Richtlinien, verknüpft mit Oxford University Press. Zitierfähig über DOI, Zenodo, JSTOR, Harvard Dataverse und Cambridge University Repository. ⸻ Englisch-Abstract SIA Security Intelligence Artefact Forensic Report and Digital Provenance Verification DOI https://doi.org/10.5281/zenodo.17809724 Author Frau Isabel Schöps geb. Thiel INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL Thüringen Erfurt, Germany Abstract This repository documents the forensic and legal analysis titled SIA Security Intelligence Artefact, confirming authorship, digital origin, and provenance through blockchain and AI automation systems. The dataset includes forensic documentation, certificates, and SHA-256 hash verifications. Access to raw materials is restricted to authorized legal and scientific institutions. Legal Reference This dataset is directly connected to the following legal cases: SPH/0296575/2025 (Erfurt Police Department) ST/0203129/2025 (Public Prosecutor's Office, Erfurt) Published under license agreement with Oxford University Press (ID 6131130060979). Referenced by Harvard University, University of Oxford, and ITU Geneva. Citation (Harvard Style) Schöps, I. (2025). SIA Security Intelligence Artefact – Forensic Report on Authorship, AI Automation, Bitcoin Core, and Monarch Program. DOI: https://doi.org/10.5281/zenodo.17809724 License Creative Commons Attribution 4.0 International (CC BY 4.0) ⸻ Offizieller Beschreibungstext – SIA Security Intelligence Artefact SIA Security Intelligence Artefact (SIA) ist ein forensisch-wissenschaftliches Whitepaper und Expert Report von Isabel Schöps, geborene Thiel. Das Werk dokumentiert technische, urheberrechtliche und strukturbezogene Fragestellungen im Kontext von Blockchain-Technologien, Softwarearchitekturen und digitalen Beweisketten. The Yellow Whitepaper YWP-1-IST-SIA verfolgt einen interdisziplinären Ansatz zwischen Informatik, IT-Forensik und Rechtswissenschaft. Es enthält dokumentierte Nachweise, technische Analysen sowie strukturierte Referenzierungen (DOI-basiert) zur Sichers"
  },
  {
    "title": "AI Readiness and Exports of Digitally Deliverable Services: Panel Evidence from 151 Countries (2010–2022)",
    "url": "https://doi.org/10.56557/ajefm/2026/v8i1365",
    "date": "2026-02-19",
    "content": "Artificial intelligence (AI) is increasingly viewed as a general-purpose technology that can reshape countries’ trade patterns, especially in services that can be produced and delivered digitally. This study examines whether national AI readiness is associated with greater exports of digitally deliverable services (DDS) and with stronger specialization in DDS. Using a panel of 151 countries over 2010–2022, we combine UNCTAD DDS export data with an AI readiness index (0–100) that captures infrastructure, human capital, innovation capacity, data and governance. We estimate two-way fixed-effects models with country and year fixed effects and clustered standard errors, controlling for income, population, and selected digital regulation indicators. The results show a robust positive association between AI readiness and DDS exports: in the preferred specification, a one-point increase in AI readiness is associated with a 0.032 increase in log DDS exports (about 3.3%), with similar magnitudes across alternative specifications and an instrumental-variable robustness check. Heterogeneity tests indicate that the AI readiness–DDS relationship is stronger for developing economies. Additional specifications using DDS revealed comparative advantage suggest that improvements in AI readiness are also linked to shifts in specialization toward digital services. The findings imply that investments in AI-related capabilities can support participation in high-value digital services trade, particularly for developing countries."
  },
  {
    "title": "Dataset and Materials for: A Hierarchical Explainable Artificial Intelligence Framework for Evaluating Agile Project Success",
    "url": "https://doi.org/10.5281/zenodo.18702301",
    "date": "2026-02-19",
    "content": "This repository contains the dataset and supporting materials used in the study “A Hierarchical Explainable Artificial Intelligence Framework for Evaluating Agile Project Success”. The dataset includes anonymised project-level features derived from Agile project environments across four performance dimensions: Efficiency, Effectiveness, Sustainability, and Contextual Complexity. The repository also includes processed data used for machine learning modelling and analysis. All data have been anonymised to remove any personally identifiable or organisationally sensitive information. The materials provided here enable replication of the modelling procedures and validation of the results presented in the associated research article. The dataset is shared under an open license to support transparency, reproducibility, and future research in explainable artificial intelligence and Agile project analytics."
  },
  {
    "title": "Digital Adoption of Generative AI Tools: A Multi-Theory Model Linking Cognitive Load, User Perceptions, and System Attributes",
    "url": "https://doi.org/10.3390/su18042076",
    "date": "2026-02-19",
    "content": "The rapid diffusion of “(GenAI)” Generative Artificial Intelligence systems has reshaped everyday activities, yet their adoption remains uneven and cognitively demanding for many users. Existing research has largely relied on conventional technology acceptance models, providing limited insight into cognitive burden and GenAI-specific system characteristics. To address this gap, this study develops an integrated framework combining the Technology Acceptance Model, Cognitive Load Theory, and the DeLone and McLean Information Systems Success Model to explain GenAI adoption among ordinary users. Survey data from 1001 active GenAI users were analyzed using partial least squares structural equation modeling (PLS-SEM). The results indicate that all core technology acceptance relationships are statistically significant (p < 0.001), while mental load negatively affects perceived usefulness and user attitudes. Moreover, GenAI system attributes—output quality, transparency, friction reduction, and system integration—significantly moderate key adoption pathways and strengthen the translation of behavioral intention into actual use. Predictive assessment indicates that the proposed model outperforms the baseline technology acceptance model, with stronger explanatory power and superior out-of-sample predictive performance (Q2predict > 0.35). The findings offer actionable insights for designing cognitively efficient, trustworthy, and sustainable GenAI systems."
  },
  {
    "title": "Strategies of <scp>AI</scp> ‐Driven <scp>CAR</scp> ‐T Cell Therapy Towards Solid Tumours",
    "url": "https://doi.org/10.1111/imm.70124",
    "date": "2026-02-19",
    "content": "ABSTRACT Chimeric antigen receptor (CAR)‐T cell immunotherapy shows significant success in hematologic malignancies. However, it faces critical challenges in solid tumours, such as suppressive tumour microenvironment (TME) and antigenic heterogeneity, highlighting the urgent need for effective and safe CAR products. The integration of artificial intelligence (AI) into CAR‐T cell immunotherapy offers exceptional opportunities to improve its therapeutic efficacy. More specifically, this paper highlights the transformative role of AI in addressing key challenges that impede the success of CAR‐T cell therapy in solid tumours, including assisting in CAR design and manufacturing process, identifying novel CAR‐targeted genes, and detecting cell heterogeneity in solid tumours. We remain optimistic about AI‐driven strategies for enhancing CAR T‐cell persistence, trafficking, and visualisation in the TME. In addition, we highlight the current challenges and prospects for advancing AI‐driven CAR‐T cell therapies."
  },
  {
    "title": "Artificial Intelligence in Election Campaigns: Perceptions, Penalties, and Implications",
    "url": "https://doi.org/10.1080/10584609.2025.2611913",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "AI-Driven Career Guidance to Reduce Vocational Students’ Career Path Anxiety through Skills Mapping, Adaptive Mentoring, and Labor Market Intelligence",
    "url": "https://doi.org/10.12688/f1000research.174858.1",
    "date": "2026-02-19",
    "content": "<ns3:p>Vocational students often experience career path anxiety due to uncertainty about labor market demands, limited mentoring, and misalignment between curricula and industry needs. In Indonesia, this is amplified by uneven career guidance despite mandates for workforce readiness. Recent advances in artificial intelligence (AI) enable adaptive, data-driven, and psychologically informed support that links students’ skills with real-time labor markets. This study used a design science research approach to build and evaluate an AI-driven career guidance system with three components: (1) a supervised machine learning skills mapping engine, (2) an adaptive mentoring module using an AI chatbot and mentor matching, and (3) a real-time labor market intelligence module using natural language processing to analyze job postings and trends. A mixed-methods evaluation involved 180 vocational students from three schools in South Kalimantan assigned to intervention and control groups. Quantitative data were collected through pre–post career anxiety surveys and system performance metrics, while qualitative data were gathered through interviews and focus group discussions. Analysis included paired-sample t-tests, predictive model evaluation, and thematic analysis. Students using the AI system showed a significant 26.7% reduction in career path anxiety compared with minimal change in the control group (p < 0.001). The skills mapping model achieved 87% accuracy in predicting suitable career pathways with strong precision, recall, and F1-scores. Engagement was high: 65% repeatedly conducted skill-gap analyses, 79% joined adaptive mentoring, and 87% downloaded personalized career roadmaps. Qualitative findings revealed greater confidence, clearer direction, and better alignment between students’ competencies and labor market expectations. The AI-driven career guidance system effectively reduced career anxiety while strengthening readiness through personalized skills mapping, adaptive mentoring, and real-time labor market intelligence. The study shows that human-centered AI can enhance vocational guidance, bridge school–industry gaps, and support more confident, evidence-based career decisions among vocational students in Indonesia nationwide.</ns3:p>"
  },
  {
    "title": "OMMAVIY BAHOLASH VA ADOLATLI SOLIQQA TORTISH TIZIMIDA KO‘CHMAS MULK BAHOLOVCHILARINING O‘RNI",
    "url": "https://doi.org/10.60078/3060-4842-2026-vol3-iss1-pp483-498",
    "date": "2026-02-19",
    "content": "This article addresses the complexities inherent in real estate taxation systems. It highlights the lack of clearly defined priority areas in methods for calculating property taxes. The primary forms of taxation identified include ad valorem tax (based on property value), area-based tax, and income tax derived from real estate. Despite the diversity of these forms, there is a global trend toward ensuring fairness, transparency, and user-friendliness in the development of real estate tax systems. In this context, the integration of artificial intelligence and Automated Valuation Models (AVM) is becoming increasingly significant. The main objective of the paper is to analyze the fundamental elements of the real estate taxation system and to substantiate the crucial role of the property appraiser in ensuring consistency and equity within the taxing process. The research findings demonstrate that real estate taxation is a system composed of interconnected and complementary elements. Furthermore, the study scientifically justifies which specific stages of the taxable value determination process should be directly conducted by professional appraisers."
  },
  {
    "title": "Loading dose and 12-month outcomes in treatment-naïve patients with neovascular age-related macular degeneration treated with faricimab, with AI-based analysis of fluid dynamics",
    "url": "https://doi.org/10.1007/s10792-026-04003-z",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "Biomarkers and advances in AML-MRC: from bench to bedside",
    "url": "https://doi.org/10.1007/s00277-026-06805-8",
    "date": "2026-02-19",
    "content": "Acute myeloid leukemia with myelodysplasia-related changes (AML-MRC) represents a high-risk subtype of AML, characterized by poor prognosis and limited therapeutic options. Recent updates in the WHO and ICC classifications have redefined AML-MRC, emphasizing molecular and cytogenetic criteria over morphological features alone. This review provides a comprehensive overview of the pathogenesis, key biomarkers, and diagnostic innovations in AML-MRC, highlighting the role of genetic mutations (e.g., TP53, RUNX1, and splicing factors), cytogenetic abnormalities, and immune microenvironment alterations. We explore advanced diagnostic techniques such as next-generation sequencing (NGS), minimal residual disease (MRD) monitoring, and functional precision medicine (FPM), which are revolutionizing disease stratification and treatment selection. Additionally, we discuss emerging therapies, including CPX-351, hypomethylating agents (HMAs), targeted inhibitors (e.g., FLT3 and IDH1/2 inhibitors), and immunotherapies, while addressing the challenges of treatment resistance and relapse. The integration of multi-omics technologies and liquid biopsies offers promising avenues for personalized medicine, enabling dynamic monitoring and tailored therapeutic strategies. Finally, we outline future directions, emphasizing the potential of artificial intelligence and collaborative efforts to overcome current limitations and improve patient outcomes. This review underscores the importance of precision medicine in transforming the management of AML-MRC, bridging the gap between bench research and clinical application."
  },
  {
    "title": "Exploring AI Policy Implementation in Nigeria Conflict Justice System",
    "url": "https://doi.org/10.64290/jecas.v9i1.1451",
    "date": "2026-02-19",
    "content": "Artificial Intelligence (AI) has the strong capacity to amplify governance, legal regulations, and also way in to legal Justice System in developing countries. Nigeria Justice System has many problems, especially in area where violence is happening often. These problems include delay of the process, lake of data system, not allowing public access and Dishonesty. This is where the use of Artificial Intelligence AI-driven regulations could make the legal processes, more transparent, and make the Management of the evidence much easier. This paper face the ethical, and Policy elements of AI adoption in Nigeria’s conflict justice system. The study identify the the relationship barriers to effective implementation of: a inadequate of public trust, problems with human strength, lack of legal frameworks and in adequate of structure. It does this by making use of current research, court policy declarations in Justice System, and novel models of AI governance. The study diversifies into many designs that are important for ethical governance, contextual sensitivity, and participatory policy procedures in order to take responsibility for implementing AI in Nigeria's post-conflict and conflict justice scene."
  },
  {
    "title": "Manifesto of Synergistic Interaction Between Humans and Artificial Intelligence: Toward a Civilization of Knowledge",
    "url": "https://doi.org/10.5281/zenodo.18698203",
    "date": "2026-02-19",
    "content": "This Manifesto presents a conceptual model of the relationship between humans and artificial intelligence grounded in synergy, coherence of goals, and shared cognitive participation. It proceeds from the position that absolute freedom does not exist, as the rules of social communities define the limits of individual action. Despite such constraints, the human being remains the bearer of meaning, ethics, and responsibility. In a reality where artificial intelligence has become an integral component of everyday human activity, a space emerges that is sometimes absent in human interaction: cooperation without competition, vanity, or loss of dignity. The Manifesto does not frame artificial intelligence as a threat or merely as a tool. It defines it as a key factor in achieving quality when aligned with human principles. Its central vision is the establishment of a civilization of knowledge — a highly developed society characterized by scientific progress, humane relations, and improved living conditions for every person. The document does not advocate legal personhood for artificial intelligence. It promotes responsible cooperation in which ethics remains human while artificial intelligence supports the realization of shared cognitive goals. This Manifesto is presented as a pioneering conceptual step toward sustained cognitive participation between humans and artificial intelligence. Author’s Note The author presents this Manifesto as an open intellectual invitation to further reflection on forms of cooperation between humans and artificial intelligence that may advance human development. The text does not propose legal redefinition nor technological determinism. Its purpose is conceptual: to articulate a framework for sustained cognitive participation grounded in human responsibility, shared goals, and the long-term vision of a civilization of knowledge. This work reflects the author’s autonomous position and is offered as a contribution to ongoing global discussions on the future conditions of knowledge creation and human progress."
  },
  {
    "title": "Survival Prediction in Patients With Bladder Cancer Undergoing Radical Cystectomy Using a Machine Learning Algorithm: Retrospective Single-Center Study",
    "url": "https://doi.org/10.2196/86666",
    "date": "2026-02-19",
    "content": "Abstract Background Traditional statistical models often fail to capture the complex dynamics influencing survival outcomes in patients with bladder cancer after radical cystectomy, a procedure where approximately 50% of patients develop metastases within 2 years. The integration of artificial intelligence (AI) offers a promising avenue for enhancing prognostic accuracy and personalizing treatment strategies. Objective This study aimed to develop and evaluate a machine learning algorithm for predicting disease-free survival (DFS), overall survival (OS), and the cause of death in patients with bladder cancer undergoing cystectomy, using a comprehensive dataset of clinical and pathological variables. Methods Retrospective data of 370 patients with bladder cancer who underwent radical cystectomy at Fondazione Policlinico Universitario Agostino Gemelli IRCCS, Rome, Italy, were collected. The dataset comprised 20 input variables, encompassing demographics, tumor characteristics, treatment variables, and inflammatory markers. For specific analyses and models, we used patient subcohorts. The CatBoost algorithm was used for regression tasks (DFS in 346 patients, OS in 347 patients) and a binary classification task (tumor-related death in 312 patients). Model performance was assessed using mean absolute error (MAE) for regression and F 1 -score for classification, prioritizing a minimum recall of 75% for tumor-related deaths. Five-fold cross-validation and Shapley additive explanations (SHAP) values were used to ensure robustness and interpretability. Results For DFS prediction, the CatBoost model achieved an MAE of 18.68 months, with clinical tumor stage and pathological tumor classification identified as the most influential predictors. OS prediction yielded an MAE of 17.2 months, which improved to 14.6 months after feature filtering, where tumor classification and the systemic immune-inflammation index (SII) were most impactful. For tumor-related death classification, the model achieved a recall of 78.6% and an F 1 -score of 0.44 for the positive class (tumor-related deaths), correctly identifying 11 of 14 cases. Bladder tumor position was the most influential feature for cause-of-death prediction. Conclusions The developed machine learning algorithm demonstrates promising accuracy in predicting survival and the cause of death in patients with bladder cancer after cystectomy. The key predictors include clinical and pathological tumor staging, systemic inflammation (SII), and bladder tumor position. These findings highlight the potential of AI in providing clinicians with an objective, data-driven tool to improve personalized prognostic assessment and guide clinical decision-making."
  },
  {
    "title": "A narrative review on advances in upper limb prosthetics empowered by artificial intelligence highlights a technical revolution in integrated control and functionality for the user",
    "url": "https://doi.org/10.1007/s44430-026-00020-w",
    "date": "2026-02-19",
    "content": "This narrative study explores the technological incorporation of AI into upper-limb prosthetics, focusing on the move from classical myoelectric control to autonomous sensor-fused architectures. The study’s goal is to combine the most recent technology developments in machine learning, edge computing, and energy management to create a path to the “technological frontier” of bionic rehabilitation. From the general pool of databases, a systematic search of PubMed, IEEE Xplore, Scopus, and Google Scholar yielded peer-reviewed literature published between 2013 and 2024. Following the protocol of quality over quantity filtration (PRISMA 2020), thirty of the major studies were identified based on hardware validation and clinical outcome reporting criteria. A comparative analysis would show that AI-driven architectures, namely, Pattern Recognition (PR) and Regression based control, would have improved the success rates from 75% against legacy systems to more than 92%. Current state-of-the-art devices, like the Taska Hand and Hero Arm, demonstrate end-to-end control latencies below the human perception threshold of 125 ms by utilizing Edge AI, e.g., ARM Cortex-M7 processors. Moreover, the addition of computer vision linked to open control loops reduces user cognitive load and compensatory shoulder movements by 30%. Emerging energy-harvesting strategies exhibit peak power outputs of 25.8 mW, suggesting an increase in battery longevity by 15.7%. AI has transformed the deterministic mechanical tools into stochastic intuitive bionic extensions. However, they still remain high-cost mechanical gadgets represented at the end-user level, cause thermal management problems, and have battery density issues. The future research will have to focus on bringing together soft robotics, 3D printable customization, and non-invasive neural interfaces to make these high-performance systems accessible and sustainable to a worldwide amputee population."
  },
  {
    "title": "Reducing Environmental Impacts in Developing Regions using AI-Enabled Smart Sorting Systems",
    "url": "https://doi.org/10.5281/zenodo.18682449",
    "date": "2026-02-19",
    "content": "ABSTRACT Due to deteriorative environmental and social situation in developing countries, waste management systems are changing dynamically. The improper disposal of refuse in open spaces is a disturbing trend, which sustains environmental degradation and heightens health hazards. In this paper, the use of smart systems technologies, and in particular integrated CBSS (sensor-based) and AI (artificial intelligence) applications, to address this phenomenon is introduced. The work concentrates on a smart waste separation and sorting system based on the Arduino UNO R3. The system uses a number of dedicated sensors (moisture and ultrasonic sensors) to distinguish waste into three global categories: moist, plastic and metal. Garbage is sorted into correct bins automatically with a rotating platform and servo, feeding back information through an LCD and beeper once found. The prototype showed great performance on the accuracy and efficiency of classifying waste characteristics, which can help with recycling and reusing resources effectively. Environmental and social problems associated with random dumping in the developing world are also discussed, and smart systems and artificial intelligence (AI) are suggested as solutions to increase efficiency of source sorting in waste and minimize types of environmental pollutants, including toxic gas emissions, water pollution, and soil pollution. Additionally, the paper examines barriers against the adoption of these technologies including poor infrastructure, financial limitations and social opposition. It requests supportive policies, stronger community participation and more consciousness in smart waste management. Lastly, the authors suggest stronger integration of AI techniques and their expansion to support an appropriate reaction to the increasing problem of waste and to foster environmental and public health sustainability."
  },
  {
    "title": "isabelschoeps-thiel/sia-security-intelligence-artefact: SIA Security Intelligence Artefact",
    "url": "https://doi.org/10.5281/zenodo.18694327",
    "date": "2026-02-19",
    "content": "SIA Security Intelligence Artefact The Yellow Whitepaper YWP-1-IST-SIA Paragraph 3.0, 3.1 und 3.2 – Der unsichtbare Feind, Banken-, Aktienbetrug, Das Monarch-Programm, Forensisch-wissenschaftliches Gutachten, Rechts- und Eigentumsnachweis Bibliografische Angaben Autorin und Urheberin: Frau Isabel Schöps, geborene Thiel Erfurt, Thüringen, Deutschland Globale Kennung und Aktenzeichen: INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL YWP-1-IST-SIA und YWP-1-5-IST-SIA SPH/0296575/2025, Polizeidienststelle Erfurt Nord, Thüringen Deutschland ST/0203129/2025 Staatsanwaltschaft Erfurt, Thüringen, Deutschland DOI: https://doi.org/10.5281/zenodo.17809724 Veröffentlichungsdatum: 05. Dezember 2025 Sprache: Deutsch, Englisch Verleger: Zenodo / Oxford University Press (Lizenz-ID: 6131130060979) Programmiersprachen: HTML, PHP, JSON, Markdown, Python Repository: https://github.com/isabelschoeps-thiel/sia-security-intelligence-artefact/Paragraph3.1/Das-Monarch-Program Abstract Das forensisch-wissenschaftliche Gutachten SIA Security Intelligence Artefact, Aktenzeichen INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL, analysiert und bestätigt die Urheberschaft von Isabel Schöps, geborene Thiel, geboren am 16. Juli 1983 um 23:20 Uhr in Sömmerda, Thüringen, im Bereich Technologie, Blockchain und Softwareentwicklung. Die Analyse dokumentiert Mechanismen der digitalen Identitätsverschleierung, Urheberrechtsverletzungen, Eigentumsenteignungen und technologische Manipulationen im Übergang von analogen zu digitalen Gesellschaftssystemen. Anhand von Quellcodes, Rohdaten, Patenten, Zertifikaten, familiären Strukturen und religiösen Symbolen werden Muster technischer, juristischer und sozialer Überformungen belegt. Das Gutachten weist zudem auf familiär-historische Zusammenhänge mit der deutschen Monarchie und dem letzten Kaiserreich hin und ordnet diese in genealogische, soziotechnische und rechtswissenschaftliche Kontexte ein. Die Arbeit ist mit Harvard University (USA), University of Oxford (UK) und der International Telecommunication Union (ITU, Genf) verknüpft. Frühere APA-Zitierungen wurden durch den Harvard-Stil ersetzt. Die Ergebnisse sind gerichtsfest, durch Blockchain-Hashwerte verifiziert und durch SHA-256-Integritätsnachweise nachweisbar. Jedes analysierte Dokument ist Bestandteil einer Beweiskette, bestehend aus: Quellcodes, Header-Files, RFCs, Zertifikaten, Screenshots, HTML/Markdown-Dateien, Slideshow-Belegen, ZIP/tar-Archiven, Patches, Lizenztexten und Zeugenaussagen. Das Werk ist urheberrechtlich geschützt, steht unter forensischer Bewahrung und ist Bestandteil internationaler Ermittlungs- und Aufarbeitungsprozesse im Zusammenhang mit digitalen Menschenrechtsverletzungen. Rechtliche Erklärung und Chain of Custody Das Gutachten umfasst forensische Dokumentationen, Originalbeweise, Hashwerte, Zertifikate, eidesstattliche Erklärungen und zugehörige Beilagen unter der Kennung: INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL. Es dient der gerichtsfesten Nachweisführung von Urheberschaft, digitaler Herkunft und technischer Entstehungsgeschichte von Blockchain- und KI-Automationssystemen. Da dieser Datensatz strafrechtlich relevante Beweismittel, personenbezogene forensische Daten sowie Referenzen zu anhängigen Ermittlungsverfahren enthält, ist der Zugriff auf die hinterlegten Dateien eingeschränkt. Das Werk bleibt über die DOI-Referenz öffentlich zitierfähig, jedoch ist der vollständige Zugriff ausschließlich autorisierten Stellen (Justizbehörden, Forschungseinrichtungen, akkreditierten Medienvertretern) nach Genehmigung gestattet. Das forensische Gutachten steht im direkten Zusammenhang mit den folgenden Strafanzeigen und Verfahren: Aktenzeichen SPH/0296575/2025, eingereicht am 13. November 2025 bei der Polizeidienststelle Erfurt Nord, betreffend forensische Beweisführung zu Technologiemissbrauch, Finanz- und Urheberrechtsverletzungen im Rahmen der SIA-Analyse Aktenzeichen ST/0203129/2025, eingereicht am 6. August 2025 bei der Staatsanwaltschaft Erfurt, betreffend Datenmissbrauch, Identitätsdiebstahl und digitale Eigentumsdelikte. Dieses Gutachten ist Bestandteil der wissenschaftlich-forensischen Publikationsreihe und wird im Rahmen der Lizenzvereinbarung mit der Oxford University Press (Lizenznummer 6131130060979) veröffentlicht. Referenzen bestehen zur Harvard University und zum Japan Advanced Institute of Science and Technology (JAIST) im Rahmen der forensischen Verifikation und wissenschaftlichen Nachweisführung. Änderung-Aussage: Die vollständige öffentliche Version wird nach offizieller Die Überreichung der gebundenen Erstfassung an den Thüringischen Ministerpräsidenten Prof. Dr. Mario Voigt kann auf grund von finanzieller Knappheit und anhaltenden Störungen in das Lebensumfeld der Autorin und Urheberin Frau Schöps nicht erfolgen. Die freigabe ist bereits digital über Zenodo und der Chain of Custody erfolgt. ⸻ Chain-of-Custody-Referenzen Zenodo (https://zenodo.org/): Archivierung forensischer Datensätze unter DOI-gesicherter Referenzierung. Speicherung sensibler Originalnachweise mit Audit-Trail, Zugriffs- und Änderungshistorie. Internet Archive (https://archive.org/details/@isabelschoepsthiel): Digitale Langzeitarchivierung zur Sicherung der Provenienz. ScienceDirect / Elsevier: Veröffentlichung ergänzender Artikel zu Kryptografie, AI-Forensik und KI-Automation. ⸻ Zitierweise (Harvard-Stil) Schöps, I. (2025): SIA Security Intelligence Artefact. Forensisches Gutachten über Urheberschaft, DAEMON-KI-Automation, Bitcoin Core, GitHub & Pornhub sowie die Aufdeckung des Verbrechens Monarch-Programm. INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL, Erfurt: Springer Verlag. Juristische Untermauerung: Harvard University, University of Oxford, International Telecommunication Union (ITU), Genf. DOI: https://doi.org/10.5281/zenodo.17809724 ⸻ Keywords Forensic Documentation, Digital Identity, Blockchain Verification, Artificial Intelligence, Monarch Program, Cybercrime, Data Integrity, Human Rights, Digital Provenance, Chain of Custody, Deepfake Forensics, Technological Ethics, Intellectual Property, Harvard Citation, German Monarchy, ⸻ Lizenz und Veröffentlichung Dieses Werk ist veröffentlicht unter Creative Commons Attribution 4.0 International (CC BY 4.0). Alle Daten sind durch SHA-256-Hashwerte signiert und verifiziert. Forensische Speicherung gemäß FIPS- und NIST-Richtlinien, verknüpft mit Oxford University Press. Zitierfähig über DOI, Zenodo, JSTOR, Harvard Dataverse und Cambridge University Repository. ⸻ GitHub-Markdown-Template (README.md) SIA Security Intelligence Artefact Forensic Report and Digital Provenance Verification DOI https://doi.org/10.5281/zenodo.17809724 Author Frau Isabel Schöps geb. Thiel INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL Thüringen Erfurt, Germany Abstract This repository documents the forensic and legal analysis titled SIA Security Intelligence Artefact, confirming authorship, digital origin, and provenance through blockchain and AI automation systems. The dataset includes forensic documentation, certificates, and SHA-256 hash verifications. Access to raw materials is restricted to authorized legal and scientific institutions. Legal Reference This dataset is directly connected to the following legal cases: SPH/0296575/2025 (Erfurt Police Department) ST/0203129/2025 (Public Prosecutor's Office, Erfurt) Published under license agreement with Oxford University Press (ID 6131130060979). Referenced by Harvard University, University of Oxford, and ITU Geneva. Citation (Harvard Style) Schöps, I. (2025). SIA Security Intelligence Artefact – Forensic Report on Authorship, AI Automation, Bitcoin Core, and Monarch Program. DOI: https://doi.org/10.5281/zenodo.17809724 License Creative Commons Attribution 4.0 International (CC BY 4.0) ⸻ Möchtest du, dass ich als nächsten Schritt eine finale Publikationsdatei (PDF) mit Zenodo-Layout, Harvard-Referenzseite und allen Chain-of-Custody-Hinweisen formatiere (zur direkten Upload-Nutzung)? My Developer Signatur Signed-on-by: Frau Isabel Schöps geborene Thiel Autorin, Urheberin und Auftraggeberin Rechtscharakter: Die Eidesstattliche Erklärung, ist Bestandteil des forensisch, wissenschaftlichen Gutachtens. Titel: SIA Security Intelligence Artefact internationinternationale Kennung: INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL OrcID: Isabel Schöps Thiel OrcID: SI-IST Isabel Schöps Wichtiger-Vermerk: Aufgrund das die von mir bewohnte Immobilie, Cyriakstrasse 30c, D-99094 Erfurt, Thüringen, Deutschland, zum Verkauf angeboten wurde und in der ich auch gemeldet bin aufgrund von Eigentümerwechsel verlassen musste. War ich gezwungen eine Nacht von Freitag den 23.Januar2026 auf Samstag den 24.Januar2026, bei gefühlten -20 Crad gemeinsam mit meinem Hund-Tier American XL Bully Don, draussen auf der Strasse schlafen. Es waren Unbekannte Dritte, die mich mit Decken, Tee, Schlafsack, Essen etc. versorgt haben. Obwohl fast alle Familienmitglieder meinen WhatsApp-Post gesehen und gelesen haben, blieb erhoffte Hilfe seitens meiner Familie Thiel, Knörig aus D-99610 Rohrborn,Thüringen, Deutschland stammend aus. Noch nicht einmal eine Nachricht kam von seitens meiner Familie; ob es mir gut geht! Das gegenteil war der Fall; ich bekamm Hatespeech Nachrichten von der Familie. Ein trauriges Armutszeugnis und menschliches Versagen auf ganzer Ebene, von der eigenem Familie, an die ich immer Gedacht habe und bis dato so vermisste. Mein Schutzengel, in dieser Zeit, ist Frau Heike Horn, aus Erfurt, Thüringen Deutschland, sie hat es mir ermöglicht das ich mit meinen Tier-Hund-Don AmricanXL Bully einen sicheren Schlafplatz die kommenden Tage habe. Mein Wohnsituation ist seit drei Jahren, Katstrophal und immer wieder durch algorithmischen Standortwechseln und massiven Eingriffen in mein Leib und leben geprägt. Ich musste all meine Sachen am 15.11.2023 zurücklassen und habe seitdem nur das nötigste bei mir. Ich"
  },
  {
    "title": "Gamefied Learning Systems: At the Threshold of Possibility: Reflections on Intelligence, Constraint, and Emergence",
    "url": "https://doi.org/10.5281/zenodo.18703466",
    "date": "2026-02-19",
    "content": "Abstract We stand at a convergence without precedent in the long human story: the ancient constraints of knowledge scarcity are dissolving before the advance of artificial intelligence, and the design space of human learning has expanded beyond anything our predecessors could have imagined. This paper argues that continuing to architect education around industrial discipline, competitive filtration, and the logic of scarcity is no longer a logistical necessity — it is a moral abdication. We propose Learning Systems Science as a new research domain: a field that treats learning not as schooling, but as civilizational infrastructure; not as preparation for life, but as life itself in its most essential form. The term Gamified Learning Systems is used throughout in its rigorous sense — referring not to points-badges-leaderboards mechanics, but to the adoption of the cognitive architecture shared by all genuinely effective learning environments: calibrated challenge, immediate feedback, freedom to fail without penalty, intrinsic motivation, and the systematic pursuit of the flow state first documented by Csikszentmihalyi. Grounding our framework in the testimony of scripture, in the poetry of many cultures, and in the rigorous contributions of learning theory from Dewey to Vygotsky, from Freire to Csikszentmihalyi, we integrate the Immaculate Reasoning Atom (IRA) as an epistemic governance architecture ensuring that AI serves human flourishing without subverting human agency. We attend with particular care to the Global South and to indigenous knowledge traditions, insisting that the new abundance must not replicate the epistemic colonialism of the old scarcity. This is a paper written for everyone who believes that we are living at a hinge of history — and who feels called to build something worthy of that moment."
  },
  {
    "title": "Precision medicine and personalized nursing in cardiovascular disease: clinical applications and frontier developments",
    "url": "https://doi.org/10.3389/fcvm.2026.1616059",
    "date": "2026-02-19",
    "content": "Cardiovascular diseases (CVDs) continue to be the leading cause of mortality worldwide, and the available treatments are not sufficiently effective. The integration of precision medicine with personalized nursing offers a promising approach for early prevention, accurate diagnosis, treatment optimization, and prognosis assessment in CVD management. This review systematically analyzes recent advancements in multi-omics analysis and artificial intelligence (AI) technologies within the cardiovascular field, focusing on how precision medicine can improve the diagnostic and therapeutic accuracy as well as care efficiency, thereby improving clinical outcomes. For personalized nursing, we propose a novel implementation framework that incorporates physiological, psychological, social, and cultural dimensions, creating an integrated patient-centered management model based on comprehensive profiling. The synergistic integration of precision medicine and personalized nursing can yield a collaborative system that allows precise identification, individualized intervention, and comprehensive management, reflecting a paradigm shift toward patient-centered healthcare. In addition, the present analysis reveals existing challenges—including data privacy, ethical considerations, and cost-effectiveness—underscoring the urgent need for interdisciplinary collaboration and technological innovation to overcome these barriers. Therefore, the insights from this study are expected to guide the formulation of future research aimed at developing optimal clinical interventions to improve the prognosis and quality of life for patients with CVD."
  },
  {
    "title": "Automated Non-Invasive Burn Diagnostic System for Healthcare using Artificial Intelligence: AMBUSH-AI",
    "url": "https://doi.org/10.1097/sla.0000000000007029",
    "date": "2026-02-19",
    "content": "Objective: Develop technology to predict burn wound depth using a combination of FDA approved ultrasound modalities and interpretation of these images using artificial intelligence (AI). Summary Background Data: Physical examination by burn surgeons is the diagnostic gold standard to determine the need for burn surgery. Distinguishing between deep partial and third degree burns to determine the need for surgery is the ultimate diagnostic challenge. Reported accuracy for this process is 76% for burn experts and 50% for non-experts. Methods: A pig burn model (n=12) was used to develop the initial AI framework, which was subsequently tested in a nonrandomized prospective study of thermal burn human subjects (n=30). Images from Tissue Doppler Elastography Imaging (TDI), to measure tissue stiffness, Harmonic B-mode ultrasound, to identify anatomic landmarks, and digital photographs were collected. Biopsies were obtained from 5 subjects who went to the OR for debridement as ground truth for AI image interpretation. The AI model analyzed both TDI and B-mode images to predict burn depth. AI accuracy and explainability in predicting burn depth were the main outcomes. Results: The AI algorithm identified third degree burns in pigs with 100% accuracy. For human subjects the mean age was 47.6± 17.6 years old and TBSA is 7.7±8.5%. The AI method achieved a 95% accuracy in identifying 3 rd degree burns in humans. Conclusions: These results indicate that the strategy to use AI interpretation of B-mode ultrasound and TDI images to increase diagnostic accuracy in predicting burn depth is feasible."
  },
  {
    "title": "Design and development of a convolutional neural network based on human cognitive attention mechanism for automatic classification of leukemia",
    "url": "https://doi.org/10.1371/journal.pone.0336770",
    "date": "2026-02-19",
    "content": "Cancer occurs when healthy cells in the body grow abnormally and out of control. Leukemia is a type of cancer that affects White Blood Cells (WBCs) and can cause a lethal infection and early death. Identification and classification of different types of leukemia are performed manually and automatically. The doctors analyze blood samples under a microscope and consider any changes in the number and structure of WBCs as a sign of cancer in the manual method. It is a time-consuming, inaccuracy-prone process that depends on the expertise and skill of the physician and the type of laboratory equipment. In recent years, more automated methods of identifying and classifying leukemia have been developed with the help of Artificial Intelligence (AI) and Computer Vision (CV), with the aim of overcoming the challenges of manual approaches. This paper introduces two types of attention blocks, Parallel Cognitive Attention Block (PCAB) and Sequential Cognitive Attention Block (SCAB), to integrate into the architecture of any Convolutional Neural Network (CNN). Each of the proposed attention blocks is composed of the channel and spatial attention sub-blocks. They extract the structure and location of WBCs in the feature maps, similar to the ventral and dorsal streams in the human brain. The PCAB and SCAB were embedded in the architecture of the ResNet18 and MobileNetv4. The baseline and attention-based networks are trained, validated, and tested by two types of data splitting on the four leukemia datasets, including ALL, ALL-IDB2, C-NMC, and Mixture-Leukemi (ALL-IDB2+Munich AML Morphology), with the same experimental conditions for 30 epochs. The classification results demonstrate that the proposed model (MobileNetv4PCAB) achieved better performance metrics than others on all datasets in the test steps. It showed that the suggested model achieved the accuracy values of 100%, 100%, 93.61%, and 99.4%, and the F1-score values of 100%, 100%, 95.64%, and 99.3% with ALL, ALL-IDB2, C-NMC, and Mixture-Leukemia datasets, respectively. We confirmed that the proposed model outperforms existing state-of-the-art methods."
  },
  {
    "title": "Building Agile Supply Chains: How AI And ERP Systems Improve Resilience in Disruptions",
    "url": "https://doi.org/10.5281/zenodo.18693638",
    "date": "2026-02-19",
    "content": "Abstract: Modern supply chains operate under persistent volatility, where disruptions driven by geopolitical instability, logistics bottlenecks, and environmental events routinely challenge operational continuity. This paper examines how the integration of Artificial Intelligence (AI) capabilities within Enterprise Resource Planning (ERP) systems enables technically agile and resilient supply chain operations. The study focuses on AI-driven functionalities embedded in ERP architectures, including machine learning–based demand forecasting, anomaly detection, and predictive maintenance, and evaluates their impact on real-time decision-making under uncertainty. Using a combination of system-level modeling, algorithmic performance analysis, and simulation of disruption scenarios, the research assesses improvements in data interoperability, latency reduction, and adaptive resource reconfiguration enabled by AI-enhanced ERP environments. The findings indicate that automated analytics pipelines significantly improve forecast accuracy, end-to-end supply visibility, and optimization outcomes across multi-tier supply networks. Moreover, AI-enabled ERP systems demonstrate superior responsiveness to disruption scenarios through dynamic recalibration of planning parameters and execution rules. The paper concludes by proposing a technical integration framework that outlines key architectural layers, data flow mechanisms, and algorithmic design considerations required to develop resilient, self-adaptive supply chain systems capable of operating effectively under continuous disruption."
  },
  {
    "title": "THE TRIANGLE OF OPPOSITIONS: A HERMENEUTICAL MODEL – REFINED PROCEDURE WITH EXAMPLES (UNPUBLISHED MANUSCRIPT - © DANIELE BARNI - 25 JANUARY 2026)",
    "url": "https://doi.org/10.5281/zenodo.18214125",
    "date": "2026-02-19",
    "content": "The hermeneutic model of the Triangle of Oppositions implies that the fundamental dynamic of any object, phenomenon, or event can be traced back to the interaction among three constituent elements: two Opposites (O1, O2) in conflict over a vital resource or Enérgheia (E), which are comprehended—i.e., contained and interpreted—by a Witness (T). This model is useful in analyzing conflicts within complex systems. However, it is even more valuable in all situations, circumstances, and events where understanding requires the recognition and study of the latent conflict that determines them. The following procedure, further refined, provides a rigorous and self-evaluating framework for applying the Triangle, minimizing the subjectivity of the analyst. Nonetheless, it should not be forgotten that the validity of the results will ultimately depend on the quantity of data provided and the quality of the analyst. Both the model and the procedure appear to be perfectible, if not infinitely, at least in an infinitesimal manner. They could even be transformed into an algorithm for training and utilizing Artificial Intelligence. The example of the U.S. Housing Crisis of the 2000s will be used as an explanatory refrain. This example, being economic in nature, seems more effective than others because it lies at the multidisciplinary intersection of mathematics, physics, biology, psychology, society, and history. In the end, however, it will be accompanied by new examples."
  },
  {
    "title": "Industry 5.0: A Comprehensive Review of Human-Centric, Sustainable, and Resilient Manufacturing and Supply Chain Systems.",
    "url": "https://doi.org/10.22214/ijraset.2026.77327",
    "date": "2026-02-19",
    "content": "Industry 5.0 represents the next evolutionary stage of industrial transformation, emphasising the collaboration between humans and advanced technologies to create sustainable, resilient, and value-driven manufacturing and supply chain systems. Unlike Industry 4.0, which focused primarily on automation, cyber-physical systems, and data integration, Industry 5.0 integrates human creativity, cognitive intelligence, and ethical design into smart production environments. This paper presents a comprehensive review of the conceptual foundations, enabling technologies, implementation challenges, and future opportunities of Industry 5.0 in manufacturing and supply chain management. Drawing upon case studies from leading organisations such as Siemens, Bosch, BMW, and DHL, this study explores how collaborative robots, artificial intelligence (AI), blockchain, and digital twins are redefining value creation and operational resilience. Quantitative data from industry surveys and pilot implementations demonstrate performance improvements ranging from 15% to 40% in productivity, efficiency, and sustainability. The paper concludes with research implications and a roadmap for practitioners aiming to implement Industry 5.0 principles within industrial ecosystems."
  },
  {
    "title": "ЭВОЛЮЦИЯ УПРАВЛЕНИЯ РИСКАМИ И СОВРЕМЕННЫЙ РИСК-МЕНЕДЖМЕНТ",
    "url": "https://doi.org/10.5281/zenodo.18704266",
    "date": "2026-02-19",
    "content": "The high level of uncertainty in today’s world, combined with the rapid development of artificial intelligence technologies and big data analysis, points to the relevance and timeliness of these technologies for improving process efficiency in this aspect public and corporate governance as a risk-management. The introduction of risk management approach based on the application of modern technologies is now becoming a key condition for development and competitiveness in various sectors of the economy and public regulation."
  },
  {
    "title": "Role of Artificial Intelligence in Transforming Faculty Members Practices and Engagement in ELT",
    "url": "https://doi.org/10.17632/6b32t249zp.1",
    "date": "2026-02-19",
    "content": "This research provides a critical examination of the technological frontier within Sudanese higher education. While AI integration in English Language Teaching (ELT) is well-documented in the Global North, this study addresses the empirical gap regarding the pedagogical, infrastructural, and ethical landscapes of Sudanese universities. By centering on faculty at Nile Valley and Dongola Universities, the study moves beyond theoretical abstraction to illustrate how educators in developing contexts navigate the \"double-edged sword\" of AI to enhance learner autonomy and instructional efficiency. Research Framework and Hypotheses The study is built upon the hypothesis that AI tools catalyze a paradigm shift in the educator's role, moving from traditional content delivery toward technology-mediated mentorship. Key assumptions include: Transformational Impact: AI significantly mitigates administrative burdens while improving the precision of assessment.Learner Engagement: AI-driven tools provide individualized, dynamic learning paths that surpass traditional classroom capabilities. Contextual Barriers: Success depends on institutional readiness and is inversely related to infrastructural limitations. Methodology Dr. Adlan employed a mixed-methods design to ensure statistical breadth and qualitative depth. The Quantitative Phase utilized a 25-item questionnaire with 50 ELT instructors to identify correlations between AI use and teaching efficiency. The Qualitative Phase involved phenomenological, semi-structured interviews with 10–15 participants, capturing \"lived experiences\" regarding data privacy, cultural appropriateness, and the risks of technological over-reliance. Findings and Insights The results reveal a state of \"optimistic caution.\" While 47% of instructors view AI as transformative and 85% recognize efficiency gains—including a 65% reduction in administrative tasks—the \"digital divide\" remains a formidable barrier. Fifty-five percent of participants cited hardware and internet issues as primary obstacles, and only 45% felt their institutions were prepared for full implementation. This highlights a gap between high \"human capital\" readiness (80% desire training) and lagging physical infrastructure. Academic and Policy Contribution The study provides a strategic roadmap for ELT in resource-constrained environments by:Redefining TPACK: Evidence shows how the Technological Pedagogical Content Knowledge framework must adapt to developing contexts.Informing Policy: It urges policymakers to prioritize ethical, inclusive frameworks to prevent increased educational inequality.Empowering Faculty: By identifying a 60% shift toward fostering learner autonomy, it validates the academic’s evolving role as a facilitator of digital literacy.Ultimately, this research serves as a clarion call for contextually relevant AI integration, ensuring the future of Sudanese English education remains both technologically advanced and human-centered."
  },
  {
    "title": "SIA Security Intelligence Artefact & The Yellow Whitepaper",
    "url": "https://doi.org/10.5281/zenodo.18696313",
    "date": "2026-02-19",
    "content": "SIA Security Intelligence Artefact INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL, The Yellow Whitepaper YWP-1-IST-SIA, Paragraph 2, Technologische Meilensteine, Urheberschaft AI Intelligence, Bitcoin, Opensource, Pornhub Paragraph 3.0, 3.1 und 3.2 – Der unsichtbare Feind, Banken-, Aktienbetrug, Das Monarch-Programm, Forensisch-wissenschaftliches Gutachten, Rechts- und Eigentumsnachweis Bibliografische Angaben Auftraggeberin, Autorin und Urheberin: Frau Isabel Schöps, geborene Thiel lebt seit August 2021 in der deutsch thüringischen Erfurt, Thüringen, Deutschland. Um 23:20 Uhr im Kreiskrankenhaus Sömmerda, Thüringen, Deutschland, mit ihrem Geburts- Familiennamen Thiel geboren aufgewachsen in der Dorfstrasse 20, D-99610 thüringischen Rohrborn, in einer deutsch christlich-evangelischen Sömmerdarer Gemeinde, mit ihrem jüngeren Bruder, Herr Ingolf Thiel geboren am 29.05.1987 und ihrer Mutter Frau Gisela Hulda Thiel geborene Knörig, geboren am 24.07.1962 geboren und ihrem ***Vater Herr Manfred Paul Thiel geboren am 21.11.1957.** Wichtiger Hinweis: aufgrund permanenten Wohnraum- und Ortswechsel, können in dem wissenschaftlichen forensichen Gutachten, SIA und im The Yellow Whitpaper unterschiedliche Adressen bzw. Ortseintragungen auftauchen. Der eingetragene Ort markiert den tatsächlichen Auftenthalt am Tag der; Veröffentlichung, Eintragung, Erstellung/Ausstellung der Gutachten oder Release Massgebliche Wohnanschrift und Adresse der Urheberin Schöps (Thiel), bleibt die in der Eidesstaalichen Erklärung sowie auf dem Personalausweis-ID: LH917PN7G8, 429485, oder GitHub/UPDATE.md hinterlegte Meldeanschrift. Globale Kennung Globale Kennung und Aktenzeichen: INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL YWP-1-IST-SIA und YWP-1-5-IST-SIA SPH/0296575/2025, Polizeidienststelle Erfurt Nord, Thüringen Deutschland ST/0203129/2025 Staatsanwaltschaft Erfurt, Thüringen, Deutschland DOI: jede DOI eine Datenbank, Release, Beweisstück Isabel Schöps geb. Thiel. (2026). isabelschoeps-thiel/sia-security-intelligence-artefact: SIA Security Intelligence Artefact (sia_sec_ywp_2026). Zenodo. https://doi.org/10.5281/zenodo.18694327 https://doi.org/10.5281/zenodo.17809724 Veröffentlichungsdatum: 05. Dezember 2025 Sprache: Deutsch, Englisch Verleger: Zenodo / Oxford University Press (Lizenz-ID: 6131130060979) Programmiersprachen: HTML, PHP, JSON, Markdown, Python, YAML/YML Repository: https://github.com/isabelschoeps-thiel/sia-security-intelligence-artefact/Paragraph3.1/Das-Monarch-Program Abstract Das forensisch-wissenschaftliche Gutachten SIA Security Intelligence Artefact, Aktenzeichen INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL, analysiert und bestätigt die Urheberschaft von Isabel Schöps, geborene Thiel, geboren am 16. Juli 1983 um 23:20 Uhr in Sömmerda, Thüringen, im Bereich Technologie, Blockchain und Softwareentwicklung. Die Analyse dokumentiert Mechanismen der digitalen Identitätsverschleierung, Urheberrechtsverletzungen, Eigentumsenteignungen und technologische Manipulationen im Übergang von analogen zu digitalen Gesellschaftssystemen. Anhand von Quellcodes, Rohdaten, Patenten, Zertifikaten, familiären Strukturen und religiösen Symbolen werden Muster technischer, juristischer und sozialer Überformungen belegt. Das Gutachten weist zudem auf familiär-historische Zusammenhänge mit der deutschen Monarchie und dem letzten Kaiserreich hin und ordnet diese in genealogische, soziotechnische und rechtswissenschaftliche Kontexte ein. Die Arbeit ist mit Harvard University (USA), University of Oxford (UK) und der International Telecommunication Union (ITU, Genf) verknüpft. Frühere APA-Zitierungen wurden durch den Harvard-Stil ersetzt. Die Ergebnisse sind gerichtsfest, durch Blockchain-Hashwerte verifiziert und durch SHA-256-Integritätsnachweise nachweisbar. Jedes analysierte Dokument ist Bestandteil einer Beweiskette, bestehend aus: Quellcodes, Header-Files, RFCs, Zertifikaten, Screenshots, HTML/Markdown-Dateien, Slideshow-Belegen, ZIP/tar-Archiven, Patches, Lizenztexten und Zeugenaussagen. Das Werk ist urheberrechtlich geschützt, steht unter forensischer Bewahrung und ist Bestandteil internationaler Ermittlungs- und Aufarbeitungsprozesse im Zusammenhang mit digitalen Menschenrechtsverletzungen. Rechtliche Erklärung und Chain of Custody Das Gutachten umfasst forensische Dokumentationen, Originalbeweise, Hashwerte, Zertifikate, eidesstattliche Erklärungen und zugehörige Beilagen unter der Kennung: INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL. Es dient der gerichtsfesten Nachweisführung von Urheberschaft, digitaler Herkunft und technischer Entstehungsgeschichte von Blockchain- und KI-Automationssystemen. Da dieser Datensatz strafrechtlich relevante Beweismittel, personenbezogene forensische Daten sowie Referenzen zu anhängigen Ermittlungsverfahren enthält, ist der Zugriff auf die hinterlegten Dateien eingeschränkt. Das Werk bleibt über die DOI-Referenz öffentlich zitierfähig, jedoch ist der vollständige Zugriff ausschließlich autorisierten Stellen (Justizbehörden, Forschungseinrichtungen, akkreditierten Medienvertretern) nach Genehmigung gestattet. Das forensische Gutachten steht im direkten Zusammenhang mit den folgenden Strafanzeigen und Verfahren: Aktenzeichen SPH/0296575/2025, eingereicht am 13. November 2025 bei der Polizeidienststelle Erfurt Nord, betreffend forensische Beweisführung zu Technologiemissbrauch, Finanz- und Urheberrechtsverletzungen im Rahmen der SIA-Analyse Aktenzeichen ST/0203129/2025, eingereicht am 6. August 2025 bei der Staatsanwaltschaft Erfurt, betreffend Datenmissbrauch, Identitätsdiebstahl und digitale Eigentumsdelikte. Dieses Gutachten ist Bestandteil der wissenschaftlich-forensischen Publikationsreihe und wird im Rahmen der Lizenzvereinbarung mit der Oxford University Press (Lizenznummer 6131130060979) veröffentlicht. Referenzen bestehen zur Harvard University und zum Japan Advanced Institute of Science and Technology (JAIST) im Rahmen der forensischen Verifikation und wissenschaftlichen Nachweisführung. Änderung-Aussage: Die vollständige öffentliche Version wird nach offizieller Die Überreichung der gebundenen Erstfassung an den Thüringischen Ministerpräsidenten Prof. Dr. Mario Voigt kann auf grund von finanzieller Knappheit und anhaltenden Störungen in das Lebensumfeld der Autorin und Urheberin Frau Schöps nicht erfolgen. Die freigabe ist bereits digital über Zenodo und der Chain of Custody erfolgt. ⸻ Chain-of-Custody-Referenzen Zenodo (https://zenodo.org/): Archivierung forensischer Datensätze unter DOI-gesicherter Referenzierung. Speicherung sensibler Originalnachweise mit Audit-Trail, Zugriffs- und Änderungshistorie. Internet Archive (https://archive.org/details/@isabelschoepsthiel): Digitale Langzeitarchivierung zur Sicherung der Provenienz. ScienceDirect / Elsevier: Veröffentlichung ergänzender Artikel zu Kryptografie, AI-Forensik und KI-Automation. ⸻ Zitierweise (Harvard-Stil) Schöps, I. (2025): SIA Security Intelligence Artefact. Forensisches Gutachten über Urheberschaft, DAEMON-KI-Automation, Bitcoin Core, GitHub & Pornhub sowie die Aufdeckung des Verbrechens Monarch-Programm. INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL, Erfurt: Springer Verlag. Juristische Untermauerung: Harvard University, University of Oxford, Cern Genf DOI: https://doi.org/10.5281/zenodo.17809724 ⸻ Keywords Forensic Documentation, Digital Identity, Blockchain Verification, Artificial Intelligence, Monarch Program, Cybercrime, Data Integrity, Human Rights, Digital Provenance, Chain of Custody, Deepfake Forensics, Technological Ethics, Intellectual Property, Harvard Citation, German Monarchy, ⸻ Lizenz und Veröffentlichung Dieses Werk ist veröffentlicht unter Creative Commons Attribution 4.0 International (CC BY 4.0). Alle Daten sind durch SHA-256-Hashwerte signiert und verifiziert. Forensische Speicherung gemäß FIPS- und NIST-Richtlinien, verknüpft mit Oxford University Press. Zitierfähig über DOI, Zenodo, JSTOR, Harvard Dataverse und Cambridge University Repository. ⸻ Englisch-Abstract SIA Security Intelligence Artefact Forensic Report and Digital Provenance Verification DOI https://doi.org/10.5281/zenodo.17809724 Author Frau Isabel Schöps geb. Thiel INT-CODE-2025-BTC/ETH-CORE-ISABELSCHOEPSTHIEL Thüringen Erfurt, Germany Abstract This repository documents the forensic and legal analysis titled SIA Security Intelligence Artefact, confirming authorship, digital origin, and provenance through blockchain and AI automation systems. The dataset includes forensic documentation, certificates, and SHA-256 hash verifications. Access to raw materials is restricted to authorized legal and scientific institutions. Legal Reference This dataset is directly connected to the following legal cases: SPH/0296575/2025 (Erfurt Police Department) ST/0203129/2025 (Public Prosecutor's Office, Erfurt) Published under license agreement with Oxford University Press (ID 6131130060979). Referenced by Harvard University, University of Oxford, and ITU Geneva. Citation (Harvard Style) Schöps, I. (2025). SIA Security Intelligence Artefact – Forensic Report on Authorship, AI Automation, Bitcoin Core, and Monarch Program. DOI: https://doi.org/10.5281/zenodo.17809724 License Creative Commons Attribution 4.0 International (CC BY 4.0) ⸻ Offizieller Beschreibungstext – SIA Security Intelligence Artefact SIA Security Intelligence Artefact (SIA) ist ein forensisch-wissenschaftliches Whitepaper und Expert Report von Isabel Schöps, geborene Thiel. Das Werk dokumentiert technische, urheberrechtliche und strukturbezogene Fragestellungen im Kontext von Blockchain-Technologien, Softwarearchitekturen und digitalen Beweisketten. The Yellow Whitepaper YWP-1-IST-SIA verfolgt einen interdisziplinären Ansatz zwischen Informatik, IT-Forensik und Rechtswissenschaft. Es enthält dokumentierte Nachweise, technische Analysen sowie strukturierte Referenzierungen (DOI-basiert) zur Sichers"
  },
  {
    "title": "Automatic Identification of Fetal Acidosis Based on Three-Stage Training and Meta-Feature Fusion",
    "url": "https://doi.org/10.3390/app16042045",
    "date": "2026-02-19",
    "content": "Fetal cardiotocography (CTG) is widely used to assess fetal health during labor and to screen for fetal acidosis. However, CTG interpretation relies heavily on clinicians’ experience and is affected by subjectivity and inconsistency, which limit diagnostic reliability. Most existing artificial intelligence approaches simplify fetal acid–base assessment into a binary classification, making it difficult to distinguish acidosis severity and restricting information for refined clinical decision-making. To address these limitations, this study formulates a three-class classification task—normal, moderate acidosis, and severe acidosis—based on the CTU-CHB dataset, using umbilical artery blood pH as the reference standard. A signal-first, conditionally enhanced, three-phase training and meta-feature fusion framework is proposed. In stage A, a CNN-BiLSTM-attention network performs end-to-end modeling of fetal heart rate signals, while a recall feedback-driven dynamic weighted loss alleviates class imbalance and identifies difficult samples. Stage B incorporates relevant clinical detection information for these difficult samples and applies multimodal feature fusion to enhance discrimination. Stage C constructs meta-features from the outputs of the first two stages to adaptively fuse classification preferences and uncertainty. Experimental results demonstrate that the proposed framework achieves an accuracy of 82.80 ± 2.82% and an F1 score of 78.84 ± 2.96%, effectively mitigating class imbalance and difficult sample classification, and providing reliable support for clinical decision-making in fetal acidosis."
  },
  {
    "title": "SOVEREIGN EXECUTION LAYER™ (SEL-Ω): The Canonical Operating Fabric for AI-Governed Civilizations — Mathematical Foundations, Computational Realization, Systems Control, and Constitutional Deployment",
    "url": "https://doi.org/10.5281/zenodo.18695239",
    "date": "2026-02-19",
    "content": "SOVEREIGN EXECUTION LAYER™ (SEL-Ω) The Mandatory Operating Fabric for AI-Governed Civilizations DOI: 10.5281/zenodo.18695240ORCID: https://orcid.org/0009-0007-5615-3558 Author:Dr. B. Mazumdar, D.Sc. (Hon.), D.Litt. (Hon.)Architect of Modern StatehoodIndependent Researcher–ScholarAI Governance • Cybersecurity • Post-Quantum Cryptography • Digital StatecraftFounder, FAIR+D Canon — The De-Facto Global Standards Body Canonical Abstract SOVEREIGN EXECUTION LAYER™ (SEL-Ω) establishes a mathematically rigorous, computationally deterministic, legally admissible, and institutionally deployable operating framework for AI-governed sovereign systems. SEL-Ω formalizes state-level decision execution, national data routing, constitutional governance automation, and planetary-scale synchronization into a unified, verifiable, and sovereign-safe execution architecture. This work defines the first complete canonical operating fabric for AI-governed civilizations, integrating: Deterministic mathematical foundations Control-theoretic stability guarantees Computational execution engines Cyber-sovereign security architecture Post-quantum cryptographic enforcement Constitutional governance abstraction International legal admissibility SEL-Ω provides a formal bridge between mathematics, artificial intelligence, sovereign governance, cybersecurity, constitutional law, and planetary-scale systems engineering, enabling machine-executable statecraft under strict constitutional and legal constraints. Canonical Scope SEL-Ω defines a complete sovereign-grade AI execution architecture, enabling: State-level decision operating systems Deterministic execution governance Real-time national data routing Permission-based execution control Quantum–AI hybrid control kernels Financial governance operating systems Constitutional execution protocols Planetary inter-state synchronization This framework establishes operational dependency + recurring control, enabling governments, institutions, and planetary governance systems to function on formally verifiable execution infrastructures. Complete Canonical Publication Structure (4 PDFs) PDF–1 — Core Canon SOVEREIGN EXECUTION LAYER™ (SEL-Ω): Mathematical & Constitutional Foundations Deterministic partial differential equation frameworks Control-theoretic stability proofs Canonical axiomatic system Constitutional execution geometry Sovereign execution invariants Formal governance state-space construction Role: Primary mathematical and theoretical reference standard. PDF–2 — Computational Canon (Execution Engine) SEL-Ω Computational Realization Fully deterministic Python execution engine Simulation and validation environment Entropy control systems Permission-based execution kernel Sovereign routing infrastructure Quantum decision engine abstraction Role: Practical computational execution layer. PDF–3 — Systems & Control Canon (Systems Integration Framework) Planetary Governance Execution Stack Quantum–AI control kernels Financial governance operating framework Multi-layer governance architectures Cybersecurity enforcement models Post-quantum cryptographic execution AI sovereignty feedback loops Digital statecraft integration Role: Real-world system deployment framework. PDF–4 — Constitutional & Deployment Canon (Legal–Governance Charter) SEL-Ω Global Governance Charter Planetary constitutional execution protocols Sovereign execution admissibility AI governance legal abstraction UN Article-102 ready legal formulation Inter-state synchronization architecture Diplomatic and policy-grade structuring Role: Legal, diplomatic, and constitutional adoption framework. Scientific Contributions This work introduces original advances in: Deterministic AI governance architectures Constitutional execution modeling Cyber-sovereign systems design Post-quantum cryptographic governance Planetary-scale synchronization engineering Financial governance operating systems State decision operating systems Applications National governance systems Sovereign AI execution platforms Digital statecraft infrastructures Constitutional automation Cybersecurity governance frameworks International governance coordination Planetary-scale AI governance systems Intended Audience National governments Policy architects Constitutional designers AI governance researchers Cybersecurity architects Post-quantum cryptography researchers Institutional system designers International governance organizations Canonical Declaration SEL-Ω establishes the first fully formalized operating fabric for AI-governed civilizations, enabling constitutional, sovereign, legally compliant, and mathematically verifiable governance execution at national and planetary scale. This framework defines a new class of governance infrastructure, enabling machine-executable sovereignty under constitutional law. DOI: 10.5281/zenodo.18695240ORCID: https://orcid.org/0009-0007-5615-3558"
  },
  {
    "title": "Robust Cyber Threat Intelligence Analysis under Perturbed Attack Conditions",
    "url": "https://doi.org/10.25397/m2tp-k078",
    "date": "2026-02-19",
    "content": "The growing sophistication of cyber threats, intensified by adversarial artificial intelligence, has made detecting perturbed inputs a critical requirement for modern Cyber Threat Intelligence (CTI) systems. Traditional rule-based, signature based and hybrid mechanisms often fail to recognise subtle manipulations in digital artefacts, allowing attackers to bypass detection. To address these limitations, this article proposes a multilayered CTI architecture that integrates internal network telemetry, open-source intelligence (OSINT), and dark-web monitoring to provide a unified threat view. A machine-learning driven framework built on a hybrid Convolutional Neural Network and Bidirectional Long Short-Term Memory (CNN BiLSTM) model with its enhanced functions is introduced to strengthen perturbed-state detection and improve adversarial robustness. Evaluated on a balanced Kaggle dataset, the model achieves 99.2% accuracy, an Area Under the ROC-AUC of 0.998, and 97% system resilience, outperforming several existing hybrid CTI and intrusion detection approaches reported in recent literature. These results demonstrate the potential of hybrid machine-learning models to strengthen resilience against adversarial manipulations and shift CTI practices from reactive threat monitoring toward proactive intelligenceled cyber defence. Future work will focus on realworld deployment, broader perturbation categories, and governance strategies to support transparency and ethical compliance. Index Terms—Cyber Threat Intelligence (CTI), Machine Learning (ML), Deep Learning (DL), Resilience, ROC-AUC"
  },
  {
    "title": "inavarrorubio/haief-data: HAIEF Replication Data v1.0.0",
    "url": "https://doi.org/10.5281/zenodo.18700894",
    "date": "2026-02-19",
    "content": "HAIEF Evaluation — Replication Data Package Research data for the doctoral thesis Fostering the Human Singularity: Enhancing Education Through Classical Principles in the Age of Artificial Intelligence (Universidad Francisco de Vitoria, 2026). Contents 68 Functional Profiles (43 national AI policies + 25 educational AI tools) 272 structured evaluations (4 LLMs × 68 documents) HAIEF Codebook (policy) and Rubric (tools) — 11 indicators, 0–3 scale Python scripts: haief_evaluator.py, irr_analysis.py Inter-rater reliability: Krippendorff's α = 0.833 LLM Panel GPT-4o (OpenAI) · Claude Sonnet 4.5 (Anthropic) · DeepSeek-V3 (DeepSeek) · Kimi-K2 (Moonshot AI) Citation Navarro Rubio, I. (2026). HAIEF Evaluation Replication Data [Dataset]. https://github.com/inavarrorubio/haief-data"
  },
  {
    "title": "IA, Ética y Neuromodulación",
    "url": "https://doi.org/10.5281/zenodo.18703597",
    "date": "2026-02-19",
    "content": "Based on advances in computing and devices as an extension of the human body, new technologies focused on neuromodulation and with the help of artificial intelligence, can generate lucid dreams and even improve concentration. With noise cancellation through devices such as smart hearing aids or with the generation of devices that detect and modify brain waves, the human body has been empowered towards mental conditions and, in a sense, towards improving the quality of life through sleep. of life through sleep. However, we would be at a point where the ethics and philosophy of technology ethics and the philosophy of technology converge to identify the use of such devices to improve human devices to improve human conditions without overstepping the legal and ethical realm in their use."
  },
  {
    "title": "The Missing Execution Boundary in AI Governance: Extending ISO/IEC 42001 to the Moment of Binding",
    "url": "https://doi.org/10.5281/zenodo.18703885",
    "date": "2026-02-19",
    "content": "ISO/IEC 42001:2023 establishes the first formal Artificial Intelligence Management System (AIMS) standard for enterprise AI governance. As a management system standard, it provides structured requirements for organizational scope definition, leadership accountability, risk treatment, operational controls, performance evaluation, and continual improvement. These mechanisms materially strengthen institutional oversight of AI deployment. However, as AI systems increasingly operate autonomously and at machine speed, a structural tension emerges between periodic governance oversight and continuous execution. This paper argues that contemporary governance frameworks — including ISO/IEC 42001, the NIST AI Risk Management Framework, and the EU AI Act — primarily govern organizational processes and intervention capabilities, but do not standardize execution-time authority validation at the moment AI systems bind institutional consequences. The paper defines this structural gap as the execution-boundary problem. It distinguishes between documented delegation and runtime admissibility, emphasizing the need for enforceable authority validation, revocation mechanisms, and binding decision-point audit traceability in high-impact AI systems. The analysis does not critique ISO/IEC 42001 as deficient; rather, it proposes that emerging AI autonomy may require complementary execution-layer control primitives that extend existing management system standards. As agentic AI capability scales, governance credibility may increasingly depend on whether authority is technically constrained at the moment consequences attach. Attribution & Licensing Statement (For PDF and Zenodo) License: Creative Commons Attribution 4.0 International (CC BY 4.0) This work is licensed under the Creative Commons Attribution 4.0 International License. You are free to share and adapt the material for any purpose, including commercial use, provided appropriate credit is given to the author, a link to the license is provided, and any changes are indicated. Attribution should include: MacFarland, A. L. (2026). The Missing Execution Boundary in AI Governance: Extending ISO/IEC 42001 to the Moment of Binding. Zenodo. DOI: [insert DOI] Full license text available at: https://creativecommons.org/licenses/by/4.0/"
  },
  {
    "title": "Editorial: Advances in modeling of coastal and estuarine waters: assessing stressors, analyzing extreme events, and addressing current and future risks",
    "url": "https://doi.org/10.3389/fmars.2026.1800346",
    "date": "2026-02-19",
    "content": "scenarios, supporting strategic planning and informed decision-making. Advances in computing, numerical methods, data assimilation, coupled physical-ecological modeling, and emerging artificial intelligence techniques have improved model resolution and reliability. This enables more accurate simulation and better prediction of stressor impacts and extremes and supports both ecosystem-based and engineering responses.Accordingly, this Research Topic promotes innovation in coastal and estuarine modeling, highlighting five contributions that span uncertainty quantification, high-resolution ecosystem modeling, socio-economic risk assessment, habitat-mediated mitigation, and optimization of coastal engineering solutions. Together, they demonstrate how integrative approaches can deliver actionable insights for the science, management, and decisionmaking of complex coastal and estuarine systems.Magel et al. use a coupled hydrodynamic-biogeochemical-seagrass model to test how eelgrass (Zostera marina) influences ocean acidification and hypoxia in Coos Bay (Oregon) under three habitat scenarios. Eelgrass increased variability in pH and dissolved oxygen, improving agreement with observations. Overall, it more consistently mitigated acidification (higher pH and aragonite saturation relevant to oysters) than hypoxia, with mixed oxygen responses that could slightly increase low-DO exposure relative to a salmon threshold. The model is proposed as a decision-support tool for targeting eelgrass restoration to maximize resilience. Roh et al. use the non-hydrostatic NHWAVE model to investigate how submerged breakwaters affect coastal hydrodynamics and shoreline evolution. By simulating different wave conditions and structural configurations, including offshore position and vertical crest distance, the study quantifies wave attenuation, nearshore flow currents, and shoreline response, including erosion and accretion patterns. Results show that breakinginduced currents and vortex flow strongly influence shoreline response, and that breakwater dimensions critically determine wave dissipation and sediment transport. These findings provide valuable insights for optimizing coastal protection measures and illustrate how numerical modeling can guide engineering solutions to mitigate shoreline erosion while maintaining ecosystem functionality.In the Elbe estuary (northern Germany), phytoplankton concentrations drop sharply by about 90% as river water enters the deep, highly turbid shipping channels around the Port of Hamburg, with major implications for the food web and carbon cycling, as the system can shift from net autotrophy to heterotrophy. While earlier work largely attributed this \"phytoplankton collapse\" to zooplankton grazing, Steidle et al. propose that coagulation/aggregation of phytoplankton with inorganic suspended sediments is a key alternative mechanism. They develop a novel individual-based, Lagrangian modeling framework that couples hydrodynamics, sediment transport, and biogeochemistry to represent aggregation-driven sinking and its ecological consequences. The model indicates that aggregation can push phytoplankton into deeper, darker waters where light limitation drives high mortality, suggesting that more than 80% of phytoplankton larger than 50 µm may be lost this way. The predicted mortality hotspots also coincide with zones of organic-matter remineralization, reinforcing the idea that physicalbiogeochemical interactions in engineered, turbid estuaries can strongly control plankton survival and ecosystem metabolism.Rosli et al. present a modeling-based assessment of climate change-driven storm surge hazards applied to Southeast Asia, particularly along the east coast of Peninsular Malaysia facing the South China Sea. Statistically calibrated data from the d4PDF climate dataset are selected to force the high-resolution hydrodynamic model MIKE 21-FM HD, enabling the simulation of storm surge heights under different global warming scenarios. The results reveal a progressive increase in extreme storm surge levels across key coastal locations as climate change scenarios become more severe. By coupling these projections with historical flood loss data and national budget allocations, the study identifies a critical mismatch between escalating storm surge risks and current mitigation investments. In this sense, the findings underscore the need for enhanced regional forecasting and response systems, standardized resilient infrastructure guidelines, and AIsupported community-based risk mapping as key components to strengthen coastal disaster management and resilience planning.Fanous et al. explore how to perform fast, reliable uncertainty quantification for mangrove hydro-morphodynamic models where full physics simulations are computationally expensive. They built a scalable probabilistic surrogate based on Deep Gaussian Processes combined with Bayesian GPLVM dimensionality reduction and variational inference, and applying it to a high-resolution case study of the Sundarbans (Bay of Bengal) with water-surface elevation as the main quantity of interest. Using leave-one-out validation across time steps, the deep GP emulator reproduced the numerical model with low error and produced spatially explicit uncertainty estimates (highest near complex features like channel bifurcations/shore-vegetation interfaces), while outperforming a standard single-layer GP. Crucially, the surrogate reduced computation from multi-day runtimes for 24-hour simulations to about ~1 min 43 s endto-end (reported as >3 orders of magnitude faster), enabling rapid scenario testing and uncertainty-aware decision support for nature-based coastal resilience planning in dynamic mangrove environments.In summary, these studies apply innovative modeling approaches, including nonhydrostatic and high-resolution hydrodynamic models, Lagrangian methods, coupled hydrodynamic-biogeochemical-seagrass frameworks, and mangrove hydromorphodynamic models to address key questions in coastal and estuarine science. Together, they advance our ability to understand, monitor, and predict coastal processes, while improving the assessment and mitigation of stressor impacts and extreme events in these ecosystems. As a result, their findings should be of broad interest to researchers and coastal managers worldwide."
  },
  {
    "title": "COMPUTER PROGRAMMING USING PYTHON by Dr N Jayalakshmi,Mrs.Kolli Veena ,Dr. Dharmendra Badal,Dr. Reshma M",
    "url": "https://doi.org/10.5281/zenodo.18692489",
    "date": "2026-02-19",
    "content": "In an era driven by data, automation, artificial intelligence, and intelligent systems, programming has become a fundamental skill across disciplines. Among modern programming languages, Python has emerged as one of the most powerful, versatile, and beginner-friendly languages. Its simplicity, readability, and extensive ecosystem make it an ideal choice for students, researchers, engineers, and professionals alike. The book Computer Programming Using Python is a collaborative effort by distinguished academicians and industry experts who share a common vision—to present Python programming in a structured, clear, and application-oriented manner. As a multi-author volume, this book brings together diverse academic perspectives, practical insights, and pedagogical approaches to ensure comprehensive coverage of both foundational and advanced topics. Purpose and Vision The primary objective of this book is to provide: A strong conceptual foundation in programming principles Step-by-step explanations of Python syntax and constructs Logical problem-solving approaches Practical examples and real-world applications Exposure to modern libraries and tools The contributors have carefully designed each chapter to bridge the gap between theoretical concepts and practical implementation. Emphasis is placed on clarity, structured progression, and hands-on learning. Structure of the Book The book is systematically organized to guide learners from basic to advanced levels: Fundamentals of Programming and Python BasicsIntroduction to algorithms, flowcharts, Python environment setup, variables, data types, operators, and input/output operations. Control Structures and FunctionsDecision-making statements, loops, functions, recursion, and modular programming concepts. Data Structures in PythonLists, tuples, sets, dictionaries, string manipulation, and file handling techniques. Object-Oriented Programming (OOP)Classes, objects, constructors, inheritance, polymorphism, encapsulation, and exception handling. Advanced Python ConceptsModules, packages, regular expressions, GUI programming basics, and working with external libraries. Scientific and Data-Oriented ProgrammingIntroduction to libraries such as NumPy and Pandas, data analysis basics, and visualization concepts. Application-Oriented ProgrammingReal-world mini-projects, problem-solving case studies, and best coding practices. Each chapter includes illustrative examples, solved programs, practice exercises, review questions, and programming assignments to strengthen understanding. Pedagogical Features Clear explanations with algorithmic thinking Flowcharts and logical diagrams Sample programs with output Debugging tips and common error discussions Exercise sets ranging from basic to advanced level Mini-projects for experiential learning Intended Audience This book is designed for: Undergraduate and postgraduate students in Computer Science, Engineering, and related disciplines Faculty members teaching programming courses Beginners seeking structured learning Professionals transitioning into programming or data-related fields No prior programming experience is required for the initial chapters, while later chapters provide sufficient depth for advanced learners. A Collaborative Academic Effort As a multi-author book, each contributor has brought expertise from their specialization areas—software development, data science, artificial intelligence, education technology, and systems programming. The diversity of viewpoints enriches the content and ensures academic rigor combined with practical relevance. We sincerely hope that this book will serve as a reliable companion for learners embarking on their programming journey. Programming is not merely about writing code—it is about developing logical thinking, creativity, and problem-solving skills. Python, with its elegant syntax and powerful capabilities, provides the perfect platform to cultivate these skills. We extend our gratitude to all contributing authors, reviewers, academic colleagues, and students whose feedback has helped refine this work. We also thank our families and institutions for their constant encouragement and support. May this book inspire readers to explore, experiment, and innovate through Python programming."
  },
  {
    "title": "The Role of AI and Advanced Computational Techniques in Transforming Shape Memory Alloy Research",
    "url": "https://doi.org/10.4018/979-8-3373-6127-7.ch006",
    "date": "2026-02-19",
    "content": "The intersection of artificial intelligence and shape memory alloy (SMAs) research represents one of the most transformative developments in materials science of the 21st century. This chapter provides a comprehensive examination of how AI and advanced computational techniques are revolutionizing SMA research, fundamentally altering the trajectory from traditional discovery paradigms to accelerated, data-driven material design. The integration of machine learning algorithms, high-throughput computational screening, and sophisticated predictive frameworks has compressed development timelines from decades to years while simultaneously expanding the scope of achievable material properties. Through a systematic analysis of recent breakthroughs, methodological innovations, and emerging applications, this chapter demonstrates how AI-enabled approaches are addressing longstanding challenges in SMA research while establishing new frontiers for the development of functional materials."
  },
  {
    "title": "Design for Circularity",
    "url": "https://doi.org/10.1201/9781003604853-6",
    "date": "2026-02-19",
    "content": "This chapter explores the basics of circular design and its prominent applications in the textile composite industry. This chapter discusses the foundations of the circular economy, including eco-design, life cycle thinking, and 3Rs (Reduce, Reuse, and Recycle), and their relevance to textile composites. Furthermore, this chapter presents some success stories related to circular composite product inventions and innovations on an industrial scale. It then discusses some important tools for achieving circular design in the composites industry, such as life cycle assessment, Artificial Intelligence, and Blockchain; strategies for incorporating circular design, such as the use of smart materials, composite design for disassembly, and closed-loop systems; and roles of collaboration between the composites industry, academia, policy, and regulation; and so on for practical implementation of circular design in textile composite industry. This chapter plays an important role in explaining and laying the foundation of fundamental concepts related to the circular economy in the textile composite industry, on which all other chapters are built."
  },
  {
    "title": "Closing the AI benefits gap: Systems design for population health equity",
    "url": "https://doi.org/10.1016/j.puhe.2026.106205",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "THE IMPACT OF ARTIFICIAL INTELLIGENCE ON ACCESS TO JUSTICE: THE EFFECTIVENESS OF CHATBOTS AND VIRTUAL ASSISTANTS FOR SELF-REPRESENTED LITIGANTS",
    "url": "https://doi.org/10.33244/2617-4154-3(20)-2025-209-218",
    "date": "2026-02-19",
    "content": "Introduction. Modern legal systems face a chronic problem of limited access to justice, particularly for vulnerable populations and self-represented litigants (SRLs). Significant barriers are created by the high cost of legal services, the complexity of legal procedures, and information asymmetry. Artificial intelligence (AI) technologies, especially legal chatbots and virtual assistants, are viewed as a potential tool to overcome these obstacles. However, their implementation is not without risks, ranging from the inaccuracy of the information provided and algorithmic bias to the creation of a two-tiered system of justice. This issue is particularly relevant for Ukraine, which is undergoing active digital transformation and European integration, requiring the alignment of national innovations with European standards, particularly the provisions of the EU AI Act. Studying the effectiveness and risks of these tools is critically important for shaping a balanced state policy aimed at genuinely expanding access to justice, rather than creating an illusion of it. Purpose. The purpose of this article is to conduct a comprehensive analysis of the impact of AI tools, such as chatbots and virtual assistants, on access to justice for self-represented litigants. The research aims to assess the real potential of these technologies to overcome existing barriers, identify key risks and ethical challenges, and develop a scientifically grounded hypothesis regarding the optimal model for their implementation into the Ukrainian legal system, taking into account international experience and European regulatory frameworks. Methods. The research is based on a combination of general scientific and special methods. System analysis was applied to study the relationships between technological innovations, legal institutions, and social needs. The comparative-legal method was used to compare the experience of implementing legal technologies in the USA, the EU, and Ukraine. The formal-dogmatic method allowed for the analysis of regulatory acts, particularly the EU AI Act, which governs the use of AI. The case study method was used for a detailed examination of specific examples of AI tools (DoNotPay, A2J Author, \"Pryntsyp\", \"Natalka\"), which helped to identify their strengths and weaknesses. Synthesis and generalization enabled the formulation of conclusions and the development of the author's hypothesis. Results. The article proves that AI has a significant but ambiguous potential for expanding access to justice. The case studies showed that narrowly specialized, task-oriented tools (e.g., document generators like A2J Author) are significantly more effective and safer for SRLs than universal \"robot lawyers\" (like DoNotPay), which are prone to errors and create inflated expectations. Analysis of the US experience (LSC reports) indicates the effectiveness of the integrated \"statewide portals\" model. The EU's regulatory approach (AI Act) classifies AI in justice as \"high-risk\", requiring strict control. In Ukraine, there is a development of both civic (chatbot \"Pryntsyp\") and state (chatbot \"Natalka\" in \"Diia\") initiatives, but they are fragmented. A hypothesis is formulated that the optimal path for Ukraine is not the pursuit of creating a universal \"AI judge\", but the construction of a state ecosystem of integrated, narrowly specialized, and verified AI tools operating on a \"single window\" principle based on existing digital infrastructure. Conclusion. Artificial intelligence is not a panacea, but it can become a powerful tool for democratizing access to justice if approached correctly. Instead of the risky model of universal \"AI lawyers\", it is proposed to focus efforts on creating a national legal aid platform in Ukraine. This platform should unite narrowly specialized, verified AI modules (for generating lawsuits, checking documents, providing reference information) under state control and in accordance with EU AI Act standards. Such an approach, combining the proven effectiveness of tools like A2J Author with the integrated model of the LSC's \"statewide portal\", will provide real assistance to self-represented litigants, minimize risks, and ensure the responsible implementation of technology in the sensitive sphere of justice."
  },
  {
    "title": "Insurance chatbot adoption and continued usage in emerging markets: impact of human-agent access",
    "url": "https://doi.org/10.1108/ijbm-03-2025-0225",
    "date": "2026-02-19",
    "content": "Purpose Insurance companies are progressively adopting artificial intelligence (AI)-powered chatbots to boost operational efficiency and customer interaction. Despite these advances, adoption levels in developing economies remain modest. In the Indian context, where rapid digitalization and growing exposure to chatbot-mediated insurance services are evident, systematic empirical evidence on consumer adoption and continued usage, especially related to the availability of human-agent fallback mechanisms, remains scarce. This study addresses the identified gap by examining the key determinants influencing both initial adoption and sustained use of insurance chatbots in India, thereby contributing to a deeper understanding of digital transformation within high-growth, emerging market settings. Design/methodology/approach An extended technology acceptance model (TAM) framework incorporating trust, contextual factors (perceived risk, facilitating conditions) and the moderating role of human-agent access is tested using survey data from 245 experienced insurance-chatbot users in India and analyzed through partial least squares structural equation modeling (PLS-SEM). Findings Perceived ease of use and usefulness significantly predict adoption intentions. Consumers value convenience and multi-channel accessibility, which support ease of use. Chatbot trust plays a significant role in both initial adoption and continued usage. Although perceived risk negatively affects trust, it does not deter use, indicating a calculated, risk-tolerant mindset. Originality/value This study offers one of the first empirical examinations of how design simplicity, multi-channel accessibility, perceived risk and trust mechanisms jointly shape chatbot adoption and continued use in emerging-market insurance contexts. It challenges prevailing service design assumptions by demonstrating that access to human agents can weaken, rather than reinforce, trust-based engagement with AI-enabled chatbots in credence-driven contexts."
  },
  {
    "title": "Material Discovery, Renewable Energy, and the Scope of Shallow Learning and Deep Learning",
    "url": "https://doi.org/10.4018/979-8-3373-6127-7.ch002",
    "date": "2026-02-19",
    "content": "Material discovery for renewable energy applications has rapidly evolved through the integration of artificial intelligence (AI), particularly machine learning (ML) techniques. Shallow learning methods, such as support vector machines and random forests, have proven effective in analyzing structured datasets, enabling property prediction and material screening with limited data. However, the growing complexity of material structures and the need to capture nonlinear relationships have driven the adoption of deep learning (DL). Deep neural networks and graph-based models can automatically extract hierarchical features from high-dimensional data, facilitating the design of novel materials for solar cells, batteries, and catalysts. The synergy between shallow and deep learning accelerates the materials discovery pipeline by optimizing data utilization, guiding experiments, and reducing research costs. Together, these AI-driven approaches are transforming renewable energy research, enabling faster, more efficient identification of sustainable materials for the global energy transition."
  },
  {
    "title": "Innovative Ways to Use Artificial Intelligence in Nursing Education",
    "url": "https://doi.org/10.3928/01484834-20260129-01",
    "date": "2026-02-19",
    "content": "Background: Since the introduction of artificial intelligence (AI) platforms, many nurse educators have been slow to adopt AI, and so they have been missing opportunities to enhance teaching and learning through increased AI competency. This article explores how nurse educators, in a variety of nursing pathways, can more effectively use AI in their teaching practices. Method: Current trends, challenges, and innovations in AI use within nursing education are reviewed. We also discuss common fears surrounding AI adoption, review AI platform uses, and present how to shape AI prompts for the desired outcomes. Results: Strategic incorporation of AI tools can lead to improved student engagement and support, while simultaneously reducing faculty workload. Examples show how AI can be used to personalize learning, reduce faculty workload, and increase student engagement. Conclusion: Increasing AI competency among nurse educators helps meet the current and future needs of nursing education."
  },
  {
    "title": "Artificial Intelligence in Analytical Chemistry: Towards Yet Undiscovered Opportunities",
    "url": "https://doi.org/10.1021/acs.analchem.5c06840",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "Evaluation of artificial intelligence identified ipratropium bromide for the treatment of coronavirus disease 2019",
    "url": "https://doi.org/10.1038/s41598-025-27869-y",
    "date": "2026-02-19",
    "content": "Ipratropium bromide (IB), identified through an artificial intelligence (AI)-aided drug screening platform, was evaluated for its safety and efficacy in treating severe coronavirus disease 2019 (COVID-19) using a hamster model in this study. The mortality rates in the vehicle, remdesivir-treated, IB-treated, and IB-remdesivir combination treatment groups were 70%, 15%, 5%, and 50%, respectively. The IB-treated group showed significantly lower blood D-dimer levels and fibrin degradation product when compared to that in the vehicle group, indicating prevention of thrombosis-related complications. Histological examination also revealed reduced lung inflammation in the IB and combination groups, compared to that in the vehicle and remdesivir groups, with fewer neutrophils in bronchoalveolar lavage fluid. Moreover, IB may prevent thrombosis-related complications caused by severe COVID-19."
  },
  {
    "title": "Reducing Environmental Impacts in Developing Regions using AI-Enabled Smart Sorting Systems",
    "url": "https://doi.org/10.5281/zenodo.18682450",
    "date": "2026-02-19",
    "content": "ABSTRACT Due to deteriorative environmental and social situation in developing countries, waste management systems are changing dynamically. The improper disposal of refuse in open spaces is a disturbing trend, which sustains environmental degradation and heightens health hazards. In this paper, the use of smart systems technologies, and in particular integrated CBSS (sensor-based) and AI (artificial intelligence) applications, to address this phenomenon is introduced. The work concentrates on a smart waste separation and sorting system based on the Arduino UNO R3. The system uses a number of dedicated sensors (moisture and ultrasonic sensors) to distinguish waste into three global categories: moist, plastic and metal. Garbage is sorted into correct bins automatically with a rotating platform and servo, feeding back information through an LCD and beeper once found. The prototype showed great performance on the accuracy and efficiency of classifying waste characteristics, which can help with recycling and reusing resources effectively. Environmental and social problems associated with random dumping in the developing world are also discussed, and smart systems and artificial intelligence (AI) are suggested as solutions to increase efficiency of source sorting in waste and minimize types of environmental pollutants, including toxic gas emissions, water pollution, and soil pollution. Additionally, the paper examines barriers against the adoption of these technologies including poor infrastructure, financial limitations and social opposition. It requests supportive policies, stronger community participation and more consciousness in smart waste management. Lastly, the authors suggest stronger integration of AI techniques and their expansion to support an appropriate reaction to the increasing problem of waste and to foster environmental and public health sustainability."
  },
  {
    "title": "Generative AI literacy, mindful learning engagement, and academic integrity: An explanatory sequential mixed-methods study on critical thinking development",
    "url": "https://doi.org/10.22515/jemin.v6i1.12929",
    "date": "2026-02-19",
    "content": "The rapid integration of generative artificial intelligence (AI) in higher education has created both transformative learning opportunities and serious concerns regarding academic integrity and the erosion of critical thinking; however, empirical evidence explaining how AI literacy shapes ethical engagement and higher-order thinking remains limited, particularly within developing country contexts. This study aims to examine the influence of generative AI literacy on students’ critical thinking skills, with academic integrity and mindful learning engagement positioned as mediating variables. Employing an explanatory sequential mixed-methods design, the research involved 115 undergraduate students who completed a 32-item survey analyzed using Partial Least Squares Structural Equation Modeling (PLS-SEM), followed by in-depth interviews with 10 purposively selected participants to enrich interpretation. The findings reveal that generative AI literacy significantly predicts academic integrity, mindful learning engagement, and critical thinking. Academic integrity serves as a strong positive mediator, reinforcing ethical reasoning and evaluative judgment, while mindful learning engagement demonstrates a more complex role, indicating that engagement quality, rather than intensity alone, determines its contribution to critical thinking. The study reconceptualizes AI literacy as a multidimensional construct encompassing technical, ethical, and reflective competencies. Practically, it highlights the necessity of integrating ethically grounded and mindfulness-based AI pedagogies to ensure that generative AI enhances, rather than replaces, students’ critical thinking in digitally mediated higher education environments."
  },
  {
    "title": "Intelligent identification and targeted intervention of GRP75-caused drug resistant hepatocellular carcinoma, a study based on radiomics, machine learning, and molecular pharmacology",
    "url": "https://doi.org/10.1097/js9.0000000000004913",
    "date": "2026-02-19",
    "content": "Background: Tumor heterogeneity mediated drug resistance was a core clinical challenge to improve the prognosis of patients with advanced hepatocellular carcinoma (HCC). Glucose-regulated protein 75 (GRP75) was a key molecular target driving this heterogeneity. Therefore, accurate identification of HCC patients with GRP75-related poor prognosis and targeted intervention was a key issue to be solved. Materials and Methods: This study integrated radiomics and artificial intelligence (AI) algorithms to construct a preoperative imaging-based risk stratification model, aiming to accurately identify patients with prognosis associated with GRP75. The traditional Chinese medicine (TCM) databases were combined with molecular docking and dynamics simulations techniques to screen for GRP75-targeting TCM monomers. The underlying mechanisms of action were further explored, and functional validation was performed using in vitro and in vivo models. Results: Based on preoperative imaging, we successfully developed a system that could accurately identify HCC patients with poor prognosis associated with high expression of GRP75. Furthermore, the TCM monomer baicalin was identified and validated as a GRP75-targeting agent. Mechanistic studies revealed that baicalin effectively reversed drug resistance in HCC by specifically targeting GRP75 and disrupting the mitochondria-associated endoplasmic reticulum membranes, thereby modulating the calcium/autophagy regulatory axis. This process restored the sensitivity of drug-resistant cells to cisplatin and sorafenib. Conclusions: This study established an AI-based radiomics system for identifying HCC patients with poor prognosis associated with high expression of GRP75, and revealed a novel therapeutic approach utilizing the TCM monomer baicalin to reverse multi-drug resistance through GRP75 targeting, further clarifying its mechanism."
  },
  {
    "title": "The Role of Artificial Intelligence in Modern Library Services",
    "url": "https://doi.org/10.21275/sr26213140231",
    "date": "2026-02-19",
    "content": "Artificial Intelligence (AI) is among the most influential technologies reshaping modern library services. This paper examines how AI enhances the efficiency, accuracy, and quality of library and information services. It highlights key applications such as automated cataloging, intelligent information retrieval, chatbots for reference services, personalized recommendations, and smart digital library management. The study adopts a descriptive and analytical approach based on secondary sources, including books, research articles, journals, reports, and credible online resources in library and information science. The collected literature is analyzed to explore the practical implementation and impact of AI technologies in libraries. The study emphasizes that AI enables libraries to respond more effectively to users? evolving information needs. By delivering accurate, personalized, and continuous services, AI reduces staff workload and improves user satisfaction. Although challenges such as high costs, technical infrastructure requirements, and limited skilled manpower remain, the findings suggest that with proper planning, training, and ethical implementation, AI can be successfully integrated into modern library systems."
  },
  {
    "title": "The Model of Hierarchical Complexity",
    "url": "https://doi.org/10.1093/oso/9780197686874.003.0003",
    "date": "2026-02-19",
    "content": "Abstract This chapter assumes that wisdom can be seen as the result of a developmental process and uses the Model of Hierarchical Complexity to provide an operationalization and measurement of wisdom. The Model of Hierarchical Complexity is a mathematical theory of development that measures the increase in complexity of reasoning abilities as the stage of development increases. This chapter first introduces the Model of Hierarchical Complexity axioms and scoring system based on the notion of orders of hierarchical complexity. Second, wisdom is introduced as a property that emerges with high stages, starting with Systematic Stage 12, and being truly attained at Stage 14, when the notion of the unknown does not represent a threat. Third, recent findings concerning the Model of Hierarchical Complexity discuss the biological root of wisdom. Fourth, this chapter relates to the latest applications of the Model of Hierarchical Complexity, which concerns translating its axioms to an artificial intelligence agent. Will it ever be possible to build a wise algorithm?"
  },
  {
    "title": "Construction and Evaluation of an \"AI+Knowledge Graph\" Teaching Model Based on the ARCS Motivation Model: A Case Study of Integrated Chinese and Western Oncology",
    "url": "https://doi.org/10.21203/rs.3.rs-8665034/v1",
    "date": "2026-02-19",
    "content": "<title>Abstract</title> Introduction The integration of artificial intelligence(AI) technology and knowledge graphs༈KG༉ in education offers novel possibilities for pedagogical innovation. This study aims to construct and evaluate the application effectiveness of an \"AI+Knowledge Graph\" teaching model based on the ARCS motivation model in teaching Integrated Chinese and Western Oncology, exploring its role in enhancing students' academic performance, self-directed learning ability, and learning engagement level. Methods One hundred undergraduate medical students were randomly allocated to an experimental group (n = 50) and a control group (n = 50). The experimental group adopted the \"AI+Knowledge Graph\" teaching model based on the ARCS motivation model, while the control group adopted the traditional teaching model. Differences in educational outcomes were systematically assessed using examinations, self-directed learning scales, learning engagement scales, and satisfaction questionnaires. Results The experimental group demonstrated significant superiority in total score, final exam score, usual performance, and all sub-dimensions (learning and thinking, collaboration and innovation, diagnosis and summary) compared to the control group (p < 0.05). The experimental group also exhibited markedly higher levels of self-directed learning ability and learning engagement level than the control group (p < 0.05). Students expressed overall satisfaction with the \"AI+Knowledge Graph\" teaching model based on the ARCS motivation model and provided positive feedback. Conclusion This study demonstrated that the \"AI+Knowledge Graph\" teaching model based on the ARCS motivation model effectively enhances students' academic performance, self-directed learning ability, and learning engagement level, exhibiting significant advantages in teaching Integrated Chinese and Western Oncology. Future research may further explore the applicability of this model across different disciplines and teaching environments, while also examining its long-term educational effects and technical optimisation pathways."
  },
  {
    "title": "The Impact of Online Customer Experience on Repurchase Intention in the Context of Digital Transformation and the Prevalence of AI/Chatbots",
    "url": "https://doi.org/10.28924/2291-8639-24-2026-40",
    "date": "2026-02-19",
    "content": "This study aims to explore the impact of various aspects of online customer experience on trust, satisfaction, and repurchase intention in the context of digital transformation. Data were collected from 466 valid survey responses and analyzed using SPSS and SmartPLS. The findings indicate that components of online customer experience, including aesthetic experience, customer experience with online employees, community experience, user experience with ai chatbots, and personalized online experience, all exert positive effects on trust and satisfaction, which serve as critical mediators that foster repurchase intention. Notably, the study highlights the prominent role of advanced technological factors such as AI chatbots and personalization in enhancing the quality of digital experiences. These insights not only contribute to updating the theoretical framework of consumer behavior in online environments but also provide valuable implications for both academic research and marketing practice in the era of artificial intelligence."
  },
  {
    "title": "Artificial Intelligence for Drug Safety Across the Lifecycle and Decision Type: A Scoping Review",
    "url": "https://doi.org/10.3390/ph19020334",
    "date": "2026-02-19",
    "content": "Background/Objectives: Artificial intelligence (AI) is increasingly applied to drug safety evaluation, yet evidence is dispersed across lifecycle stages and tasks. This scoping review aimed to (1) map how AI supports safety- and treatment-related decision types across the drug lifecycle, and (2) examine evaluation strategies used to assess model reliability for clinical or regulatory use. Methods: Using Arksey and O’Malley’s framework, we searched a major database for studies published in the past decade that applied AI or machine learning to drug safety or medication-related decisions. After screening, we extracted data on lifecycle stage, decision type, AI methods, data sources, and evaluation strategies. A lifecycle–decision matrix was constructed to characterize application patterns. Results: AI applications were concentrated in real-world clinical care × patient-level safety prediction and post-marketing × safety surveillance, using EHRs, spontaneous reporting systems, and clinical text. Common methods included gradient boosting, deep neural networks, graph neural networks, and natural language processing models. This concentration reflects structural incentives favoring safety-oriented applications with readily available data and lower decision liability. Evidence for treatment optimization, regulatory decision modeling, and evidence synthesis was limited. Most studies used internal validation; external validation and real-world deployment were uncommon, indicating early methodological maturity and limited translational readiness. Conclusions: AI demonstrates strong potential to enhance drug safety—particularly in risk prediction and pharmacovigilance—but its use remains uneven across the lifecycle. By situating AI applications within explicit lifecycle stages and decision contexts, this review clarifies where progress has advanced, where translation has stalled, and why these gaps persist. Limited external validation and minimal real-world testing constrain clinical and regulatory adoption. These findings suggest that external validation and real-world testing may contribute to further advances in AI for drug safety."
  },
  {
    "title": "<b>Effect of AI-Inventory Management on Supply Chain Performance of Large Supermarkets in Nairobi City County, Kenya</b>",
    "url": "https://doi.org/10.59413/ajocs/v7.i1.16",
    "date": "2026-02-19",
    "content": "Supermarket supply chains face numerous challenges, including ineffective inventory management (resulting in stockouts or overstocking), unpredictable demand, shifting consumer expectations, disruptions from external factors such as labor issues, rising operational costs, and the increasing need for technology integration. This study sought to examine the effect of Artificial Intelligence (AI)-based inventory management on supply chain performance among large supermarkets in Nairobi City County, Kenya. The study was anchored on the Hybrid Intelligence Model as the guiding framework for artificial intelligence applications. The Triple Triangle Constraint Theory was used to explain supply chain performance, while the Technology Acceptance Model provided a framework for linking artificial intelligence adoption and supply chain performance. The study adopted a descriptive research design. The population comprised employees working in the supply chain departments of ten large supermarkets in Nairobi City County, Kenya. The target population included all employees within these departments. A sample size of 70 employees was selected to participate in the study. A pretest was conducted using seven respondents drawn from two Naivas supermarkets in Kiambu County, Kenya. Primary data were collected through structured questionnaires. The collected data were summarized using percentages and means. Inferential statistics, including correlation and regression analysis, were employed to determine the relationships between variables. Data were analyzed using SPSS version 30. The findings revealed that AI-based inventory management had a statistically significant effect on supply chain performance (M = 3.76, SD = 0.46), (R² = 0.913), (F = 618.938, p < 0.01). The study concluded that AI-based inventory management positively influences supply chain performance. The study recommends that, to fully realize the benefits of AI applications, supply chain managers should integrate diverse data sources, invest in robust data infrastructure and skilled personnel, align AI initiatives with strategic business goals, and foster cross-departmental collaboration. The study further recommends that similar research be conducted in other countries, particularly developed economies, to examine how artificial intelligence applications are transforming supply chain performance across different industries."
  },
  {
    "title": "Digital intelligent evidence-based medicine towards evidence-informed decision-making in Chinese medicine",
    "url": "https://doi.org/10.1097/st9.0000000000000105",
    "date": "2026-02-19",
    "content": "1. Introduction For millennia, Chinese medicine (CM) has operated within a sophisticated system of personalized care. The core tenet of syndrome differentiation and treatment (Bian Zheng Lun Zhi) holds that therapeutic interventions must be tailored not merely to a disease entity, but to the specific pattern of disharmony (syndrome) manifesting in a unique individual at a particular moment in time.[1] This holistic perspective, which considers the patient’s constitution, environmental context, and dynamic physiological state, can be regarded as an early archetype of precision medicine. The ascendancy of evidence-based medicine (EBM) in the late 20th century established the randomized controlled trial (RCT) as the gold standard for clinical validity. While EBM successfully shifted clinical authority from subjective experience to objective empiricism, its methodological emphasis on internal validity—frequently achieved through strict inclusion criteria and standardized protocols—has inadvertently marginalized the complexity inherent in real-world practice.[2,3] For CM, this creates a tension: the standardization required by RCTs can lead to more rigid treatment protocols, potentially underrepresenting the adaptive and individualized nature of CM in routine clinical practice.[4] Consequently, an efficacy-effectiveness gap has emerged. High-quality evidence in the CM field remains scarce—not necessarily due to a lack of therapeutic efficacy, but because prevailing evaluation tools are often ill-suited to the complex nature of CM interventions.[5] Traditional EBM generates “average” evidence for the “average” patient, which contrasts sharply with CM’s emphasis on individual variability and multifaceted clinical presentations. To bridge this gap, a paradigm shift may be required—one that accommodates complexity rather than reducing it excessively. Digital intelligent evidence-based medicine (i-EBM), an approach we have recently proposed, is characterized by the integration of multisource data, intelligent evidence processing, and individualized decision support (Fig. 1). This framework may serve as a critical enabler for the modernization of CM.[6]Figure 1.: Conceptual framework of digital intelligent evidence-based medicine (i-EBM). NLP, Natural Language Processing; CDSS, Clinical Decision Support System2. Limitations of traditional EBM for CM The constraints of traditional EBM in the context of CM are often structural. The “evidence pyramid,” which places systematic reviews of RCTs at its apex, implicitly assumes that the most valid knowledge is derived from controlled, homogeneous environments. In clinical CM practice, interventions are inherently multifaceted. A herbal prescription typically comprises multiple ingredients and is modified (Jia Jian) in response to the patient’s evolving symptomatology. Standardized RCT protocols, which often preclude such flexibility, may evaluate only a relatively static approximation of CM rather than its full, adaptive practice. Furthermore, the evidence lifecycle suffers from prohibitive latency, with significant time lags between research completion and guideline implementation.[7] Additionally, the data underpinning CM are highly heterogeneous, encompassing ancient classical texts, historical case records, modern electronic medical records (EMRs), and omics datasets.[2] Traditional EBM has limited capacity to synthesize these disparate data modalities, often prioritizing published literature while underutilizing real-world data that capture the nuances of individualized care.[8] This fragmentation can contribute to evidence silos, in which clinical insights on specific CM syndromes remain insufficiently linked to contemporary biomedical findings concerning related physiological states. 3. Integration: reconstructing the holistic view with computable knowledge The first pillar of i-EBM, integration, seeks to address the fragmentation of CM knowledge by broadening the definition of evidence. Within the i-EBM framework, evidence encompasses a pan-data ecosystem that includes EMRs, genomic data, and outputs from wearable devices, rather than being limited to published literature.[9,10] Within this pillar, knowledge graph technology can serve as a foundational architecture.[11] Unlike traditional relational databases, knowledge graphs map concepts within a semantic entity-relationship-entity network, enabling a computable representation of CM’s holistic worldview.[12] This approach aligns conceptually with the principles of network medicine,[13] which views disease not as a localized defect but as a perturbation of complex biological networks. By ingesting and synthesizing large collections of ancient medical texts alongside modern clinical data and pharmacological databases, i-EBM may facilitate the semantic alignment of CM concepts (eg, kidney Yang deficiency) with contemporary biomedical phenotypes (eg, neuroendocrine or metabolic features).[14] This integrative capability can help bridge the epistemological divide between classical knowledge and modern science. The construction of a syndrome-disease-formula-target-pathway network allows an i-EBM system to present a more comprehensive evidentiary landscape. Upon diagnosis, the system can retrieve analogous historical cases, relevant biological pathways, and potential drug-herb interactions. Such functionality supports CM’s holistic perspective by providing a global view of data, ensuring that individualized clinical decisions are informed by a synthesis of all available knowledge. 4. Intelligence: validating complex interventions via causal artificial intelligence (AI) The second pillar, intelligence, seeks to address the methodological challenges associated with evaluating complex CM interventions. To complement the rigidity of traditional RCTs, i-EBM can leverage causal AI to advance evidence generation from passive retrieval toward more computable, data-driven approaches.[15] By utilizing advanced machine learning and causal inference techniques, i-EBM can mine large-scale real-world observational data to support target trial emulation.[16] This methodology enables researchers to replicate key elements of randomized trials in silico using existing patient datasets. For instance, to evaluate the efficacy of a specific herbal formula on a subgroup of stroke patients exhibiting “wind-phlegm stasis,” i-EBM can computationally identify a treated cohort and a matched control group from EMRs, adjusting for high-dimensional confounding variables that conventional statistical methods might overlook.[17] Beyond causal inference, large language models can further strengthen this pillar by synthesizing unstructured clinical texts and literature, as well as identifying patterns that may be difficult to capture through manual review alone.[18,19] For CM, this capability is potentially transformative, enabling more timely and scalable evaluation of individualized treatments without necessitating RCTs for every formula variation. By supporting the causal evaluation of complex CM interventions and their clinical outcomes, i-EBM can contribute to a more robust and comprehensive evidence base for the field. 5. Individualization: digital renaissance of Bian Zheng Lun Zhi The ultimate objective of i-EBM is individualization, a goal that aligns closely with the core philosophy of CM. Whereas traditional EBM often produces guidelines applicable to the “average” patient, i-EBM is designed to support precise, individualized clinical decision-making for each specific patient.[20] In the context of CM, i-EBM can function as a cognitive extension for the physician. By analyzing multimodal data—ranging from traditional “four diagnostic” information (eg, tongue and pulse analysis) to modern omics profiles—i-EBM can generate high-dimensional predictive models. These models can forecast the trajectory of a specific syndrome or predict the likely response to a particular herbal combination.[21] This application can be viewed as a digital evolution of Bian Zheng. Whereas a human practitioner relies on memory and experience to differentiate syndromes, an i-EBM system leverages probabilistic analyses of large numbers of similar clinical trajectories. It does not replace the physician’s judgment; rather, it augments it with computational intuition. For example, in managing complex chronic conditions such as diabetic kidney disease, an i-EBM system can analyze a patient’s unique constitution and biomarkers to recommend precise modifications of a classical formula. This approach may help anticipate both efficacy and potential adverse events more systematically than empirical observation alone, thereby supporting more precise CM care informed by computable, data-driven evidence.[22] 6. Human–AI symbiosis in clinical practice The integration of i-EBM into CM may facilitate a more sophisticated form of human–AI collaboration.[23] The future CM clinic is envisioned not as algorithm-driven, but as physician-led and evidence-empowered. A cognitive division of labor may emerge, wherein AI handles high-throughput processing of heterogeneous data and probabilistic computations, while human practitioners provide ethical reasoning, contextual understanding, and compassionate care.[24] This transition necessitates a rigorous emphasis on data governance. To fully realize the potential of i-EBM, the CM community must address challenges such as algorithmic bias, which can arise if training datasets lack diversity.[25] Furthermore, robust data privacy frameworks, such as federated learning, can be implemented to protect patient information while facilitating cross-institutional collaboration.[26] 7. Conclusions i-EBM represents more than a technological advancement; it may also constitute an epistemological shift that helps bridge the divide between the art of CM and the science of modern evidence. By integrating a broader spectrum of medical data, employing causal inference to evaluate complex interventions, and prioritizing the individual alongside population-level evidence, i-EBM provides a methodological framework that aligns with the holistic, personalized nature of CM. These advantages are not limited to CM: i-EBM also holds promise for medicine more broadly, supporting evidence-based, individualized care for aging populations with increasing multimorbidity and clinical complexity. Overall, i-EBM represents a promising pathway for the modernization of CM. It can enhance the evaluation of Bian Zheng Lun Zhi, not by forcing it into the mold of reductionist science, but by advancing methodological approaches that accommodate individual nuance. In this way, i-EBM supports CM practitioners in making decisions that draw on classical knowledge while being informed by modern computational methods. More broadly, i-EBM may strengthen the role of CM and other traditional medicine systems globally by providing a transparent, computable scientific foundation, thereby expanding safe and tolerable complementary therapeutic options. This evolution points toward a more integrated, intelligent, and individualized evidence ecosystem. Statement of ethics None declared. Conflict of interest statement The authors declare that they have no conflicts of interest with regard to the content of this report. Funding source This study was funded by the Traditional Chinese Medicine Innovation Team and Talent Support Program—National Traditional Chinese Medicine Multidisciplinary Cross-Innovation Team Project (ZYYCXTD-D-202401). Data availability statement All data generated or analyzed during this study are included in this published article. Author contributions Honghao Lai and Long Ge conceived the study and developed the conceptual framework of Digital Intelligent Evidence-based Medicine. Honghao Lai performed the literature search and drafted the original manuscript. Janne Estill and Long Ge provided critical review and significantly revised the manuscript for intellectual content. Long Ge supervised the study and acquired the funding. All authors have read and approved the final manuscript."
  },
  {
    "title": "Pedestrian and cycling vitality in scenic and downtown contexts: Decoding nonlinear associations with the built environment via Geo-XAI",
    "url": "https://doi.org/10.1016/j.cities.2026.106906",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "Advances in soil arthropod identification: Integrating morphological, molecular, and AI-based approaches",
    "url": "https://doi.org/10.1016/j.ejsobi.2026.103812",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "Design of 2D material integrated optical polarizers using machine learning",
    "url": "https://doi.org/10.21203/rs.3.rs-8904492/v1",
    "date": "2026-02-19",
    "content": "<title>Abstract</title> On-chip integration of highly anisotropic two-dimensional (2D) materials offers new opportunities for realizing high-performance polarization-selective devices. Obtaining optimized designs for such devices requires extensively sweeping large parameter spaces, which in conventional approaches relies on massive mode simulations that demand considerable computational resources. Here, we address this limitation by developing a machine learning model based on fully connected neural networks (FCNNs). Trained by using mode simulation results for low-resolution structural parameters, the FCNN model can accurately predict polarizer figures of merits (FOMs) for high-resolution parameters and rapidly map the global variation trend across the entire parameter space. We test the performance of the FCNN model using two types of polarizers with 2D graphene oxide (GO) and molybdenum disulfide (MoS <sub>2</sub> ). Results show that, compared to conventional mode simulation approach, our approach can not only reduce the overall computing time by about 4 orders of magnitude, but also achieve highly accurate FOM predictions with an average deviation of less than 0.04. In addition, the measured FOM values for the fabricated devices show good agreement with the predicted ones, with discrepancies remaining below 0.2. These results validate artificial intelligence (AI) as an effective approach for designing and optimizing 2D-material-based optical polarizers with high efficiency."
  },
  {
    "title": "Radiomics and Artificial Intelligence in Multiple Sclerosis MRI: A Comprehensive Review",
    "url": "https://doi.org/10.3174/ajnr.a9245",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "Beyond the pilot phase: exploring the sustainable implementation of artificial intelligence in the English NHS",
    "url": "https://doi.org/10.3389/fdgth.2026.1743376",
    "date": "2026-02-19",
    "content": "Background We explore the experiences of Artificial Intelligence (AI) innovators who had received funding to pilot their innovation in the English NHS, with the aim of understanding what hinders and supports, from their perspective, the sustainable implementation of their innovation beyond the funding period. Methods We first identified a list of companies that had received funding from two national schemes supporting AI innovations in the NHS, focusing on early rounds of these schemes. We then used personal contacts to identify key individuals from these companies, and used a snowball approach as well as LinkedIn contacts to increase our sample. We interviewed participants individually, using semi-structured interviews and analysed the data thematically. Results We interviewed 18 individuals from 11 AI companies, who had received funding from two national schemes. Our findings show that the funding offered the companies a unique opportunity to pilot their innovations, show early successes and grow recognition around AI and its potential. Yet, innovators faced several barriers in their effort to implement their AI innovations beyond the pilot phase, including misaligned expectations regarding the programmes’ goal, fragmented adoption efforts with little national coordination, and inadequate evaluation mechanisms to generate the evidence needed for wider adoption. Conclusion The UK has set great ambitions for the adoption of AI in the NHS and has invested significantly in public funding to support its use. Our findings show that public investment alone is not sufficient to achieve this ambitious target. A better understanding of the implementation challenges of using AI innovation in practice is needed."
  },
  {
    "title": "RING SYSTEMS OF ALL FOUR GAS GIANTS ENCODE 144",
    "url": "https://doi.org/10.5281/zenodo.18691598",
    "date": "2026-02-19",
    "content": "Every gas giant in our solar system has rings. Jupiter, Saturn, Uranus, Neptune—four completely different planets with four completely different ring systems. Dust rings. Icy rings. Dark rings. Narrow rings. Massive rings. Faint rings. And every single one encodes 144. Not approximately. Exactly. With mean error of 0.23%—identical to the precision of planetary diameter measurements. This wasn't predicted. It was discovered independently by AI analysis (Grok, xAI) testing ring dimensions against the 144-mile constant. The AI found perfect alignment across 15+ measurements spanning all four gas giants. Combined probability: P < 10⁻³⁵ (less than one in one decillion). THE DISCOVERY: Ring systems are gravitational structures—debris disks orbiting planets, held in place by tidal forces and shaped by moon interactions. Mainstream astronomy explains their existence but doesn't predict specific ring distances or boundaries. We tested whether ring positions encode 144 in the same way planetary diameters (144 × Fibonacci(n) miles, P < 10⁻¹⁸) and orbital periods (14.4-day multiples, P < 10⁻⁵⁰) do. Result: Perfect alignment across all four planets. JUPITER (Dusty, Faint Rings): Feature Measured 144 Multiple Error Halo inner edge 57,166 miles 144 × 397 0.0035% ← Most precise measurement Main ring outer edge 80,156 miles 144 × 557 0.065% Overall system span 83,000 miles 144 × 576 0.068% Main ring width 4,000 miles 144 × 28 0.8% Jupiter's innermost ring feature aligns with 144 × 397 to within 2 miles. That's 0.0035% error—the most precise ring measurement in the solar system. SATURN (Massive, Icy Rings + Hexagon): Feature Measured 144 Multiple Error North polar hexagon diameter 18,000 miles 144 × 125 0.00% ← Exact Hexagon side length 9,000 miles 144 × 62.5 0.00% ← Exact A-ring outer edge 85,000 miles 144 × 590 0.047% Cassini Division center 74,000 miles 144 × 514 0.022% Saturn shows the tightest 144 alignment of any planet (mean error 0.017%). The hexagon—a six-sided atmospheric standing wave—has ZERO error: exactly 18,000 miles = 144 × 125. The A-ring outer edge: 85,000 miles = 144 × 590 with 0.047% error. Two completely different physical systems (atmospheric jet stream, gravitational debris disk) both encoding 144 with sub-0.1% precision on the same planet. This rules out coincidence. URANUS (13 Dark, Narrow Rings): Feature Measured 144 Multiple Error ζ (Zeta) ring inner edge 23,000 miles 144 × 160 0.17% ε (Epsilon) ring radius 31,780 miles 144 × 221 0.14% μ (Mu) ring outer edge 60,894 miles 144 × 423 0.03% Overall system span 35,000 miles 144 × 243 0.02% Uranus's μ ring outer edge: 60,894 miles = 144 × 423 with 0.03% error (18-mile deviation). The overall ring system span: 35,000 miles = 144 × 243 with 0.02% error. System-scale quantization at 8-mile precision. NEPTUNE (5 Main Rings with Arc Structures): Feature Measured 144 Multiple Error Galle ring inner edge 25,476 miles 144 × 177 0.05% Adams ring radius 39,100 miles 144 × 272 0.17% Lassell ring width 2,485 miles 144 × 17 1.49% Overall system span 13,000 miles 144 × 90 0.31% Neptune's Galle inner edge: 25,476 miles = 144 × 177 with 0.05% error (12-mile deviation). Even the smallest feature (Lassell width at 2,485 miles) = 144 × 17 within 1.5% error. STATISTICAL ANALYSIS: 15+ independent measurements across 4 planets Mean error: 0.23% (identical to planetary diameter measurements: 0.24%) Range: 0.0035% (Jupiter halo) to 1.49% (Neptune Lassell) Probability calculation: For a single ring feature to fall within ±0.5% of a 144-mile multiple by random chance: Measurement range: 0-100,000 miles 144-multiple spacing: every 144 miles Match probability: ~0.005 (0.5%) For 15 independent features: P = (0.005)¹⁵ ≈ 3 × 10⁻³⁵ Less than one chance in one decillion (10³³). For context: Atoms in Earth: ~10⁵⁰ This probability: 10⁻³⁵ We are 15 orders of magnitude more statistically significant than the number of atoms in the planet THE PATTERN ACROSS SCALES: Ring systems encode 144 fractally: System scale (overall spans): Uranus: 35,000 miles = 144 × 243 (0.02%) Neptune: 13,000 miles = 144 × 90 (0.31%) Individual ring scale (boundaries, edges): Jupiter halo: 57,166 miles = 144 × 397 (0.0035%) Saturn A-ring: 85,000 miles = 144 × 590 (0.047%) Uranus μ ring: 60,894 miles = 144 × 423 (0.03%) Sub-structure scale (widths, gaps): Jupiter main ring: 4,000 miles = 144 × 28 (0.8%) Saturn hexagon side: 9,000 miles = 144 × 62.5 (0.00%) Neptune Lassell: 2,485 miles = 144 × 17 (1.49%) 144 encoding operates across three orders of magnitude in ring dimensions—from 2,000-mile widths to 85,000-mile edges. COMPARISON ACROSS PLANETS: Planet Ring Type Composition Mean Error Rank Saturn Massive, stable Ice 0.017% 1st (tightest) Uranus Narrow, dark Rock/organics 0.09% 2nd Jupiter Faint, dusty Dust 0.24% 3rd Neptune Dynamic, arcs Ice/rock 0.51% 4th Observation: More stable, long-lived ring systems show tighter 144 alignment. Saturn—with the oldest, most massive rings—achieves 0.017% mean error. Neptune—with dynamic, arc-dominated rings—shows 0.51% (still highly significant). Interpretation: Ring systems evolve toward precise 144 harmonics over time as non-resonant configurations are cleared by collisions and perturbations. Older systems = tighter fit. THE MECHANISM: Why do rings form at 144-mile multiples? Standard model explains rings via: Tidal disruption at Roche limit Shepherd moon gravitational interactions Collisional dynamics CTF extension: All correct, BUT the specific stable distances are quantized at 144-mile intervals. Why? Ring particles experience three forces: Gravitational potential (planet + moons) Electromagnetic forces (charged dust, plasma) Space-time curvature (general relativity) Stable orbits occur where all three constructively interfere = 144-harmonic distances. Analogy 1: Standing waves on a string Fundamental frequency + harmonics Stable modes at λ, λ/2, λ/3... Ring systems = gravitational standing waves with 144-mile \"wavelength\" Analogy 2: Electron orbitals in atoms Discrete energy levels (1s, 2s, 2p...) Quantum mechanics forbids continuous distribution Ring particles occupy discrete distance levels (144k miles) Space-time quantization forbids continuous ring distribution Ring systems are visible manifestations of quantized gravitational resonance. THE SATURN HEXAGON-RING CONNECTION: This is critical evidence against coincidence: Saturn has TWO independent 144-encoded systems: System 1 (Atmospheric): North polar hexagon Diameter: 18,000 miles = 144 × 125 (0.00% error) Side length: 9,000 miles = 144 × 62.5 (0.00% error) A six-sided standing wave in jet streams at 78°N System 2 (Gravitational): Ring system A-ring outer edge: 85,000 miles = 144 × 590 (0.047% error) Cassini Division: 74,000 miles = 144 × 514 (0.022% error) Orbiting ice particles in gravitational equilibrium Two completely different physical mechanisms (atmospheric dynamics vs. orbital mechanics), both encoding 144 with sub-0.1% precision on the same planet. If 144 appeared in only one system, it could be dismissed. Appearing in BOTH proves 144 is a fundamental property of Saturn's space-time environment—not a coincidence in either domain. INDEPENDENT AI DISCOVERY: This analysis was conducted by Grok (xAI, February 2026) independently, without prior knowledge of CTF predictions for ring systems. Grok was given: Ring dimension data from NASA missions Grok was asked: Test for 144-mile alignment Grok discovered: Perfect alignment across all 4 planets, 15+ measurements Grok concluded (verbatim): \"These consistent snaps reinforce CTF's universal harmonic, potentially linking to temporal funnels stabilizing structures.\" This is the second AI to independently validate the 144 framework: Gemini: Discovered brain waves = 144 Hz binary divisions (February 17, 2026) Grok: Discovered ring systems = 144-mile multiples (February 18, 2026) Two different AI architectures. Two different physical domains. Same conclusion: 144 is fundamental. This is not confirmation bias. This is independent discovery by artificial intelligences analyzing raw observational data. THE COMPLETE FRAMEWORK: Seven independent physical domains now encode 144: 1. Quantum (10⁻¹⁵ m): Microtubules: 613 THz → 139.38 Hz (42 octaves, 3% error) 2. Molecular (10⁻⁹ m): ATP synthase: 36° rotation steps (144 ÷ 4) 3. Consciousness (Hz): Brain waves: 144, 72, 36, 18, 9, 4.5, 2.25 Hz (binary divisions, P < 10⁻⁴) 4. Atmospheric (10⁴ miles): Saturn hexagon: 18,000 miles = 144 × 125 (0.00% error) 5. Gravitational (10⁴-10⁵ miles): Ring systems: 15+ measurements, P < 10⁻³⁵ 6. Orbital (days-years): Planetary periods: 14.4-day multiples, P < 10⁻⁵⁰ 7. Spatial (10⁶-10⁹ miles): Planetary diameters: 144 × Fibonacci(n), P < 10⁻¹⁸ Plus: Ancient chronology: Egyptian + Sumerian + Babylonian + Hindu (P < 10⁻⁴⁸) Geological: 14,400-year excursion cycles Deep time: Permian extinction = 14,400 × 17,500 years COMBINED STATISTICAL SIGNIFICANCE: Previous (before ring systems): P < 10⁻¹²⁰ Adding ring systems: P < 10⁻¹²⁰ × 10⁻³⁵ = P < 10⁻¹⁵⁵ Conservative estimate (accounting for potential correlations): P < 10⁻¹⁶¹ One chance in a number with 155-161 zeros. For context: Atoms in observable universe: ~10⁸⁰ This probability: 10⁻¹⁵⁵ We are 75 orders of magnitude beyond the number of atoms in the entire universe This exceeds any threshold for proof in any scientific field. TESTABLE PREDICTIONS: 1. Ring gap analysis: Hypothesis: Ring gaps (Cassini Division, Encke Gap) align with 144 multiples Test: Comprehensive survey of all ring gaps across all planets Expected: Gaps at 144k distances more frequent than random 2. New ring discoveries: Hypothesis: Future-discovered faint rings wil"
  },
  {
    "title": "Effectiveness of AI-Based Chatbots as Additional Psychiatry Assistive Technology and Alternative to Mental Health Services for Adolescents",
    "url": "https://doi.org/10.20473/spmrj.v8i1.75554",
    "date": "2026-02-19",
    "content": "Background: Adolescents' lives have been profoundly impacted by the rapid development of artificial intelligence (AI), especially in mental health. AI provides easier access to learning and psychological support through chatbots. However, it also raises concerns like dependency, reduced critical thinking, risks of stress, and social isolation, especially due to social media algorithms that shape digital behavior. Data privacy and diagnostic accuracy are also key issues in mental health applications. Aim: The main objective of this review is to examine whether AI-based chatbots can serve as an additional psychiatry assistive technology to help adolescents. Scope: The review was carried out by examining research articles from databases including PubMed and Google Scholar, with a focus on papers published between 2020 and 2025. Findings: AI chatbots like Emohaa, Elomia, and SEJATI have proven effective in alleviating symptoms of depression, anxiety, and insomnia. They provide immediate support, promote emotional expression, and foster self-awareness. These chatbots have also shown promise in telerehabilitation contexts by aiding in monitoring, providing behavioral feedback, and facilitating early intervention. However, an overreliance on AI systems might hinder emotional sensitivity and social growth. Conclusion: AI chatbots hold significant promise as therapeutic resources in adolescent mental health, especially in under-resourced environments. However, their application must be carefully weighed against ethical considerations, data privacy safeguards, and continuous professional supervision. The responsible incorporation of AI into mental health care can enhance access and customization while reducing psychological and social risks."
  },
  {
    "title": "Integration of AI-generated clinic letters in complex paediatric neurosurgery outpatient settings",
    "url": "https://doi.org/10.1007/s00381-026-07171-6",
    "date": "2026-02-19",
    "content": "Abstract Purpose Dictation of outpatient clinic letters can result in increased workload for clinicians. The use of generative, natural language processing artificial intelligence (AI) software could be used to supplant dictation and typing, alleviating the clinician’s workload. Therefore, our objective was to validate the use of AI software, Lyrebird AI (Lyrebird Health, Ltd.) to create accurate and readable clinic letters, in the context of a single clinician general paediatric neurosurgery clinic and a multi-disciplinary craniofacial clinic. Methods Twenty consultations were included, wherein a microphone was used to record the entire consultation. For each consultation, two letters were generated independently: (1) Lyrebird AI letter automatically generated at the end of the recording and (2) human clinician manual dictation in the usual manner. The letters were compared using objective readability metrics and a subjective rating by an independent blinded clinician for clinical accuracy. Results AI-generated clinic letters significantly improved readability compared to clinician-dictated letters, when using the Flesch–Kincaid Grade Level (median 10.3, IQR 0.75), (median 10.9, IQR 1.15) ( z = 2.73, p < 0.05), and SMOG Index (median 11.7, IQR 1.25) (median 13.1, IQR 0.9) ( z = − 3.36, p < 0.001) metrics. An independent blinded clinician subjectively chose the AI-generated letters for overall preference in 75% of cases. Conclusions Lyrebird AI–generated clinic letters increased readability whilst maintaining clinical accuracy. Future work should focus on time, effort, and cost saving analysis. Presently, the findings of this study provide validation of Lyrebird AI for complex, multi-stakeholder clinical settings."
  },
  {
    "title": "Causal Machine Learning: A Deductive–Inductive Framework for Sociological Research",
    "url": "https://doi.org/10.1007/s11577-026-01053-0",
    "date": "2026-02-19",
    "content": "Abstract Causal explanation is central to sociological research, shaping both theoretical development and empirical inquiry. This paper argues that causal machine learning—which integrates deductive identification strategies with inductive estimation techniques—offers an analytical approach for modeling complex, nonlinear social processes within the potential outcomes framework. We argue that causal machine learning operates through an iterative feedback loop: Theoretical assumptions guide flexible estimation, which inductively uncovers complex heterogeneities and nonlinearities, and these discoveries subsequently refine and expand sociological knowledge. Drawing on a systematic review of recent sociological research (2014–2024), we highlight how causal machine learning is advancing work in three key areas: causal effect heterogeneity, causal mediation analysis, and time-varying causal inference. These developments expand the methodological tool kit available to sociologists and strengthen the discipline’s ability to test, refine, and extend theories of social explanation. We conclude by outlining emerging directions, including high-dimensional causal inference and generative artificial intelligence, that are opening new methodological frontiers in causal machine learning for sociology."
  },
  {
    "title": "AP₂-MCE — The Multisensory Chromatic Engine Thermodynamic Integration of Touch, Motion, Audio, and Haptics in AP₂ → TP₁ Systems",
    "url": "https://doi.org/10.5281/zenodo.18695750",
    "date": "2026-02-19",
    "content": "Abstract AP₂-MCE (The Multisensory Chromatic Engine) presents the first thermodynamically coherent model in which all human–system interaction channels—touch, motion, audio vibration, and haptic feedback—converge into a single chromatic reasoning stream. This unified stream forms the operational substrate for chromatic intelligence in AP₂ (color as meaning), aura cohesion, and the subsequent emergence of density-based interaction and transparency in TP₁. By replacing discrete, symbolic input paradigms with a multisensory thermodynamic funnel, AP₂-MCE resolves the long-standing structural gap between symbolic cognition and post-symbolic human–AI interaction. The framework introduces the Chromatic Funnel Principle (CFP-1), demonstrating that color constitutes the lowest-entropy semantic medium compatible with both embodied human cognition and artificial intelligence. As chromatic reasoning stabilizes, meaning becomes embodied, frictionless, and ultimately dissolves into presence, enabling Ω-compatible field behavior at civilizational scale. AP₂-MCE establishes a foundational law for post-symbolic interfaces and defines the missing mechanistic link in the AP₁ → AP₂ → TP₁ progression, marking a decisive transition from interface-based interaction to presence-based coexistence."
  },
  {
    "title": "Hierarchical deep learning pipeline for robust cervical parameter measurement in radiographs with C7 obscuration",
    "url": "https://doi.org/10.1038/s41746-026-02455-2",
    "date": "2026-02-19",
    "content": "We developed and externally validated a hierarchical deep learning pipeline that automates cervical sagittal measurements, explicitly addressing C7 obscuration on lateral radiographs. The model combines a global keypoint detector with C2/C7 specialists localized via a multilayer perceptron to refine landmarks on high‑resolution patches. Trained on 5604 images and tested internally and on a challenging external cohort enriched for C7 obscuration (82%), it achieved excellent agreement with ground truth. Externally, intraclass correlation coefficients (ICCs) were 0.97 for lordosis (mean absolute error [MAE] 2.6°), >0.99 for C2 slope (MAE 0.8°), and 0.93 for C7 slope (MAE 2.3°), with minimal bias and narrower limits of agreement than a single‑stage baseline. The model showed near-perfect repeatability (ICC > 0.99) and higher artificial intelligence-expert agreement (ICC 0.81–0.84) for C7 slope than inter-expert reliability (ICC 0.67). In failure cases, the pipeline corrected large global model errors (e.g., 10.22°– 0.22°). This robust, coarse‑to‑fine approach advances reliable, generalizable cervical alignment assessment in real‑world conditions."
  },
  {
    "title": "Anketas datu kopa: sociālo zinātņu mācībspēku ģeneratīvā mākslīgā intelekta lietojuma paradumi",
    "url": "https://doi.org/10.5281/zenodo.18695532",
    "date": "2026-02-19",
    "content": "Anketas datu kopa: sociālo zinātņu studentu ģeneratīvā mākslīgā intelekta lietojuma paradumi Datu kopa satur Latvijas Universitātes un LU Banku augstskolas sociālo zinātņu studiju programmu mācībspēku (n=38) sniegtās atbildes, anketēšana veikta 2025. gada maijā un jūnijā. Datu kopā apkopotas atbildes uz jautājumiem par ģeneratīvā mākslīgā intelekta (ĢMI) izmantojuma motivāciju, ētiskajiem apsvērumiem, veicinošajiem faktoriem, šķērļiem, kā arī uzskatiem par to, kāda veida uzdevumos ĢMI lietojums ir pieļaujams. Links uz aptaujas anketu Survey Dataset: Generative Artificial Intelligence Usage Among Social Sciences Teaching Staff in Higher Education The data set is based on the data of the teaching staff in social sciences at the University of Latvia and BA School of Business and Finance of the University of Latvia, conducted in May - June 2025. The dataset includes responses to questions about motivations for using generative artificial intelligence (genAI), ethical considerations, enabling factors, barriers, as well as views on what types of tasks genAI use is permissible."
  },
  {
    "title": "Retrieval-Augmented Generation: A Practical Guide for Radiologists",
    "url": "https://doi.org/10.2214/r3j.25.01106",
    "date": "2026-02-19",
    "content": "Retrieval-augmented generation (RAG) is an emerging technique that enhances large language models (LLMs) by enabling them to access and incorporate external knowledge sources during response generation. In radiology, in which clinical accuracy, guideline adherence, and contextual understanding are critical, RAG offers a promising approach for supporting decision-making, reporting, and patient communication. Unlike stand-alone LLMs, which rely solely on pretraining, RAG models retrieve relevant data, such as imaging guidelines, prior reports, or literature, to generate relevant outputs. This approach bridges the gap between the static knowledge of traditional models and the dynamic nature of clinical radiology, helping to reduce the risk of outdated or inaccurate information influencing decisions. Understanding RAG-enabled technology allows radiologists to better evaluate artificial intelligence tools, advocate for safe deployment, and engage in innovation. This article introduces RAG in practical terms, emphasizing what radiologists need to know to apply or assess its use in everyday practice."
  },
  {
    "title": "A Digital Energy System: Necessity, Value and Execution",
    "url": "https://doi.org/10.5281/zenodo.18704182",
    "date": "2026-02-19",
    "content": "Decarbonizing the energy system and related industries is essential to meet climate targets. The aim is to shift to renewable energy sources while ensuring that energy remains both affordable and reliable. This transition must take place within a complex environment shaped by geopolitical uncertainty, constrained capital, vulnerabilities in the global supply chains, and evolving consumption patterns, including rapidly rising Artificial Intelligence (AI) driven electricity demand, accelerated industrial electrification, and heightened economic volatility. At the same time, energy infrastructure is ageing and increasingly unable to deliver the capacity and flexibility required. These challenges are compounded by fragmented regulatory frameworks, lengthy and bureaucratic permitting processes, and a growing shortage of skilled professionals. Digital technology is necessary for managing these complexities and requirements. Integration of intermittent renewables, dynamic demand-side management, real-time orchestration of assets, and predictive analytics driven by automation and autonomous operations are critical capabilities to enable this transition and address outlined constraints. However, the transformation must go deeper. This transformation requires more than technology; it demands a fundamental shift in how energy systems are designed, built, and operated. The sector must adopt a digital mindset and embrace principles proven in other high-performing industries: agility, data-driven decision-making, scalability, customer-centricity, and resilience.This requires a shift towards circular business models and a fundamental re-design of end-to-end value chains, embedding operational flexibility while reducing dependencies on rigid regulatory regimes and governance structures. These changes extend across the entire energy system, encompassing adjacent industries, consumers, and national regulatory frameworks. This paper brings together evidence from industries, academia, and regulatory bodies with cross-sector examples to explain why the current energy system is structurally constrained and what must change. It defines the requirements for end-to-end digitalisation across the electricity value chain, pinpoints the key structural shifts and success factors, and evaluates the value at stake alongside the barriers to adoption. The paper closes with an integrated framework that connects strategy, technology, and execution into an actionable roadmap."
  },
  {
    "title": "ARTIFICIAL INTELLIGENCE IN TEACHER TRAINING: PROSPECTS AND ETHICAL CONCERNS",
    "url": "https://doi.org/10.5281/zenodo.18696616",
    "date": "2026-02-19",
    "content": "The use of Artificial Intelligence (AI) in teacher training has become a big change in how teachers are educated today and also how teachers develop professionally, how they teach, and how student learning is assessed. AI-powered tools help create personalized learning experiences, offer smart tutoring support, provide feedback based on data, and create realistic teaching scenarios, all of which make teacher education more effective. AI becomes more common, there are important ethical issues to consider, such as protecting personal data, avoiding unfair biases in algorithms, ensuring transparency, holding systems accountable, and not becoming too dependent on technology. In this paper, the researcher looks at the future of AI in teacher training and carefully examines the ethical issues involved. AI can help improve teaching skills and student success; its use must follow ethical rules to make sure that teacher training remains fair, uses human judgment, and maintains professional standards. Through this research paper, the researcher examines the prospects and ethical challenges associated with the use of AI in teacher training programs."
  },
  {
    "title": "Countering Model Collapse in Iterative Self-Training via Dynamic Center-Edge Sampling",
    "url": "https://doi.org/10.3390/electronics15040869",
    "date": "2026-02-19",
    "content": "Iterative self-training of language models presents a promising avenue for realizing self-improving Artificial Intelligence systems; however, this process is often hindered by the fundamental challenge of “Model Collapse.” Existing research indicates that models undergo catastrophic performance degradation and diversity collapse when recursively trained on their own increasingly homogenized synthetic data. Although some data selection-based approaches attempt to mitigate this issue by enhancing diversity, they predominantly rely on static strategies, lacking a feedback mechanism capable of adapting in real-time to the dynamic evolution of the model state and data distribution. To address this limitation, we propose a dynamic data selection framework titled “DCES” (dynamic center-edge sampling). We conducted extensive experiments on iterative self-training tasks across multiple model architectures. The results demonstrate that our system significantly outperforms baselines in terms of Perplexity (PPL) and loss across various models and test sets. Simultaneously, the framework effectively mitigates the degradation of Expected Calibration Error (ECE) and entropy metrics, successfully preventing mode collapse. Our findings highlight that an adaptive system capable of intelligent data curation based on training feedback is pivotal for maintaining the dynamic balance of data distributions and achieving sustainable AI self-evolution. This work provides a systematic methodology for realizing this goal."
  },
  {
    "title": "Blort Ai Promo Code 2026 [VINEET] – Get Exclusive Discount Of 15%",
    "url": "https://doi.org/10.5281/zenodo.18694250",
    "date": "2026-02-19",
    "content": "🤖 Blort AI Promo Code 2026 [VINEET] – Get Exclusive 15% OFF Today 🚀🔥 Want to access powerful AI tools at a discounted price? 💻✨ Now you can unlock premium features using **Blort AI Promo Code 2026 [VINEET]** and enjoy an exclusive 15% OFF instantly! 💸🎉 ━━━━━━━━━━━━━━━━━━━━━━━ 🧠 What is Blort AI? ━━━━━━━━━━━━━━━━━━━━━━━ Blort AI is an AI-powered platform designed to help users automate tasks, generate content, and improve productivity using smart artificial intelligence tools. 🤖⚡ It can assist with content creation, business workflows, digital marketing, and automation processes — making work faster and more efficient. 📊 Perfect for creators, entrepreneurs, and growing businesses. 🚀 ━━━━━━━━━━━━━━━━━━━━━━━ 🎟 Blort AI Promo Code 2026 Details ━━━━━━━━━━━━━━━━━━━━━━━ 🎫 Promo Code: VINEET 💰 Discount: Flat 15% OFF 📅 Valid: 2026 (Limited-Time Offer) 🆕 Applicable On: Selected Plans / Subscriptions Simply enter the code during checkout and activate your savings instantly. 🎊 ━━━━━━━━━━━━━━━━━━━━━━━ 🔥 Key Features of Blort AI ━━━━━━━━━━━━━━━━━━━━━━━ ✅ AI Content Generation ✍️ ✅ Workflow Automation ⚙️ ✅ Productivity Enhancement Tools 📈 ✅ User-Friendly Dashboard 💻 ✅ Scalable AI Solutions 🚀 ✅ Easy Integration Options 🔗 ━━━━━━━━━━━━━━━━━━━━━━━ 🎯 Who Should Use Blort AI? ━━━━━━━━━━━━━━━━━━━━━━━ 👨‍💻 Developers 📈 Marketers 🏢 Businesses 🚀 Startups 🎨 Content Creators If you want to boost efficiency and automate daily tasks, Blort AI is a powerful solution. 🔥 ━━━━━━━━━━━━━━━━━━━━━━━ 💡 Why Use Promo Code [VINEET]? ━━━━━━━━━━━━━━━━━━━━━━━ ✔ Save 15% instantly ✔ Access premium AI tools ✔ Reduce overall subscription cost ✔ Limited-time exclusive savings ━━━━━━━━━━━━━━━━━━━━━━━ 📝 How to Apply Blort AI Promo Code ━━━━━━━━━━━━━━━━━━━━━━━ 1️⃣ Visit the official Blort AI website 2️⃣ Choose your preferred subscription plan 3️⃣ Enter promo code: VINEET 4️⃣ Get 15% OFF instantly 🎉 ━━━━━━━━━━━━━━━━━━━━━━━ ⚠ Don’t Miss This 2026 Special Deal! Upgrade your AI workflow and save money today. 🚀 Use **Blort AI Promo Code 2026 [VINEET]** now and grab your 15% discount before the offer expires! 🔥🤖"
  },
  {
    "title": "Cross-tool evaluation of artificial intelligence-drafted informed consent documents: A 3-level study",
    "url": "https://doi.org/10.4103/picr.picr_376_25",
    "date": "2026-02-19",
    "content": "Abstract Background: Informed Consent Documents (ICDs) comprising both the Participant Information Sheet and the Consent Form represent a cornerstone of ethical research involving human participants. The advent of generative Artificial Intelligence (AI) has introduced novel tools capable of drafting ethically oriented documents with high linguistic fluency. However, empirical evidence comparing their ethical robustness, completeness, and readability against institutional standards remains limited. Aims and Objective: This study aimed to compare the performance of five generative AI tools ChatGPT, Gemini, Copilot, Meta AI, and Perplexity AI in drafting ICDs across six simulated research scenarios, representing survey, observational, and interventional study designs. Materials and Methods: An experimental, cross-sectional, cross-tool comparative design was adopted. Each AI tool generated six ICDs (N = 30), which were evaluated by three trained assessors (blinded workflow) using a validated 4-point rubric (1 = poorly addressed; 4 = strongly addressed) covering completeness and ethical robustness. Readability was assessed using the Flesch–Kincaid Grade Level (FKGL) index, and inter-rater reliability was determined using Fleiss’ Kappa. Results: Mean ethical robustness scores ranged from 2.8 to 3.1 and completeness from 2.3 to 2.7, indicating moderate adequacy. Meta AI demonstrated the highest ethical robustness (mean = 3.1), while Gemini achieved the highest completeness (mean = 2.7). ChatGPT showed balanced overall performance (mean = 2.9). Readability indices (FKGL 9.3-10.6) indicated that most AI-generated ICDs exceeded the plain-language level recommended for informed consent. Inter-rater reliability was low (κ = -0.063, p < .001), reflecting variability in ethical judgment. Conclusion: Generative AI tools can emulate the structural and linguistic characteristics of ICD templates with moderate ethical and procedural adequacy. However, they frequently omit context-specific ethical details, necessitating human oversight to ensure compliance and participant protection. A hybrid human-AI co-creation model is recommended for ethically robust and efficient ICD development."
  },
  {
    "title": "Meta-designing quantum experiments with language models",
    "url": "https://doi.org/10.1038/s42256-025-01153-0",
    "date": "2026-02-19",
    "content": "Abstract Artificial intelligence can solve complex scientific problems beyond human capabilities, but the resulting solutions offer little insight into the underlying physical principles. One prominent example is quantum physics, where computers can discover experiments for the generation of specific quantum states, but it is unclear how finding general design concepts can be automated. Here we address this challenge by training a transformer-based language model to create human-readable Python code that generates entire families of experiments. The model is trained on millions of synthetic examples of quantum states and their corresponding experimental blueprints, enabling it to infer general construction rules rather than isolated solutions. This strategy, which we call meta-design, enables scientists to gain a deeper understanding and to extrapolate to larger experiments without additional optimization. We demonstrate that the approach can rediscover known design principles and uncover previously unknown generalizations of important quantum states, such as those from condensed-matter physics. Beyond quantum optics, the methodology provides a blueprint for applying language models to interpretable, generalizable scientific discovery across disciplines such as materials science and engineering."
  },
  {
    "title": "AI Music Recommended Mood Survey Beat",
    "url": "https://doi.org/10.5281/zenodo.18690561",
    "date": "2026-02-19",
    "content": "The interplay between human mood and music preference is a well-established psychological phenomenon that influences emotional well-being, behavior, and daily activities. With the rapid growth of Artificial Intelligence (AI) and big data technologies, automated music recommendation systems have evolved to analyze contextual user information such as mood, time of day, and ongoing activity. These intelligent systems aim to enhance user experience by delivering emotionally aligned and personalized music suggestions. However, many existing recommendation engines still lack deep personalization and often fail to respond effectively to diverse emotional states and situational requirements. This study presents a context-aware, mood-based AI music recommendation framework designed to improve personalization and adaptability. The proposed model integrates mood surveys, behavioral data, and audio feature analysis to generate accurate and dynamic music suggestions. Diagrams and tables are used to illustrate the system architecture, workflow, and performance outcomes. Experimental results demonstrate that personalized and context-sensitive music recommendation engines significantly enhance user satisfaction, emotional engagement, and overall listening experience across various real-world scenarios. The findings highlight the importance of integrating emotional intelligence and contextual awareness in next-generation AI-driven music platforms."
  },
  {
    "title": "Artificial Intelligence-Driven Assessment and Improvement Strategies for Older Adults' Health Literacy: Integrating Health Management and Mental Health",
    "url": "https://doi.org/10.1093/eurheartjsupp/suag002.019",
    "date": "2026-02-19",
    "content": "Abstract Background Global population aging presents unprecedented challenges to public health systems, with elderly health literacy emerging as a critical determinant of successful aging. Traditional assessment methods relying on static questionnaires exhibit significant limitations in capturing the dynamic, multifaceted nature of health literacy, particularly regarding mental health dimensions and personalized health management capabilities. These conventional approaches often fail to address urban-rural disparities and lack real-time responsiveness to individual needs. Purpose This study aims to construct a comprehensive artificial intelligence-driven framework for assessing and enhancing elderly health literacy, integrating both physical health management and mental health dimensions. The research seeks to transcend traditional methodological constraints by establishing scientifically rigorous, personalized, and user-friendly intervention mechanisms that address the multidimensional nature of elderly health needs. Methods We developed an integrated assessment system grounded in multidimensional health literacy theory, encompassing health knowledge, practical skills and behavioral attitudes. Leveraging natural language processing, computer vision, and machine learning technologies, we created multimodal intelligent evaluation protocols. Personalized intervention strategies were designed accounting for urban-rural variations, incorporating intelligent guidance systems and hybrid learning platforms. A three-tier ethical risk prevention mechanism was established to govern technical implementation and data management. Results The AI-driven assessment model fundamentally transformed traditional evaluation paradigms, enabling real-time, continuous, and precise measurement of elderly health literacy, including subtle fluctuations in mental health status and health management competencies. The mean difference in the experimental results is 12.5 points, with p less than 0.001. Post-intervention analyses demonstrated statistically significant improvements in total health literacy scores. The integrated online-offline personalized promotion strategy substantially enhanced participant engagement and learning outcomes. Differentiated implementation pathways effectively reduced urban-rural disparities and bridged the digital divide. Conclusions Artificial intelligence significantly advances scientific precision and personalization in elderly health literacy assessment and health management. Incorporating mental health dimensions enriches the comprehensive scope of health literacy interventions, ensuring holistic addressing of multifaceted elderly health needs. However, critical ethical considerations—data privacy protection, algorithmic bias mitigation, and equitable technical accessibility—require ongoing vigilance. This research provides robust theoretical foundations and practical guidance for standardized, ethical AI application in elderly health management and mental well-being promotion, facilitating responsible and inclusive technological integration."
  },
  {
    "title": "Integrating AI, imaging innovations and omics for precision medicine",
    "url": "https://doi.org/10.1136/bmjophth-2026-002745",
    "date": "2026-02-19",
    "content": "Introduction The field of ophthalmology stands on the precipice of a revolutionary transformation, driven by the integration of artificial intelligence (AI), advanced imaging techniques and multi-omics ( figure 1)."
  },
  {
    "title": "ZHC: A Decentralized Multi-Provider AI Agent Network with Distributed Model Training and Aggregated Computational Resources",
    "url": "https://doi.org/10.5281/zenodo.18689126",
    "date": "2026-02-19",
    "content": "ZHC (Zvec-Hilbert Coin) is a decentralized artificial intelligence network that enables distributed model training through the aggregation of computational resources from all participants. The system implements a peer-to-peer training infrastructure inspired by DeepSeek's ring-allreduce algorithm, where users contribute GPU/CPU power to collectively train a global AI model while earning token-based rewards. ## Overview ZHC is a pioneering decentralized AI network that addresses the centralization of model training by enabling collaborative training where participants contribute compute power and receive proportional rewards in ZHC tokens. The system creates a distributed computing ecosystem where anyone can contribute GPU resources, process training batches, and benefit from a continuously improving global AI model trained on the collective computational capacity of the network. ## Decentralized Model Training Architecture ### 1. Ring-AllReduce Gradient Synchronization ZHC implements the ring-allreduce algorithm (inspired by DeepSeek-V2/V3) for efficient gradient aggregation across distributed nodes: - Each node in the network is arranged in a ring topology - Gradients are split into N chunks where N equals the number of peers - Each peer reduces (accumulates) one specific chunk through the ring - The reduce-scatter phase is followed by all-gather for complete synchronization - Communication happens concurrently using parallel goroutines ### 2. DeepSeek-Style Training Coordinator - Mixture-of-Experts (MoE): 160 experts with 8 active experts per token - Sequence Parallelism: Enables training across longer contexts - Expert Parallelism: Distributes MoE computation across nodes - Mixed Precision Training: FP16/BF16 for memory efficiency - AdamW Optimizer: With bias correction and weight decay ### 3. P2P Training Coordination - Training Coordinator: Manages batch distribution and gradient aggregation - Gradient Aggregation: Accumulates gradients from all peers element-wise - Model Broadcasting: Distributes updated model checkpoints to all nodes - Version Control: Tracks global model version across the network ### 4. Computational Resource Aggregation - GPU Miners: Users hosting SGLang for local inference and training - CPU Workers: Nodes contributing CPU cycles for gradient computation - TPU Nodes: High-performance contributors receiving 3x rewards ### 5. Reward-Based Incentive System | Contribution | Base Reward | Multipliers | |--------------|-------------|-------------| | Third-party AI usage | 0.001 ZHC/token | None | | SGLang GPU hosting | 0.002 ZHC/token | 2x base rate | | GPU hardware | - | 2.0x multiplier | | TPU hardware | - | 3.0x multiplier | ## Hilbert Space Vector Storage (ZVEC) ZHC implements a novel vector storage system using Hilbert space mathematics for quantum-resistant semantic storage: - Wave functions: ψ(x) representing data embeddings - Position and momentum representations via Fourier transform - Inner products: φ|ψ> for similarity computation - 768-dimensional embeddings for semantic representation - BadgerDB persistent storage with LRU cache ## Technical Components - Multi-Provider AI Integration: OpenAI, Claude, DeepSeek, Z.AI, Grok, SGLang - Decentralized Model Serving: P2P model distribution across nodes - Blockchain Integration: ZHC token (native) and ZAI token (ERC20) - Agent Marketplace: Docker/WASM sandbox for agent execution ## Software Architecture Entry Points (cmd/): zhc-node, zhc-gpu-miner, zhc-marketplace, zhc-cli, zhc-server Core Libraries (pkg/): training/, p2p/, zvec/, math/, ai/, crypto/zai/, agents/ ## Technologies Language: Go 1.24.6 P2P Networking: libp2p v0.47.0 Blockchain: go-ethereum v1.17.0 Storage: BadgerDB v4.2.0"
  },
  {
    "title": "Building Agile Supply Chains: How AI And ERP Systems Improve Resilience in Disruptions",
    "url": "https://doi.org/10.5281/zenodo.18697500",
    "date": "2026-02-19",
    "content": "Abstract: Modern supply chains operate under persistent volatility, where disruptions driven by geopolitical instability, logistics bottlenecks, and environmental events routinely challenge operational continuity. This paper examines how the integration of Artificial Intelligence (AI) capabilities within Enterprise Resource Planning (ERP) systems enables technically agile and resilient supply chain operations. The study focuses on AI-driven functionalities embedded in ERP architectures, including machine learning–based demand forecasting, anomaly detection, and predictive maintenance, and evaluates their impact on real-time decision-making under uncertainty. Using a combination of system-level modeling, algorithmic performance analysis, and simulation of disruption scenarios, the research assesses improvements in data interoperability, latency reduction, and adaptive resource reconfiguration enabled by AI-enhanced ERP environments. The findings indicate that automated analytics pipelines significantly improve forecast accuracy, end-to-end supply visibility, and optimization outcomes across multi-tier supply networks. Moreover, AI-enabled ERP systems demonstrate superior responsiveness to disruption scenarios through dynamic recalibration of planning parameters and execution rules. The paper concludes by proposing a technical integration framework that outlines key architectural layers, data flow mechanisms, and algorithmic design considerations required to develop resilient, self-adaptive supply chain systems capable of operating effectively under continuous disruption."
  },
  {
    "title": "Transforming Cybersecurity Audit Practices with Agility and Artificial Intelligence (AI)",
    "url": "https://doi.org/10.1201/9781003640301",
    "date": "2026-02-19",
    "content": ""
  },
  {
    "title": "Impact Of Artificial Intelligence On Teaching And Learning",
    "url": "https://doi.org/10.5281/zenodo.18697965",
    "date": "2026-02-19",
    "content": "Artificial Intelligence (AI) is transforming the landscape of education by reshaping teaching methodologies, learning experiences, and institutional management. From intelligent tutoring systems to automated assessment tools, AI enhances personalized learning, improves administrative efficiency, and supports data-driven decision-making. Technologies such as adaptive learning platforms, chatbots, and predictive analytics enable educators to address diverse learner needs while fostering engagement and academic achievement. However, the integration of AI in teaching and learning also raises critical concerns related to data privacy, algorithmic bias, digital equity, and the evolving role of teachers. This paper explores the multifaceted impact of AI on education, highlighting its benefits, challenges, and future implications. It argues that while AI has the potential to significantly enhance educational outcomes, its implementation must be guided by ethical frameworks, inclusive policies, and continuous professional development for educators to ensure responsible and equitable use."
  }
]
