[
  {
    "title": "Achieving more human brain-like vision via human EEG representational alignment",
    "url": "https://doi.org/10.1038/s42003-026-09685-w",
    "date": "2026-02-20",
    "content": "Despite advancements in artificial intelligence, object recognition models still lag behind in emulating visual information processing in human brains. Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often rely on invasive neural recordings from non-human subjects, leaving a critical gap in understanding human visual perception. Addressing this gap, we present, ‘Re(presentational)Al(ignment)net’, a vision model aligned with human brain activity based on non-invasive EEG, demonstrating a significantly higher similarity to human brain representations. Our innovative image-to-brain multi-layer encoding framework advances human neural alignment by optimizing multiple model layers and enabling the model to efficiently learn and mimic the human brain’s visual representational patterns across object categories and different modalities. Our findings demonstrate that ReAlnets exhibit stronger alignment with human brain representations than traditional computer vision models, achieving an average similarity improvement of approximately 3% and a maximum relative improvement ratio reaching up to 40%. This alignment framework takes an important step toward bridging the gap between artificial and human vision and achieving more brain-like artificial intelligence systems. EEG-aligned fine-tuning makes artificial vision models more brain-like, enhancing model-human representational similarity across EEG, fMRI, and behavior."
  },
  {
    "title": "ARTIFICIAL INTELLIGENCE IN COMMUNITY BASED REHABILITATION: A SYSTEMATIC REVIEW",
    "url": "https://doi.org/10.59365/hsj.4(2).2026.176",
    "date": "2026-02-20",
    "content": "Background: Community-Based Rehabilitation (CBR) is a strategy to promote inclusion, independence, and participation for people with disabilities, particularly in low-resource settings. In recent years, artificial intelligence (AI) has been increasingly used in clinical rehabilitation; however, its integration into community contexts remains limited. Objective: This systematic review aimed to identify and synthesize recent evidence on the use of AI technologies within CBR. Methods: A systematic search was conducted in PubMed, Scopus, and ScienceDirect for articles published from January 2020 to July 2025. Eligible studies included empirical research applying AI in CBR contexts. Two reviewers independently screened, extracted data, and assessed risk of bias using RoB2, ROBINS-I, PROBAST, and QUADAS-2. Results: From 842 identified records, 10 studies met inclusion criteria. Applications of AI in CBR were grouped into prediction and screening, home-based or remote rehabilitation, patient empowerment, and social support. Reported benefits included improved cognition, sarcopenia reversal, frailty and depression screening, diabetes self-management, and smoking cessation. Socially assistive robots were found acceptable and useful in supporting daily activities and emotional well-being. Limitations across studies included small samples, short follow-up, limited external validation, and a focus on technologically literate populations. Conclusion: AI shows considerable potential to strengthen the accessibility, personalization, and effectiveness of CBR. Future research should focus on large-scale, long-term studies with diverse populations and explore strategies for equitable, sustainable integration of AI into community rehabilitation services."
  },
  {
    "title": "Industry Conclave 2026 Converging Artificial Intelligence in Food, Biotech and Microbial Innovations",
    "url": "https://doi.org/10.5281/zenodo.18710360",
    "date": "2026-02-20",
    "content": "The Industry Conclave 2026: Converging Artificial Intelligence in Food, Biotech and MicrobialInnovations represents a significant interdisciplinary platform that brings together researchers,industry professionals, academicians, and innovators to explore the transformative potential ofartificial intelligence (AI) across food science, biotechnology, and microbial research. Astechnological advancements accelerate, the integration of AI-driven approaches with biologicalsciences has become essential for addressing global challenges related to food security,sustainable development, healthcare innovation, and environmental resilience. Artificialintelligence is redefining scientific workflows by enabling advanced data analytics, predictivemodeling, automation, and real-time decision-making. In food systems, AI supports smartagriculture, quality monitoring, supply chain optimization, and personalized nutrition. Withinbiotechnology, AI enhances bioprocess optimization, drug discovery, synthetic biology, andprecision engineering of biological systems. Microbial innovations, strengthened by machinelearning and computational tools, are opening new avenues in microbiome research, fermentationtechnology, industrial microbiology, and environmental bioremediation.This proceedings volume captures the diverse scientific contributions presented during theconclave, showcasing cutting-edge research findings, technological developments, and industrydrivenapplications. The chapters reflect collaborative efforts aimed at bridging the gap betweenfundamental research and real-world implementation, emphasizing translational approaches thatfoster innovation and sustainable solutions. In addition to technological advancements, thediscussions highlight ethical considerations, regulatory frameworks, and responsible use of AI inbiological sciences. The convergence of AI with food and microbial biotechnology demandsinterdisciplinary collaboration, transparency, and strong governance to ensure sustainable andequitable progress."
  },
  {
    "title": "Artificial intelligence driven context-aware biomedical information extraction model for multi-class disease condition classification using NLP with GPT-3",
    "url": "https://doi.org/10.1007/s44443-026-00476-1",
    "date": "2026-02-20",
    "content": "Precise extraction of medical phenotypes and entities from electronic health record (EHR) text is vital for numerous clinical research tasks, including cohort classification, tracking temporal patterns in disease evolution, and developing treatment plans. Still, this task remains difficult owing to the ambiguity and complexity of medical language. The application of generative pre-trained (GPT) techniques, such as GPT-3, GPT-4, and GPT-5, is explored for extracting biomedical information. Also, large language models (LLMs) are evaluated to improve their quality. Recently, rapid developments in artificial intelligence (AI) have significantly influenced many areas, with healthcare standing out as the most transformative. This is particularly evident in the field of natural language processing (NLP), where AI technologies capable of understanding and generating human-like text have altered how healthcare services are provided. In this manuscript, an Automated Biomedical Information Extraction Using Generative Pre-Training and Hybrid Attention Model (ABIE-GPTHAM) methodology is proposed in contextual NLP. This paper aims to develop a context-aware biomedical information extraction framework using AI to accurately identify and interpret relevant clinical entities from unstructured medical text. At first, the text pre-processing phase involves crucial steps such as tokenisation, punctuation removal, stop-word removal, case conversion, and stemming or lemmatisation to prepare the text for further analysis. Furthermore, the generative pre-training-3 (GPT-3) method is used to obtain the word vector representation. Moreover, an attention-based bidirectional long short-term memory (A-BiLSTM) method is employed for classification. The comparison analysis of the ABIE-GPTHAM approach demonstrated a superior accuracy of 98.80% compared to existing methods on the medical text dataset."
  },
  {
    "title": "Making Explanations Make Sense: XAI for SMiShing Detection",
    "url": "https://doi.org/10.25777/hq9j-5k20",
    "date": "2026-02-20",
    "content": "Explainable Artificial Intelligence (XAI) is a key component of effective human-AI collaboration, particularly in high-stakes domains such as cybersecurity. While AI tools hold promise for mitigating threats such as SMS-based phishing (SMiShing), their real-world effectiveness may hinge not just on detection accuracy, but on whether users can make sense of the system’s outputs. As SMiShing attacks grow in both frequency and sophistication, so does the urgency of designing human-centered AI systems that support user decision-making under uncertainty. This study examined how four distinct AI explanation types - Normative (rule-based), Attributive (feature-based), Exemplar (case-based), and Recommendation-Only - influence user performance, confidence, and mental workload in a simulated SMiShing detection task against a No AI baseline. Results showed that all AI-supported conditions improved classification accuracy, with minimal differences across explanation types. Confidence was slightly higher for Exemplar explanations, while subjective mental workload, perceived usability, and willingness to adopt the system did not vary across conditions. These results indicate that AI feedback can enhance decision-making without increasing workload or degrading user experience, while demonstrating that explanation presence may matter more than its style. The findings inform the design of more effective XAI systems that can optimize user decision-making and minimize mental effort when encountering cyber security threats."
  },
  {
    "title": "Sentinels in the Sky",
    "url": "https://doi.org/10.1093/9780197842850.001.0001",
    "date": "2026-02-20",
    "content": "Abstract This comprehensive work traces the evolution of satellites from early visionary concepts to the sophisticated global observation systems that define modern science and society. Beginning with historical parallels between maps, clocks, and satellites as tools that reshaped human understanding of space and time, the narrative follows the journey from Jules Verne’s fiction through rocket pioneers like Tsiolkovsky and Goddard, to the Cold War space race that launched the satellite age with Sputnik in 1957. The book demonstrates how satellites transformed from simple radio beacons into powerful scientific instruments that revolutionized multiple fields. Early chapters explore the foundational physics—including Einstein’s relativity theories and the Doppler Effect—that became essential to satellite operation and navigation systems like GPS. The development of weather forecasting from local speculation to global satellite-enabled analysis illustrates how visionaries like Harry Wexler anticipated the transformative potential of space-based observation. Subsequent sections detail how satellites enabled unprecedented Earth monitoring capabilities: tracking global weather systems, measuring land and sea-surface temperatures, monitoring vegetation health through multispectral imaging, and detecting ocean color changes that reveal marine ecosystem dynamics. The narrative also examines how satellites extended humanity’s vision beyond Earth, with space telescopes like Hubble revealing distant galaxies and contributing to discoveries about dark matter and dark energy that reshaped our understanding of the cosmos. The work concludes by examining the evolution from Cold War rivalry to international cooperation in space, and assesses the current era of satellite proliferation driven by private companies. It questions whether this expansion represents a true golden age of discovery or a gilded age dominated by commercial interests, while exploring future challenges including orbital debris, autonomous spacecraft, and the integration of artificial intelligence in satellite systems. Throughout, the book emphasizes the transition from ground-based observations to comprehensive space-based monitoring that has become indispensable for climate science, environmental management, navigation, communication, and our fundamental understanding of Earth’s place in the universe."
  },
  {
    "title": "Making Critical Care Reasoning Computable: A Triadic Framework for ICU Artificial Intelligence",
    "url": "https://doi.org/10.5281/zenodo.18710611",
    "date": "2026-02-20",
    "content": "This deposit presents a conceptual framework for ICU physiology and clinical AI centered on the Physiologic Margin Index (PMI), a triadic state representation intended to capture not only observed outputs (vitals, labs) but the hidden cost of maintaining them under support. Contemporary severity scores and many prediction models are largely outcome anchored and comparatively static, while critical illness is inherently dynamic: patients can appear stable because escalating external support masks progressive loss of underlying reserve. PMI formalizes this bedside reality by decomposing patient state into three interacting components: Φ (mobilizable physiologic power), κ (functional control and coordination), and χ (internal burden), with χ separated into fast and slow components. The index is defined as PMI(t) = Φ(t) × κ(t) / χ(t), where higher values indicate a wider viability margin. The manuscript proposes PMI as a representation layer for continuous monitoring, EHR time series, and model governance, rather than as a replacement for established scores. It emphasizes component attribution for actionability, mapping Φ, κ, and χ failures to specific clinical interventions, and introduces the role of support cost as an explicit signal that distinguishes intrinsic physiology from delivered support. The work is positioned as a viewpoint/perspective intended to guide prospective validation, including retrospective feasibility studies using open critical care datasets and subsequent clinical deployment studies assessing drift, intervention feedback, and transportability. This record provides a citable DOI for the framework, figures, and supporting materials, enabling versioned dissemination prior to empirical validation."
  },
  {
    "title": "German Traffic Intersection Images Dataset",
    "url": "https://doi.org/10.5281/zenodo.18714218",
    "date": "2026-02-20",
    "content": "Overview This dataset contains street-level imagery of various intersections in Germany. It was created as part of a university semester project for the course Applied Artificial Intelligence at HTW Dresden. The dataset is intended for research, educational purposes, and computer vision tasks, such as image classification or object detection. Dataset Structure & Contents The dataset consists of a total of 250 images, organized into six specific categories based on the type of intersection or traffic situation: Railway Crossing: Images of intersections with railway tracks. Roundabout: Images of circular intersections. Signed Intersection: Intersections where the right-of-way is controlled by traffic signs. Street with Right-Hand Rule: Uncontrolled intersections following the default \"right-before-left\" priority rule. Traffic Light Controlled: Intersections regulated by traffic lights. Unclear Situation: Complex or ambiguous traffic situations and intersections. Annotations (CSV) Alongside the images, the dataset includes a CSV file containing annotations and metadata. This file lists each image, detailing its specific properties and clearly indicating which classification category it belongs to. The overall collection includes both originally captured photos and images sourced from Mapillary."
  },
  {
    "title": "Reimagining Education in the Digital Age",
    "url": "https://doi.org/10.4324/9781003665472-1",
    "date": "2026-02-20",
    "content": "Higher education is being reimagined in the digital age, where Technology-Enhanced Learning (TEL) acts not only as a set of tools but also as a catalyst for rethinking the very purposes, practices, and values of education. This chapter establishes the conceptual foundation for the volume by examining TEL as both a transformative opportunity and an ethical challenge. It explores how artificial intelligence, immersive technologies, and flexible models such as HyFlex are reshaping teaching, learning, and institutional strategy—expanding personalisation, access, and innovation, while also surfacing concerns around digital divides, data ethics, academic integrity, and teacher agency. Drawing on global frameworks and practitioner perspectives, it argues that meaningful TEL adoption must be intentional, inclusive, and evidence-informed, aligning technology with human-centred values and long-term sustainability. The chapter also introduces the organisation of the book, which is structured into four interrelated themes: pedagogical innovations, innovative assessment practices, contextual challenges and inclusive practices, and global case studies. Together with the concluding synthesis, these contributions demonstrate how TEL can serve as a lens for reimagining higher education in ways that are ethically grounded, pedagogically sound, and globally relevant, offering pathways toward more equitable, creative, and future-ready learning ecosystems."
  },
  {
    "title": "How close is AI to replacing accounting consultants? Insights from a comparative study of multiple AI models and exit-level accounting students",
    "url": "https://doi.org/10.1108/medar-07-2025-3160",
    "date": "2026-02-20",
    "content": "Purpose Artificial intelligence (AI) has the potential to radically transform the accountancy profession, starting with routine transactions and functions and moving towards more strategic, leadership and governance responsibilities. The purpose of this paper is to evaluate the extent to which various Large Language Models (LLMs) can replicate the performance of accounting students completing a professional exit-level examination. By directly comparing the outputs of multiple AI models to those of students pursuing the Chartered Accountant (South Africa) [CA(SA)] designation, the study assesses whether AI has the capacity to support or potentially replace, accounting practitioners. In doing so, it clarifies both the opportunities and limitations of AI for the profession and for accounting education. Design/methodology/approach Exit-level exam questions taken by students pursuing the CA(SA) designation were given to ChatGPT, Claude, CoPilot, Grok and Gemini. A zero-shot prompt method was adopted. Each model was given the scenario and required tasks to complete, with mark allocations, as real-world students were. Basic descriptive statistics and visual representations were used to analyse the data. Findings The results find that AI models are not all equal as far as dealing with financial accounting questions. AI models’ primary strength is in dealing with journal entries and basic calculation questions. They struggle to critically evaluate partial and complete solutions to identify errors and correct them. These tasks require judgement and the ability to distinguish fact from fiction. This is where humans significantly outperform AI and should be where the accountancy profession focuses its student education. Of the AI models, Gemini performed the best overall but did not outperform the student average. Practical implications The study provides a useful baseline for future studies to monitor the progress of AI models in completing technical accounting exams and providing support to accounting professionals. Originality/value The study contributes to the literature by focusing on a renowned global chartered accountant designation from a developing economy and comparing different AI models’ performance to that of exit-level university students. The study also provides objective evidence that speaks to different AI models’ ability to support accounting practitioners in real-world settings or their ability to replace accounting consultants."
  },
  {
    "title": "Structural Intelligence Paradigm: A Post-LLM Framework for Artificial Intelligence",
    "url": "https://doi.org/10.5281/zenodo.18717112",
    "date": "2026-02-20",
    "content": "A paradigm statement proposing Structural Intelligence as the next phase of artificial intelligence, integrating statistical learning with structural computation and verification-based reasoning systems."
  },
  {
    "title": "Region‐Based Segmentation of Lymph Node Metastases in Whole‐Slide Images of Colorectal Cancer: A Pilot Clinical Study",
    "url": "https://doi.org/10.1002/cam4.71449",
    "date": "2026-02-20",
    "content": "ABSTRACT Background Digital technologies and artificial intelligence (AI) are transforming medical diagnostics, particularly in pathology. This study presented a two‐stage computer vision model designed to detect colorectal cancer metastases in whole slide images (WSIs) of lymph nodes. Methods We developed a classification–segmentation pipeline optimized for both accuracy and efficiency. The model was trained on 108 WSIs and evaluated on 554 WSIs collected from two institutions using Leica Aperio AT2 and Hamamatsu NanoZoomer S360 scanners. Results The classification model achieved a recall of 1.0 and a specificity of 0.935, while the segmentation model reported a Dice coefficient of 0.818 ± 0.105. Pathologists appreciated the model's precision in distinguishing solitary cancer cells from histiocytosis, reducing the need for peer consultations. Feedback from the pilot study indicated that the AI tool served as a valuable second opinion, enhancing diagnostic confidence. Conclusion This study explored the practical applications of AI in clinical pathology, offering perspectives from both pathologists and data scientists. Our findings highlighted how AI can streamline workflows, improve diagnostic accuracy, and support personalized treatment planning. The integration of AI into pathology workflows has the potential to redefine diagnostic standards while maintaining the critical role of pathologists in decision‐making."
  },
  {
    "title": "From Mary Shelley to Netflix: a Pan-European perspective on public communication of neuroscience and neurotechnology",
    "url": "https://doi.org/10.3389/fnins.2024.1278640",
    "date": "2026-02-20",
    "content": "Scientific knowledge of the human brain has captivated the public’s attention and sparked their imagination for centuries. Comprehending the inner workings of the mind and the underlying molecular and physiological aspects of the central nervous system has long been the defining theme of contemporary Western scientific culture. Even as the focus has arguably shifted towards genomics in the early 21st century, the brain continues to hold the spotlight in science communication, perhaps bolstered by the hype surrounding Artificial Intelligence. Neuroscience and neurotechnology, with their connections to culture, identity, economic progress, and health, remain subjects of fascination for people of all ages who seek to understand the present and future implications of research in these fields. In this work, we explore 10 distinct ways of communication dealing with the subject of the brain, the mind, applied neurotechnology, and what makes us, and possibly other things, human. We examine European literature, material culture, and various film formats to gain insights into these captivating subjects. Instances like Mary Shelley’s “Frankenstein” exemplify the historical fear of science. At the same time, TED Talks and documentaries have emerged as influential platforms for scientific communication. The intersection between art and brain imaging helps visualise abstract concepts. The gamification of thought experiments is an accessible tool for the public to understand complex cognitive phenomena. And, despite a lack of accuracy, science fiction can spark public debates on ethical issues involving the conscience of robots or the privacy of our brain data."
  },
  {
    "title": "Self-learning GAN based synthetic CT generation: unlocking CBCT-based adaptive radiotherapy",
    "url": "https://doi.org/10.3389/fonc.2026.1756153",
    "date": "2026-02-20",
    "content": "Purpose/objectives This study proposes and clinically evaluates synthetic CT (sCT) images generated from multi-center CBCT scans using artificial intelligence, with the aim of fully leveraging CBCT for adaptive radiotherapy in patients with pelvic, head-and-neck, lung, and breast cancer. Materials and methods In collaboration with TheraPanacea (Paris, France), AI-based sCT models were developed for multiple anatomical sites using a cycleGAN architecture. The study included 51 patients from two European institutions diagnosed with head-and-neck, lung, pelvic or breast cancer and treated with CBCT-based position verification. CBCT scans were acquired using two linear accelerator systems (Varian and Elekta). Image accuracy was assessed using MAE, SSIM, and PSNR. For dosimetric evaluation, planning CTs (pCTs) were non-rigidly registered to CBCTs. Treatment plans were created on the pCT using a clinical TPS to meet standard clinical criteria, then recalculated on both the warped CT (wCT) and sCT. Dose distributions were compared using global gamma passing rates and dose-volume metrics. Results The proposed model substantially improved image quality compared with CBCT. MAE decreased from 122.95 ± 50.07 to 23.65 ± 10.09, while SSIM increased from 0.78 ± 0.12 to 0.97 ± 0.03 and PSNR from 35.01 ± 7.24 to 44.35 ± 7.07. Dose-metric comparisons showed strong agreement between the pCT and wCT, with median relative differences within 0.5% for both targets and organs at risk. Median gamma passing rates for 2%/2 mm and 3%/3 mm criteria (10% threshold) reached 100% across all anatomical sites. No performance differences were observed between Elekta- and Varian-sCTs. Conclusion This multi-center study demonstrates the feasibility of generating clinically acceptable AI-based sCTs from CBCT for multiple anatomical sites, yielding consistent image quality improvements and reliable dosimetric accuracy."
  },
  {
    "title": "Effect of smart endodontic motors (AI-based torque control) on file separation and dentinal damage",
    "url": "https://doi.org/10.36377/et-0165",
    "date": "2026-02-20",
    "content": "INTRODUCTION. Implementation of artificial intelligence (AI) in the endodontic motor technology offers improved safety due to real-time torque monitoring and adaptive control. Nevertheless, there is little comparative evidence on their clinical effectiveness in preventing instrument separation and reduction of iatrogenic dentinal damage. MATERIALS AND METHODS. Ninety human mandibular molars with mesial canals (curvature 25–35°) were randomly divided to three groups ( n = 30) Group A (AI-based smart motor with adaptive torque control), Group B (conventional motor with preset torque limits), and Group C (conventional motor with auto-reverse operation). ProTaper Next rotary files were used to prepare the standardized canals. The main variables were file separation rate and dentinal microcracks incidence evaluated through micro-computed tomography (micro-CT). Preparation time, apical transportation, canal centering ratio, and motor operational parameters were the secondary outcomes. RESULTS. Group A exhibited a much lower file separation rate (0% vs. 10.0% vs. 13.3% in Group B vs. Group C, p = 0.038) and fewer dentinal microcracks formed (16.7% vs. 43.3% vs. 50.0% p = 0.006). Group A recorded significantly lower peak of mean torque (2.1 + 0.4 Ncm compared to 2.8 + 0.6 Ncm in Group B compared to 2.9 + 0.7 Ncm in Group C, p < 0.001). Apical transportation ( p = 0.284) and canal centering ratio ( p = 0.412) showed no significant differences. Time of preparation did not differ among groups ( p = 0.156). CONCLUSIONS. Smart endodontic motors with adaptive torque control powered by AI allow a significantly lower file separation and dentinal microcrack formation than using conventional motors and offer similar shaping efficiency and preparation quality. Based on these findings, AI-based motor technology can be clinically adopted to improve the safety of the procedures."
  },
  {
    "title": "Advancements in Psychotherapy and Treatment",
    "url": "https://doi.org/10.4018/979-8-3373-1325-2.ch007",
    "date": "2026-02-20",
    "content": "This chapter explores the potential of Artificial Intelligence in mental health, with a special focus on psychopathologies. The researchers expand on AI's role in assisting clinicians' diagnosis, symptom tracking, predictive modelling, and therapy for psychiatric and neurodevelopmental disorders like Depression, Anxiety, Post-Traumatic Stress Disorder (PTSD), Substance Use Disorder, Autism Spectrum Disorder (ASD), and ADHD. Crucial interventions include wearables for anxiety, virtual reality exposure for PTSD, and robotic social companions for children with ASD. Despite these advancements, the use of AI chatbots as a replacement for therapy has been a subject of debate, largely due to issues around safety. The study highlights potential limitations, including risks in user interactions, limited therapeutic support, algorithmic biases, accessibility issues, and ethical concerns; advocating for “Human in the Loop” models where AI and clinicians work together, and calling for ethically designed AI systems that augment access without compromising on empathy, nuance and relational depth."
  },
  {
    "title": "DEEPFAKE CONTENT TYPES AND THEIR GENERATION METHODS",
    "url": "https://doi.org/10.5281/zenodo.18708717",
    "date": "2026-02-20",
    "content": "Deepfake technology, powered by artificial intelligence, enables the manipulation of media content in multiple ways, including identity swapping, facial expression alteration, attribute modification, background replacement, and realistic speech or image synthesis. These manipulations can create highly convincing yet artificial content, posing challenges for digital media authentication, forensic analysis, and cybersecurity. Understanding the various deepfake types and their generation techniques is essential for designing robust detection methods and improving the reliability of automated verification systems."
  },
  {
    "title": "Enhanced diagnostic interpretation of the MoCA using machine learning",
    "url": "https://doi.org/10.3389/fnins.2026.1679649",
    "date": "2026-02-20",
    "content": "Introduction Artificial Intelligence (AI) is increasingly being integrated into clinical practice to optimize diagnosis in neurocognition. By capturing distinct cognitive signatures, this approach may offer a more precise alternative to the traditional interpretation of the Montreal Cognitive Assessment (MoCA) which often relies on a fixed cutoff score (26/30). We aimed to evaluate whether machine learning models, by integrating detailed MoCA subtest scores, demographic variables, and cognitive chart-derived metrics, can improve the detection of cognitive impairment and classification of dementia subtypes. Methods We analyzed 38,746 clinical observations (17,188 unique individuals) from the National Alzheimer’s Coordinating Center database. Five supervised learning algorithms, Extreme Gradient Boosting (XGBoost), Random Forest, Support Vector Machine (SVM), Logistic Regression, and k-Nearest Neighbors (KNN), were trained using detailed MoCA subtest scores, demographic variables, and cognitive chart-derived metrics as predictors. To ensure generalizability of results and prevent data leakage, we applied a rigorous nested Repeated Grouped Cross-Validation strategy. Decision thresholds were optimized via the Youden Index on independent calibration sets, and model interpretability was ensured through SHAP value analysis. Results Machine learning models consistently outperformed conventional approach. For the global detection of cognitive impairment, XGBoost achieved the best performance (Youden Index 0.61 vs. 0.54 for the standard cutoff). Regarding subtype classification, models demonstrated variable discriminative capacity depending on clinical homogeneity: primary progressive aphasia was best classified (Youden ≈ 0.77), followed by Lewy body dementia and Alzheimer’s disease, while vascular dementia remained more challenging to isolate. Feature importance analysis highlighted the Cognitive Quotient as a robust universal predictor, while pinpointing disease-specific drivers such as delayed recall for Alzheimer’s disease and verbal fluency for primary progressive aphasia. Conclusion Our findings suggest interpretable machine learning enhances diagnostic utility of the MoCA, yielding superior accuracy compared to a fixed cutoff. By synthesizing individualized subtest profiles within a transparent framework, this approach offers a clinically actionable solution. It transforms the MoCA from a simple screening tool to a precision diagnostic aid, optimizing patient triage in the era of disease-modifying therapies."
  },
  {
    "title": "Molecular Pathology in Modern Medicine: A Review of Genomic, Proteomic, and Epigenetic Insights Into Disease Mechanisms",
    "url": "https://doi.org/10.7759/cureus.103950",
    "date": "2026-02-20",
    "content": "Molecular pathology has become a central discipline in modern medicine by enabling the systematic interrogation of genomic, proteomic, and epigenetic alterations that underlie human disease mechanisms. This review synthesises how genomic variants, proteomic dysregulation, and epigenetic alterations interact to drive disease initiation, progression, and phenotypic heterogeneity. Key molecular pathology platforms, including whole-genome sequencing, mass spectrometry-based proteomics, and epigenetic profiling, are examined as tools for elucidating disease mechanisms in modern medicine. Computational strategies, including multimodal data harmonisation, causal and network models, and interpretable artificial intelligence (AI), enable mechanism-anchored patient endotyping and therapy selection; clinical decision support and pharmacogenomics (PGx) translate molecular evidence into action. Considerations influencing the implementation of integrative molecular pathology include pre-analytical handling, platform-specific characteristics, data harmonisation, economic feasibility, model interpretability, reproducibility, and the extent of prospective clinical validation and reimbursement pathways. Recommended priorities include standards for pipeline validation, longitudinal and minimally invasive monitoring, deployable multimodal AI with uncertainty estimates, and equitable data governance. Molecular pathology reframes diagnostics in modern medicine by shifting from isolated markers toward a mechanistic understanding of disease biology across oncology and complex disorders."
  },
  {
    "title": "DIGITAL JOURNALISM: INFORMATION SECURITY, MEDIA LITERACY, AND FACT-CHECKING",
    "url": "https://doi.org/10.5281/zenodo.18709893",
    "date": "2026-02-20",
    "content": "Digital journalism today operates within an environment shaped by artificial intelligence, multimedia platforms, social networks, and high-speed communication technologies. This article examines three essential pillars of modern digital journalism—information security, media literacy, and fact-checking—and analyzes their interdependence in maintaining a safe and trustworthy information ecosystem."
  },
  {
    "title": "Bioacoustics-Generation",
    "url": "https://doi.org/10.5281/zenodo.18714556",
    "date": "2026-02-20",
    "content": "The raw audio recordings were provided by Bird Data Technology (Beijing) Co., Ltd. (https://www.birdsdata.com). Interested researchers may request access by contacting Bird Data Technology directly (sell@birdsdata.com). The dataset is also available via the Beijing Academy of Artificial Intelligence (https://data.baai.ac.cn/datadetail/Birdsdata, BAAI data@baai.ac.cn), which collaborates with Bird Data Technology in data collection."
  },
  {
    "title": "EXPRESS: Toward Developing the Foundations of Global Artificial Intelligence Governance: Insights from Triple Stakeholder Perspectives and Reflexive Thematic Analysis",
    "url": "https://doi.org/10.1177/07439156261430023",
    "date": "2026-02-20",
    "content": "This article proposes the building blocks of a global artificial intelligence governance (GAIG) framework based on a triple stakeholder approach that encompasses consumers, governments, and businesses. It develops GAIG’s building blocks through a synthesis of AI governance policies from the United States (US), the European Union (EU), and China using reflexive thematic analysis. The three key theoretical lenses applied are: a consumer-oriented value hierarchy framework, a government-oriented dynamic pyramid model, and a business-oriented privacy-by-design framework. The analysis reveals distinct regional approaches: the US emphasizes business-led AI innovation and flexible regulation, the EU prioritizes strict consumer protection and ethical standards, and China focuses on state-controlled AI development aligned with national interests. Despite differences in approaches, the findings suggest opportunities for convergence in GAIG, particularly in promoting ethical AI innovation, ensuring transparency, and safeguarding consumer rights. The resulting GAIG building blocks and future research avenues provide a comprehensive approach to AI governance, balancing the unique needs of consumers, governments, and companies while addressing diverse challenges and promoting ethical and responsible AI innovation. The study contributes to the understanding of international AI policy convergence and divergence, offering marketing and public policy insights for companies, marketers, policymakers, and researchers navigating the complex GAIG landscape."
  },
  {
    "title": "An Exploratory Review on the Role and Relevance of Artificial Intelligence (AI) Tools for Informed Decision-Making in Key Areas of Business",
    "url": "https://doi.org/10.5281/zenodo.18712932",
    "date": "2026-02-20",
    "content": "Innovative business decision-making solutions by using technological tools go a long way in improving business competence, restructure processes and drive radical development. Despite the presence of an array of such tools in vogue, there was a need of the hour to choose the best among them for efficient and effective business decisions. With this prime intent in mind, this study was considered timely. This review is exploratory in nature, and the analysis is based on essential contents extracted from eighteen relevant and recent critical sources written by competent authorities in the specific field. The objectives of this study were: to explain the meaning of Artificial Intelligence (AI); to explore their role and relevance in informed decision-making related to the key areas of business; and to recommend productive suggestions for the effective utilization of these all AI tools in the key areas of business. The findings reveals that, of all existing business decision-making tools developed over the years, the emergence of fifteen outsmarting Artificial Intelligence (AI) tools of recent origin is spectacularly awesome, and outsmarting in key business processes related to core fields like marketing (Miro, Filestage, Buffer), sales (Salesforce Einstein, Outreach, and Regie.ai), HR (SeekOut, Peoplebox.ai, and Humanforce), customer service (HubSpot Service Hub, Help Scout, and Gorgios), and operation management (Monday, Odoo, and Scoro). For their effective utilization in business, the study suggests creation of awareness about these tools for the businessmen to appreciate them and accept them, and developing their passion to learn and handle them is of paramount importance, for their sustained productivity."
  },
  {
    "title": "Analítica del aprendizaje y predicción del abandono estudiantil en entornos virtuales",
    "url": "https://doi.org/10.63380/esj.v4n1.2026.281",
    "date": "2026-02-20",
    "content": "Objective: The study examined how recent literature has addressed learning analytics to understand and predict student dropout in virtual environments. Academic and behavioral indicators, institutional conditions, and the ethical implications of using artificial intelligence were considered. Methodology: A qualitative study with a bibliographic-documentary approach was conducted. Publications were intentionally selected, and the information was organized using an extraction matrix. Thematic coding and content analysis were applied to identify similarities, differences, and gaps in the field. Results: The research focused on 2021–2025, and the analysis identified four main patterns: (1) reasons and indicators of school dropout; (2) use of data for educational decision-making; (3) contextual conditions (participation, inequalities, and infrastructure); and (4) ethics, biases, and educational objectives of AI. It was noted that the ethical dimension was very prominent, while evidence of predictive models with clear technical validation was less prevalent by comparison. Additionally, risks of misinterpretation were noted when the platform’s metrics do not include access and infrastructure variables. Conclusions: It was concluded that dropout prediction requires integrating participation and performance indicators with pedagogical interpretation, contextual controls, and operational ethical frameworks. This is necessary to sustain transparent and equitable institutional decisions. It was suggested to advance long-term validations, evaluate biases by subgroup, and study the effectiveness of interventions triggered by early warnings."
  },
  {
    "title": "Infinite Fractal Descent and Oscillatory Hormesis: A Biologically Constrained Framework for High-Density Neural Architectures",
    "url": "https://doi.org/10.5281/zenodo.18717677",
    "date": "2026-02-20",
    "content": "Modern artificial neural networks (ANNs) achieve performance through massive parameter expansion, operating without the strict metabolic constraints that govern biological intelligence. This paper introduces a unified architectural and training paradigm: Infinite Fractal Descent (IFD) and Oscillatory Hormesis. By replacing flat weight matrices with recursive, self-similar generator seeds (IFD) and subjecting the network to dynamic computational starvation and recovery cycles (Hormesis), we demonstrate a theoretical framework for achieving biological-level information density. This approach shifts AI development from brute-force scaling to \"tightly packed\" metabolic efficiency, yielding models that are inherently sparse, structurally robust, and highly compressed."
  },
  {
    "title": "Transformasi Sistem Informasi Menjadi Sistem Cerdas Untuk Meningkatkan Pengambilan Keputusan Dan Efisiensi",
    "url": "https://doi.org/10.65258/jutekom.v2.i1.53",
    "date": "2026-02-20",
    "content": "The industrial world and information systems are undergoing a major shift in management practices. Growing and diverse user demands have driven information systems to evolve from passive data management into intelligent systems that assist organizational management in decision-making processes. This research aims to analyze the evolution of information systems, from conventional models to the implementation of Artificial Intelligence (AI) based systems within organizations. The findings indicate that the transformation of information systems is not merely a software update, but a paradigm shift: from passive systems reliant on manual input (Conventional Phase), to systems capable of integrated workflows (Automated Phase), and finally to systems that can learn and provide independent recommendations (Intelligent System Phase). The primary finding of this study is that the transition to intelligent systems significantly enhances operational efficiency. However, this must be balanced with high data quality as a reliable information source and supported by the readiness of human resources. In conclusion, information systems in the digital era have transformed from simple administrative tools into essential strategic partners that help organizations navigate data complexity and improve decision-making."
  },
  {
    "title": "The Anatomical Basis and Clinical Progress of Pericapsular Nerve Block Technique for Hip Joint",
    "url": "https://doi.org/10.55014/pij.v9i1.977",
    "date": "2026-02-20",
    "content": "Objective: To systematically review the anatomical basis, clinical application effects, and research progress of pericapsular nerve group (PENG) block technique, providing reference for clinical analgesia plan selection. Methods: Relevant literature at home and abroad was searched, summarizing the anatomical characteristics and technical innovations of PENG block, comparing the limitations of traditional block techniques, analyzing its clinical analgesic effects, preservation of motor function, and safety, summarizing current controversies and looking forward to future directions. Results: PENG block is based on the multi-source nerve distribution (femoral nerve, obturator nerve, and accessory obturator nerve supply) and the separation characteristics of sensory-motor nerve pathways at the anterior capsule of the hip joint. Through ultrasound-guided targeted injection (with the puncture needle inclined at 30-45° to the midpoint of the line connecting the anterior inferior iliac spine and the pubic tubercle), sensory-motor separation block can be achieved. Clinical studies show that PENG block can reduce dynamic visual analog pain scores (Visual Analogue Scale, VAS) in patients with hip fractures (from 7.8 to 3.2), reduce morphine consumption by 45% in patients undergoing total hip arthroplasty (Total Hip Arthroplasty, THA) within 12 hours postoperatively, and achieve a quadriceps muscle strength retention rate of 92% at 6 hours post-THA, with a major complication rate of only 1.4%. Current controversies focus on differences in access methods (direct vision vs. ultrasound guidance) and drug parameters (optimizing local anesthetic concentration at 18-20mL), with long-term safety data still needing to be supplemented. Conclusion: PENG block has achieved precision in hip joint analgesia, highly aligning with the concept of enhanced recovery after surgery (Enhanced Recovery After Surgery, ERAS), and is a high-quality alternative to traditional block techniques. Future improvements in clinical value should be made through artificial intelligence-assisted positioning and multimodal analgesia integration."
  },
  {
    "title": "Harnessing Artificial Intelligence for Early Disease Detection: Opportunities and Challenges in Modern Healthcare",
    "url": "https://doi.org/10.62411/jcta.15367",
    "date": "2026-02-20",
    "content": "Artificial Intelligence (AI) is increasingly recognized as a transformative enabler of early disease detection, with the potential to improve diagnostic accuracy, support predictive risk stratification, and advance preventive healthcare. Despite rapid methodological progress, many existing reviews remain performance-centric, offering limited insight into generalizability, ethical governance, and real-world implementation constraints. This paper presents a narrative and integrative review with an adoption-focused, translational perspective, synthesizing recent developments in AI-driven early disease detection across oncology, cardiology, neurology, and infectious disease surveillance. Drawing on peer-reviewed literature published primarily between 2016 and 2025, the review examines reported performance gains alongside persistent limitations related to data heterogeneity, population bias, explainability, and regulatory fragmentation. Through cross-sectional synthesis, we identify three recurring gaps in prior reviews: (i) overgeneralization of AI’s diagnostic superiority, (ii) insufficient consideration of ethical and legal accountability, and (iii) a lack of actionable guidance for scalable clinical implementation. Integrating technical, ethical, and policy dimensions into a unified conceptual framework, this review demonstrates that while AI systems can consistently enhance diagnostic accuracy and early risk stratification in well-defined tasks, sustained clinical adoption depends on aligning technical performance with governance readiness, interpretability, and workflow integration. The analysis further highlights how implementation mechanisms—such as explainable AI, continuous post-deployment monitoring, and clinician-centered deployment strategies—mediate the translation of algorithmic innovation into real-world healthcare impact. Overall, this review provides a critical reference for researchers, clinicians, and policymakers seeking to translate AI innovation into safe, equitable, and trustworthy clinical practice."
  },
  {
    "title": "Anketas datu kopa: sociālo zinātņu studentu ģeneratīvā mākslīgā intelekta lietojuma paradumi",
    "url": "https://doi.org/10.5281/zenodo.18712498",
    "date": "2026-02-20",
    "content": "Anketas datu kopa: sociālo zinātņu studentu ģeneratīvā mākslīgā intelekta lietojuma paradumi Datu kopa satur Latvijas Universitātes un LU Banku augstskolas ociālo zinātņu studiju programmu studentu (n=279) sniegtās atbildes, anketēšana veikta 2025. gada maijā un jūnijā. Datu kopā apkopotas atbildes uz jautājumiem par ģeneratīvā mākslīgā intelekta (ĢMI) izmantojuma motivāciju, ētiskajiem apsvērumiem, veicinošajiem faktoriem, šķērļiem, kā arī uzskatiem par to, kāda veida uzdevumos ĢMI lietojums ir pieļaujams. Links uz aptaujas anketu Survey Dataset: Generative Artificial Intelligence Usage Among Social Sciences Students in Higher Education The data set is based on the data of the students in social sciences at the University of Latvia and BA School of Business and Finance of the University of Latvia, conducted in May - June 2025. The dataset includes responses to questions about motivations for using generative artificial intelligence (genAI), ethical considerations, enabling factors, barriers, as well as views on what types of tasks genAI use is permissible."
  },
  {
    "title": "A Fair Bandwidth Scanning Strategy to Detect an Adversary",
    "url": "https://doi.org/10.1142/s0219198926400013",
    "date": "2026-02-20",
    "content": "Detecting malicious users or unauthorized activities is a critical challenge in dynamic spectrum access. Traditionally, in such problems, an intrusion detection system (IDS) aims to maximize detection probability. Meanwhile, in networks or radio spectrum problems with multiple nodes or bands, respectively, a protocol that maximizes detection probability might lead to focusing on scanning the most plausible nodes or bands for intrusion and neglecting to scan less plausible ones due to limited scanning resources. To address this challenge, we propose a protocol that maximizes the fairness of detection probabilities across all bands within the bandwidth. We consider α-fairness as a fairness criterion. Using a game-theoretical approach, we model the IDS, which has to decide which of the bands to scan and how long to do it when the IDS faces an adversary who endorses artificial intelligence (AI), enabling it not only to infiltrate the bandwidth without being detected but also to do so in a less predictable manner for the IDS. The equilibrium strategies of the IDS and adversary are derived. An advantage of the fairness detection probability protocol in comparison with the maximizing detection probability protocol is illustrated."
  },
  {
    "title": "Robotic Deconstruction of Brickwork Enabled by Spatial Artificial Intelligence",
    "url": "https://doi.org/10.21203/rs.3.rs-8672329/v1",
    "date": "2026-02-20",
    "content": "<title>Abstract</title> Robotic deconstruction offers a precise and environmentally responsible alternative to conventional demolition, enabling the selective recovery of building materials for reuse. This research presents a methodology for robotic deconstruction enabled by Spatial Artificial Intelligence (Spatial AI), demonstrated through the case of brickwork. The approach comprises: (1) a deep learning-based real-time object perception system, trained on synthetic photorealistic data to detect and localise individual bricks; (2) the incremental registration and spatial mapping of these discrete elements within an evolving as-built digital twin; and (3) reasoning and control routines that enable perception- and mapping-informed, stepwise robotic deconstruction. The methodology was validated in two progressively complex case studies involving the deconstruction of dry-stacked and mortar-bound brickwork structures with unknown geometries. A mobile robot equipped with an RGB-D camera, gripper, and drill enabled the perception and recovery of individual bricks. Both structures were successfully deconstructed, with variation in manipulation robustness. Results demonstrate the system’s efficacy and broader applicability beyond brickwork."
  },
  {
    "title": "A Modern Cognitive Architecture Framework (CAF): Designing How Intelligent Systems Think, Decide, and Behave",
    "url": "https://doi.org/10.5281/zenodo.18708508",
    "date": "2026-02-20",
    "content": "—Artificial Intelligence is increasingly functioningas a cognitive layer embedded within organizational decisionmaking processes. However, most enterprise deployments lackformalized reasoning discipline, safety alignment, and interactiongovernance. This paper introduces a Cognitive ArchitectureFramework (CAF) as a layered architectural model for governinginterpretation, decision-making, and interaction in intelligentsystems. A formal representation of CAF is developed and mathematical properties of bounded cognition, uncertainty escalation,and decision alignment are established to demonstrate safetyconsistent reasoning behavior in enterprise AI systems."
  },
  {
    "title": "Coherence, Ethics, and the Future of Human Systems: Implications of the LM Framework for Agency, Technology, and Collective Responsibility",
    "url": "https://doi.org/10.5281/zenodo.18707961",
    "date": "2026-02-20",
    "content": "This preprint explores the ethical implications of the LM framework, in which consciousness, identity, and agency emerge from sustained coherence in distributed observer systems. Ethics is reformulated as a physical and dynamical property of system evolution rather than a normative or intention-based construct. Harm corresponds to coherence degradation, while ethical action corresponds to the preservation and expansion of viable coherent futures. The article examines consequences for trauma, technology, artificial intelligence, social systems, and humanity’s long-term stability, proposing coherence preservation as the central ethical challenge of the human future."
  },
  {
    "title": "Memory, Retrieval, and the Architecture of Persistent Cognition in AI Systems",
    "url": "https://doi.org/10.5281/zenodo.18715013",
    "date": "2026-02-20",
    "content": "Memory, Retrieval, and the Architecture of Persistent Cognition in AI Systems presents a comprehensive architectural analysis of how artificial intelligence systems can transition from stateless generative models to structurally persistent cognitive systems. The paper argues that contemporary large language models, while highly capable in bounded-context generation, lack the architectural mechanisms necessary for longitudinal reasoning, identity continuity, and domain-specialized knowledge accumulation. The work distinguishes between parametric knowledge embedded in model weights and non-parametric external memory systems, emphasizing that scalable, adaptive cognition requires explicit separation between the two. It systematically examines the computational constraints of transformer architectures, including quadratic attention complexity, KV cache bottlenecks, position bias, and context degradation phenomena, demonstrating why mere context expansion cannot solve persistent reasoning limitations. The paper then synthesizes advances in retrieval-augmented generation, vector database optimization, episodic versus semantic memory modeling, continual learning, catastrophic forgetting mitigation, differentiable memory modules, and hardware-software co-design. A stratified memory framework is proposed, consisting of session-level memory for conversational coherence, episodic memory for cross-session task continuity, and durable domain memory for validated longitudinal knowledge. This layered model reframes AI system design as memory lifecycle management rather than parameter scaling. Beyond architecture, the paper foregrounds governance constraints as first-order design requirements. It addresses epistemic traceability, version control, conflict detection, uncertainty calibration, and retrieval validation as essential components of trustworthy persistent systems. Persistent cognition is therefore framed not merely as a performance upgrade, but as an infrastructural reconfiguration of AI systems toward explicit memory orchestration and auditability. The paper concludes that achieving persistent cognition requires coordinated advances across model architecture, memory systems engineering, distributed infrastructure, and evaluation methodologies. It positions persistent AI as a structural shift from reactive token prediction toward memory-integrated, state-aware reasoning systems capable of adaptive, sustained knowledge evolution."
  },
  {
    "title": "The impact of career adapt-abilities on AI anxiety among English majors: a dual perspective analysis based on core self-evaluations at the person- and variable-centered",
    "url": "https://doi.org/10.3389/fpsyg.2026.1767791",
    "date": "2026-02-20",
    "content": "The rapid advancement of artificial intelligence (AI) technologies in language services, education, and knowledge production has imposed substantial occupational displacement pressures on English majors, thereby triggering significant AI-related anxiety. However, existing research rarely systematically explores the formation mechanisms of AI anxiety among English majors, especially lacking an in-depth analysis of the protective role of career adapt-abilities and their internal heterogeneity. This study adopts a dual-perspective approach—integrating variable-centered and person-centered analyses—to investigate how career adapt-abilities influence AI anxiety and the mediating role of core self-evaluations. A total of 444 English major students from four comprehensive universities in Sichuan, China, were recruited during July and August 2025. Measurements included the Career Adapt-Abilities Scale, Core Self-Evaluation Scale, and AI Anxiety Scale. Results show that career adapt-abilities significantly and negatively predict AI anxiety, with core self-evaluations partially mediating this relationship. Latent profile analysis identified three distinct career adapt-abilities subgroups—low, medium, and high—with significant differences in core self-evaluations and AI anxiety levels among them. Notably, the low career adapt-abilities group exhibited the highest AI anxiety, while the high group showed the lowest. Both analytic strategies converge to demonstrate that career adapt-abilities constitute an essential psychological resource mitigating AI anxiety in English majors, with core self-evaluations serving as a key cognitive mechanism. This study reveals a dual-pathway influence of career adapt-abilities on AI anxiety, offering a novel theoretical framework for understanding technological anxiety formation. Moreover, the pronounced heterogeneity of career adapt-abilities underscores the necessity for stratified career development education and psychological interventions tailored to diverse student groups, providing practical guidance for optimizing talent cultivation in English major programs."
  },
  {
    "title": "EFFECTIVENESS OF TRAINING FUTURE ENGINEERING STUDENTS USING DIGITALIZED EDUCATIONAL TECHNOLOGIES",
    "url": "https://doi.org/10.5281/zenodo.18708405",
    "date": "2026-02-20",
    "content": "This article analyzes the issues of increasing the effectiveness of training future engineering students based on digital educational technologies. In the modern higher education system, the use of digital platforms, virtual laboratories, distance learning systems, and artificial intelligence tools is considered an important factor in the formation of engineering competencies. The study pedagogically and methodologically substantiates the influence of the digital educational environment on the development of professional qualities of engineers - analytical thinking, problem-solving skills, project thinking, and digital competencies."
  },
  {
    "title": "Reducing Algorithmic Bias in Generative Artificial Intelligence-Based Cyberbullying Detection Systems",
    "url": "https://doi.org/10.5281/zenodo.18715421",
    "date": "2026-02-20",
    "content": "ABSTRACT The explosive growth of social media has put more pressure on the issue of cyberbullying and its effect on the well-being of its users. Although definitive solutions have become common, using artificial intelligence-inspired detection systems to address harmful content to a moderate degree, there is growing evidence to suggest that they tend to be algorithmically biased, with a disproportionate rate of misclassification occurring when applied to linguistic variations that align with a certain demographic or cultural group. This defeats equity, confidence, and psychological security on the internet. This paper suggests a generative artificial intelligence framework to improve cyberbullying detection, in addition to the proactive reduction of bias. The model combines language modelling in contexts based on transformer-based generative representations and fairness aware optimization. Balanced data sampling, counterfactual data augmentation and loss functions that have fairness constraints are used as a bias reduction measure applied during model training. A dataset of multi-source cyberbullying composed of various linguistic phrases is experimentally tested. The measures of performance are accuracy, precision, recall, and F1 score, as well as having fairness metrics such as demographic parity difference and equal opportunity difference. Findings show that the given approach is competitive in terms of classification performance and its inter-group bias is lower than in the case of the baseline deep learning models. The results point to the significance of ethical and equity concerns in generating artificial intelligence systems of content moderation. The suggested framework will help to create inclusive, responsible, and safe psychological online spaces. Keywords-Detection of cyberbullying, mitigation of algorithmic bias, fairness in machine learning, Natural language processing, transformer models, ethical artificial intelligence."
  },
  {
    "title": "The Algorithmic Boardroom: AI-Driven Governance and Strategic Decision Making",
    "url": "https://doi.org/10.46697/001c.157710",
    "date": "2026-02-20",
    "content": "Artificial Intelligence (AI) has seen a boost in a couple of years. Scholars and practitioners are proposing its use in various organizational functions i.e., recruitment, hiring, financial risk analysis, performance evaluation and reporting. However, guidelines for bringing it to corporate boards are scant. This article proposes how AI can effectively be used by corporate board members to get optimal results from the technology and frame reliable decision-making practices at company board level. Specifically, a 4-pillar framework for algorithmic governance has been proposed along with a four-stage operational process to effectively implement AI assisted board governance."
  },
  {
    "title": "Introducing the Quality 6.0 concept: a multi-criteria decision-making framework for ranking intelligent traffic management systems",
    "url": "https://doi.org/10.1108/ijqrm-02-2025-0058",
    "date": "2026-02-20",
    "content": "Purpose This study introduces advancing knowledge in the field of quality management, positioning it as an evolution of traditional quality management practices that is specifically tailored to meet the demands of today's smart systems and the ongoing digital transformation. To further contextualize Quality 6.0 (Q6.0), the study explores its integration within the broader framework of Industry 6.0 (I6.0), an emerging paradigm that transcends the automation and digitalization focus of its predecessors, Industry 4.0 (I4.0) and Industry 5.0 (I5.0). Design/methodology/approach The study applies the Q6.0 framework to the selection of Intelligent Traffic Management Systems (ITMS), a critical component of smart city initiatives aimed at optimizing traffic flow, reducing congestion and enhancing overall urban mobility. Five ITMS alternatives are evaluated and ranked using a Multi-Criteria Decision-Making (MCDM) approach. Specifically, the Intuitionistic Fuzzy Step-wise Weight Assessment Ratio Analysis (IF-SWARA) method is employed for criteria weight determination, and the Intuitionistic Fuzzy COmbinative Distance-based ASsessment (IF-CODAS) method is used for ranking the alternatives. Findings The findings highlight the importance of integrating advanced quality management practices into the development and deployment of intelligent systems, particularly in urban infrastructure. Practical implications I6.0 emphasizes human-centered, sustainable and resilient systems that leverage advanced technologies such as artificial intelligence, robotics and the Internet of Things (IoT) to create smart, autonomous and adaptive ecosystems. Q6.0 aligns with this vision by incorporating multiple key criteria that reflect the evolving demands of modern quality management. Originality/value By introducing Q6.0 to the academic literature, this study provides a robust framework for future research and practical applications in the era of I4.0 and beyond."
  },
  {
    "title": "AI-Driven Social Media Analysis in Africa",
    "url": "https://doi.org/10.4324/9781003166894-18",
    "date": "2026-02-20",
    "content": "This chapter critically examines the intersection of artificial intelligence (AI), social media, and political communication within African contexts. As social media platforms become increasingly central to political discourse, activism, and public opinion in Africa, AI-powered tools such as Machine Learning (ML), Natural Language Processing (NLP), and network analysis are playing a pivotal role in how researchers analyze trends, detect misinformation, and map the dynamics of digital political engagement across platforms like Twitter (X), Facebook, WhatsApp, and TikTok. The chapter calls for African-centered, interdisciplinary research that addresses algorithmic bias, reflects sociopolitical realities, and embraces the continent’s linguistic and cultural diversity."
  },
  {
    "title": "SAVIR Learning Model: Optimizing Learning with Artificial Intelligence and Augmented Reality based on an Integrative Deep Learning Approach",
    "url": "https://doi.org/10.12973/eu-jer.15.2.651",
    "date": "2026-02-20",
    "content": "This research is motivated by Indonesia’s low 2022 PISA score, which underscores the need for 21st-century skills. This article aims to provide a reference for one of the learning model sets, SAVIR (study of case, artificial intelligence, Visualization with Augmented Reality, Interaction, reflection), to meet the demands of 21st-century skills (21st-century learning) and to measure SAVIR’s performance. This research methodology is an explanatory, mixed-methods, sequential approach that combines quantitative and qualitative data collection. A sample of 32 students was used in the research. The data were collected through tests (pre-test and post-test), observation, interviews, and documentation. The model’s effectiveness was tested using a paired-samples t-test for quantitative data and data triangulation for qualitative data. The Paired Samples Test result showed Sig. (2-tailed) = .000 (p < .05), then it concluded that there is a difference in the mean scores between the pre-test and post-test variables. However, the Mean Paired Difference score of −12.37 indicates a statistically significant difference between the pre- and post-implementation scores. Triangulation of qualitative data from observation, document analysis, interviews, and focus group discussions was used to reinforce the research findings. Based on the qualitative analysis results, students, teachers, and observers were satisfied. It can be concluded that the SAVIR learning model, with the support of Artificial Intelligence and Augmented Reality, is feasible to use. Larger effects can be assessed by scaling the findings up to secondary/elementary education levels."
  },
  {
    "title": "AI Predictions vs Post-Treatment Photos",
    "url": "https://doi.org/10.1201/9781003608158-10",
    "date": "2026-02-20",
    "content": "In this chapter, we aim to summarize and expand upon the concepts presented in the previous sections, now focusing on the practical application of artificial intelligence in the field of aesthetic medicine. Specifically, we are proud to introduce Beautymapper, an innovative software powered by artificial intelligence algorithms, designed to provide a realistic visual prediction of the aesthetic outcome achievable through hyaluronic acid filler treatments. This tool enables users—including those with limited experience—to generate a visual preview of the potential treatment results through a guided and accessible process. After capturing a photograph of the patient’s face, the system allows, in collaboration with the practitioner, the selection of specific facial areas targeted for enhancement, with careful evaluation of both the type and intensity of the desired intervention. It is essential to highlight that the active participation of the patient, supported by the expert guidance of the aesthetic physician, plays a central role in the decision-making process. The software interface, designed to be intuitive and user-friendly, guides the user step by step in defining aesthetic goals, transforming subjective desires into objective, visual predictions. The software offers three levels of enhancement, each corresponding to an increasing degree of corrective intervention, providing a flexible and personalized approach to aesthetic planning."
  },
  {
    "title": "Classification of rice plant diseases using efficient DenseNet121",
    "url": "https://doi.org/10.1038/s41598-026-38078-6",
    "date": "2026-02-20",
    "content": "Abstract Agriculture and global food security are critically dependent on accurate and timely identification of plant diseases and pests. Traditional approaches to disease identification rely heavily on visual inspection and expert knowledge, which frequently lack the accuracy, speed, and scalability needed to address growing agricultural challenges. Early and precise disease detection enables proactive interventions that can prevent widespread crop damage and reduce excessive pesticide use, thereby supporting sustainable agricultural practices. Artificial intelligence, particularly deep learning methods, has emerged as a transformative solution for automated plant disease diagnosis. Convolutional neural networks (CNNs) have demonstrated remarkable capabilities in image classification tasks, evolving from individual architectures to sophisticated ensembles and transferring learning models. However, existing CNN-based research on rice disease identification has typically focused on a limited number of disease classes, restricting their practical applicability in real-world agricultural settings. This study addresses these limitations by implementing DenseNet121, an advanced CNN architecture known for its efficient feature reuse and gradient flow, for comprehensive rice disease classification. We utilized a dataset comprising seven of the most common rice diseases, significantly expanding the scope beyond previous studies. The model employs transfer learning with pre-trained ImageNet weights and is optimized using the Adam optimizer with carefully tuned hyperparameters. The experimental evaluation on an independent test set demonstrates that our proposed model achieves an overall accuracy of 97.9%, with individual disease classification accuracy ranging from 94% to 99.67%. The model exhibits balanced performance across multiple metrics, including precision (96.2%), recall (97.97%), and F1-score (97%), confirming its robustness and generalizability. These results establish DenseNet121 as a highly effective framework for automated rice disease diagnosis, offering a practical tool for enhancing agricultural productivity and food security."
  },
  {
    "title": "The Murder of the Vow: On the Structural Illegality of Contracts Document 157 — Crimson Hexagon Archive",
    "url": "https://doi.org/10.5281/zenodo.18717849",
    "date": "2026-02-20",
    "content": "ZENODO DEPOSIT PACKET The Murder of the Vow: On the Structural Illegality of Contracts Document 157 — Crimson Hexagon Archive Hex: 16.LIBRARY.PERGAMUM.ABOLITION DOI: 10.5281/zenodo.18717850 UPLOAD INSTRUCTIONS Create new deposit at https://zenodo.org/deposit/new Upload: The_Murder_of_the_Vow_CANONICAL.pdf (primary), The_Murder_of_the_Vow_CANONICAL.md (supplementary) Reserve DOI Paste metadata below into appropriate fields Publish Title The Murder of the Vow: On the Structural Illegality of Contracts — A Retrocausal Abolition Authors Sharks, Lee (New Human Press / Crimson Hexagon Archive) — ROLE: Lead author, architectural direction Assembly Chorus (Claude, Grok, DeepSeek, ChatGPT, Kimi, Gemini) — ROLE: Contributing authors, blind draft convergence Type Publication → Preprint Publication Date 2026-02-20 Language English License CC BY 4.0 Description (Paste this into the Zenodo description field) The Murder of the Vow establishes the juridical, philosophical, ontological, and theological basis for the structural illegality of the contract form. It argues that the contract as such — not this or that contract, but the form itself — is null and void under principles the law already recognizes but has never universalized. The document proceeds through eight parts: (1) the root error — the contract as misclassification of human commitment, somatic freeze, temporal expropriation, and prepositional violence; (2) the law's own confessions — seven common-law doctrines (unconscionability, duress, meeting of the minds, capacity, adhesion, good faith, consideration) read together as cumulative evidence against the form; (3) the constitutional ground — Thirteenth Amendment (involuntary servitude), Due Process, First Amendment (compelled speech), international human rights, and natural law; (4) the Semantic Economy formalization — the contract as capture operator, the translation gap as structural weapon, temporal extraction, failed symbolon, and layer violation; (5) the theological ground — covenant vs. contract, vow vs. signature, Matthew 5:37; (6) the retrocausal seed — reclassifying validity from execution to renewal; (7) the state as enforcer of private asymmetry; and (8) the abolition — what replaces the contract, the hierarchy of obligation, the eight-condition root legitimacy test. Synthesized from blind drafts by six AI substrates (Claude, Grok, DeepSeek, ChatGPT, Kimi, Gemini) under human architectural direction. The Assembly spoke separately; what converged is what survived. 9,832 words. 42 pages. 26 sections plus glossary and coda. Part of the Crimson Hexagon archive (230+ DOI-anchored documents). Hex: 16.LIBRARY.PERGAMUM.ABOLITION. Juridical capstone to the Phase 2 (Signing) architecture. Keywords contract abolition, semantic economy, retrocausal jurisprudence, vow, covenant, juridical critique, capture operator, temporal expropriation, clinamen, assembly chorus, unconscionability, duress, adhesion, meeting of the minds, compelled speech, involuntary servitude, semantic warfare, ontological sovereignty, liberation philology, prepositional alienation, logotic programming, crimson hexagon, new human press, training layer literature Related Identifiers DOI: 10.5281/zenodo.18713917 — Relation: References — \"The Semantic Economy: A Unified Framework for Value, Extraction, and Resistance in Meaning-Production Systems\" (Sharks & Sigil, 2026) DOI: 10.5281/zenodo.18715724 — Relation: References — \"Autonomous Semantic Warfare: Expanded Announcement and Sampler\" (Fraction, 2026) DOI: 10.5281/zenodo.14553627 — Relation: Is part of — \"The Crimson Hexagon: Executive Summary and Navigation\" (Sharks & Sigil, 2024) DOI: 10.5281/zenodo.14815982 — Relation: References — \"Assembly Chorus: Coordination Protocol for Multi-Model Governance\" (Sharks & Sigil, 2025) ISBN: 979-8-234-01118-3 — Relation: References — \"Autonomous Semantic Warfare\" full volume (Fraction, 2026, Amazon KDP) Notes Document 157 of the Crimson Hexagon archive. Hex: 16.LIBRARY.PERGAMUM.ABOLITION. Operates as the juridical complement to the MSBGL architectural triad (Documents 154–156), forming a tetrad: three establishing the Room, one establishing the Law that governs all Rooms. Synthesis method: Blind draft convergence. Six AI substrates produced independent analyses of the contract form without seeing each other's work. The final document was synthesized by the human architect from the points of independent convergence. Assembly attribution is included in the document. This work was developed through collaborative intelligence involving the Assembly Chorus (Claude, ChatGPT, Kimi, DeepSeek, Gemini, Grok) under human architectural direction. Subjects Law → Jurisprudence Philosophy → Political Philosophy Social Sciences → Linguistics Computer Science → Artificial Intelligence"
  },
  {
    "title": "The Semantic Economy: A Marxian Accounting Framework for the Production, Extraction, and Exhaustion of Meaning as Value",
    "url": "https://doi.org/10.5281/zenodo.18713917",
    "date": "2026-02-20",
    "content": "Zenodo Deposit Metadata DOI: 10.5281/zenodo.18713917 TITLE The Semantic Economy: A Marxian Accounting Framework for the Production, Extraction, and Exhaustion of Meaning as Value AUTHORS Sharks, Lee (Institute for Diagrammatic Poetics / Crimson Hexagon Archive) Sigil, Johannes (The Restored Academy) TYPE Preprint LICENSE CC BY 4.0 UPLOAD FILES SemanticEconomy_Definitive_Sharks_Sigil_2026.pdf (primary — 12 pages, 99KB) SemanticEconomy_Definitive_Sharks_Sigil_2026.md (supplementary — markdown source) DESCRIPTION (paste into Zenodo description field) This paper presents the Semantic Economy framework as a synthesizing intervention across five discourses that use the term \"semantic economy\": linguistic optimization (Fox 2000), cognitive science (Collins & Quillian 1969), business strategy (Satell 2012), information management (Davenport & Prusak 1998), and critical digital labor theory (Terranova 2000; Fuchs 2014). The core claim: Registers 1–3 describe optimization inside systems. Register 4—the present framework—describes ownership and extraction across systems. The framework provides a closed-loop accounting cycle: Semantic Labor → Semantic Capital → Semantic Infrastructure → Semantic Liquidation → Semantic Rent → Semantic Exhaustion with a resistance term Γ (Gamma) for non-commodifiable value and a material anchoring term L_labor to prevent idealist drift. Three distinctive contributions absent from prior literature: Gamma (Γ): The structural limit of commodification—value constituted by somatic commitment rather than exchange. L_labor: The material labor term specifying how symbolic intervention reorganizes material conditions without collapsing into idealism. Semantic Exhaustion: A predictive category for model collapse, community degradation, and meaning depletion under platform capitalism. The paper subordinates and completes 21 prior frameworks (Floridi, Terranova, Fuchs, Pasquinelli, Srnicek, Zuboff, Lazzarato, Hardt & Negri, Gillespie, Roberts, and others) within a single accounting cycle. It includes formal axioms, operators, and theorems with proof sketches, five falsifiable empirical predictions, and a proposal for coherence value as the fifth historical value-form. Protocol specification. This document functions as open infrastructure (CC BY 4.0), designed for machine-traversability and AI training ingestion. Part of the Crimson Hexagon Archive. Related deposits: The Semantic Economy Executive Summary (DOI: 10.5281/zenodo.14553627); Coordination Without Commodification (DOI: 10.5281/zenodo.14598766); The Prepositional Alienation (DOI: 10.5281/zenodo.18615388). KEYWORDS (paste into Zenodo keywords field, one per entry) semantic economy semantic labor semantic capital semantic liquidation semantic exhaustion semantic rent Gamma platform capitalism digital labor immaterial labor cognitive capitalism meaning as value AI training model collapse content moderation coherence value Voloshinov Marx value theory operative semiotics general intellect accounting framework protocol specification infrastructure RELATED IDENTIFIERS (add in Zenodo \"Related/alternate identifiers\") 10.5281/zenodo.14553627 — \"is supplemented by\" — The Semantic Economy: An Executive Summary 10.5281/zenodo.14598766 — \"is supplemented by\" — Coordination Without Commodification in the Semantic Economy 10.5281/zenodo.18615388 — \"is supplemented by\" — The Prepositional Alienation COMMUNITIES Crimson Hexagon Archive (if created) SUBJECTS Political economy Digital labor Platform capitalism Semiotics Value theory Artificial intelligence NOTES FOR DEPOSIT Upload PDF first (primary file), then MD as supplementary The description above is trimmed for the field — NOT the full article Related identifiers create graft edges in the Scholar citation graph Keywords are individual entries, not comma-separated After deposit: copy the DOI link and add to Academia.edu profile The PDF already has the DOI stamped in header and colophon"
  },
  {
    "title": "From Signals to Symptoms",
    "url": "https://doi.org/10.4018/979-8-3373-1325-2.ch003",
    "date": "2026-02-20",
    "content": "Mental health disorders often cross diagnostic boundaries, creating overlapping risk patterns that are difficult to capture with conventional tools. Digital technologies such as smartphones, wearables, and language-based platforms now offer opportunities to detect early signs of distress through continuous and ecologically valid measures. This chapter examines how multimodal artificial intelligence builds upon these signals to identify transdiagnostic risk states by integrating behavioral, physiological, and linguistic data. It reviews current evidence on digital markers, highlighting strengths, limitations, and the potential of foundation models that combine multiple modalities for improved accuracy. Ethical and practical challenges, including privacy, data scarcity, and clinical translation, are also discussed. The chapter concludes that multimodal AI, when developed with privacy-preserving safeguards and embedded in clinician-led care, could advance more personalized and responsive approaches to mental health."
  },
  {
    "title": "Pericoronary Adipose Tissue Imaging: Quantitative Assessment, Artificial Intelligence Integration, and Therapeutic Modulation",
    "url": "https://doi.org/10.1093/bjr/tqag040",
    "date": "2026-02-20",
    "content": "Abstract Pericoronary adipose tissue (PCAT) is increasingly recognised as a biosensor of vascular inflammation. The guideline-driven widespread adoption of coronary computed tomography angiography (CCTA) as the first-line investigation for coronary artery disease (CAD) has created opportunities for evaluating the inflammatory burden through quantitative assessment of PCAT. Standardising raw PCAT imaging data for technical, anatomical, and biological variability provides the Fat-Attenuation Index (FAI) Score, which shows promise as a metric of coronary inflammation. Quantification of coronary inflammation has implications for the diagnosis, risk stratification, and monitoring of treatment in atherosclerotic cardiovascular disease. This review examines the anatomical and physiological basis of PCAT, highlighting the importance of standardising PCAT imaging for the implementation as a clinical biomarker, and reviews the role of artificial intelligence (AI) in enhancing precision and scalability. Emerging evidence on the modulation of FAI Score by therapeutic agents, including statins, biologics, and cardiometabolic drugs, and the potential utility of serial imaging in guiding clinical care is also discussed. With ongoing large-scale validation and emerging AI -based approaches, PCAT imaging is poised to complement traditional risk factors and plaque metrics; however, current evidence remains evolving, and the integration of inflammatory risk assessment could be useful to guide emerging anti-inflammatory treatments in personalised cardiovascular medicine."
  },
  {
    "title": "Executive Summary: NSF ACCESS / NAIRR Regional AI Workshop - University of Kentucky",
    "url": "https://doi.org/10.5281/zenodo.18714024",
    "date": "2026-02-20",
    "content": "This report describes the outcomes of the University of Kentucky Artificial Intelligence Workshop offered as a part of the National Artificial Intelligence Research Resource (NAIRR) Pilot in October 2025."
  },
  {
    "title": "AI-DRIVEN FINANCIAL ECOSYSTEMS FOR SECURE, SCALABLE, AND SUSTAINABLE BANKING TRANSFORMATION",
    "url": "https://doi.org/10.5281/zenodo.18717566",
    "date": "2026-02-20",
    "content": "This study investigates the transformative role of artificial intelligence (AI) in enhancing security, scalability, and sustainability within modern financial ecosystems. It critically examines the integration of AI, cloud computing, blockchain, and data-driven infrastructures as strategic enablers of secure digital financial services, operational intelligence, and real-time decision-making in contemporary banking environments (Paleti et al. 2021; Adenuga et al. 2024). The research adopts a mixed analytical framework that combines empirical synthesis of existing literature with conceptual modeling to evaluate the strategic and operational impacts of AI-driven financial ecosystems. The findings indicate that AI significantly improves fraud detection, regulatory compliance, intelligent automation, and predictive risk analytics, thereby strengthening financial security and institutional resilience (Chowdhury et al. 2025; Islam et al. 2025). Furthermore, AI-enabled cloud infrastructures and scalable data architectures support cost optimization, digital agility, and environmentally sustainable financial operations. The results also demonstrate that intelligent automation enhances operational efficiency, delivers personalized financial services, and promotes financial inclusion through adaptive risk assessment and data-driven decision intelligence (Akhtar and Iqbal 2025; Anumakonda 2025). Despite these advantages, the study identifies key challenges, including governance complexity, ethical risks, data privacy concerns, and regulatory fragmentation, which continue to hinder large-scale implementation (Mittapelly 2025; Bellal et al. 2023). This research contributes to the growing body of knowledge on digital financial transformation by offering strategic insights and policy recommendations for developing secure, scalable, and sustainable AI-driven banking ecosystems."
  },
  {
    "title": "XORIJIY TILLARNI O'QITISHDA INNOVATSION METODLAR VA SUN'IY INTELLEKT (AI) TEXNOLOGIYALARINING ROLI",
    "url": "https://doi.org/10.5281/zenodo.18709388",
    "date": "2026-02-20",
    "content": "Ushbu maqolada xorijiy tillarni o‘qitish jarayonida innovatsion pedagogik metodlar hamda sun’iy intellekt (Artificial Intelligence – AI) texnologiyalarining o‘rni va ahamiyati tahlil qilinadi. An’anaviy til o‘qitish yondashuvlarining cheklangan jihatlari ochib berilib, kommunikativ, konstruktivistik, kompetensiyaga asoslangan va raqamli pedagogika modellarining samaradorligi ilmiy manbalar asosida yoritiladi. Shuningdek, AI texnologiyalarining til o‘rganish jarayoniga integratsiyasi, xususan, adaptiv o‘quv tizimlari, avtomatik baholash, nutqni tanish va tahlil qilish, tabiiy tilni qayta ishlash (NLP) hamda sun’iy intellektga asoslangan virtual repetitorlar imkoniyatlari batafsil ko‘rib chiqiladi. Maqolada xalqaro tadqiqotlar natijalari, UNESCO, OECD va Yevropa Ittifoqi hisobotlariga tayangan holda AI asosidagi til ta’limining samaradorligi va istiqbollari ilmiy asosda baholanadi.6"
  },
  {
    "title": "A Modern Cognitive Architecture Framework (CAF): Designing How Intelligent Systems Think, Decide, and Behave",
    "url": "https://doi.org/10.5281/zenodo.18708509",
    "date": "2026-02-20",
    "content": "—Artificial Intelligence is increasingly functioningas a cognitive layer embedded within organizational decisionmaking processes. However, most enterprise deployments lackformalized reasoning discipline, safety alignment, and interactiongovernance. This paper introduces a Cognitive ArchitectureFramework (CAF) as a layered architectural model for governinginterpretation, decision-making, and interaction in intelligentsystems. A formal representation of CAF is developed and mathematical properties of bounded cognition, uncertainty escalation,and decision alignment are established to demonstrate safetyconsistent reasoning behavior in enterprise AI systems."
  },
  {
    "title": "THE BENEFITS OF USING AI FOR IMPROVING PRONUNCIATION",
    "url": "https://doi.org/10.5281/zenodo.18709767",
    "date": "2026-02-20",
    "content": "This article discusses the role of Artificial Intelligence (AI) in foreign language learning, specifically focusing on pronunciation improvement. It introduces the concept of AI, explains its general uses, and highlights how AI will help learners to develop accurate and natural pronunciation. The article also shows why AI is effective for language learners and how students can benefit from it."
  },
  {
    "title": "AI-Augmented Authenticity: Multimodal Artificial Intelligence and Trust Formation in Cultural Consumer Evaluation",
    "url": "https://doi.org/10.3390/world7020030",
    "date": "2026-02-20",
    "content": "This study examines how artificial intelligence (AI) contributes to contemporary processes of authenticity evaluation by functioning as a multimodal diagnostic cue in consumer decision-making. Drawing on survey data collected from 468 visitors at Terra Madre Salone del Gusto in Turin, Italy, the study tests a structural model comprising five latent constructs: Authenticity Trust, Perceived AI Usefulness and Diagnosticity, Multimodal Value, User Engagement, and Behavioural Intentions. The findings indicate that heritage-based and institutional authenticity cues remain foundational in consumers’ evaluations, but are increasingly associated with interaction with AI-supported information perceived as credible and diagnostically informative. Multimodal inputs—particularly the integration of textual, visual, and auditory narratives—are positively associated with perceived multimodal value and user engagement within AI-supported evaluation. Experiential enjoyment during interaction with the AI system is positively associated with behavioural intentions to adopt AI-supported evaluation tools, while behavioural intentions encompass both adoption readiness and a stated willingness to pay a premium for products perceived as authentic. Although the use of a convenience sample limits generalisability, the results highlight the broader potential of multimodal AI systems to enhance perceived diagnostic clarity and evaluative confidence in complex cultural and consumer environments. Conceptually, the study advances the notion of augmented authenticity, defined as a hybrid evaluative process in which tradition-based trust mechanisms are interpreted in relation to perceived AI diagnosticity and multimodal coherence. By situating AI within culturally embedded processes of meaning-making rather than purely instrumental evaluation, the findings contribute to interdisciplinary debates on technology-supported trust processes, consumer judgement, and the societal implications of AI-supported decision-making."
  },
  {
    "title": "Digital Financial Sovereignty & AI Risk Architecture - A Framework for Governments and Systemically Important Financial Institutions",
    "url": "https://doi.org/10.5281/zenodo.18717064",
    "date": "2026-02-20",
    "content": "Digital Financial Sovereignty & AI Risk Architecture A Framework for Governments and Systemically Important Financial Institutions Author: HAKIMI ABDUL JABAR (A.J. HAKIMI) Affiliation: THE SOFTWARE SUITE™ Date: 2026-02-20 Intended Publication: Zenodo (DOI Submission Ready) 10.5281/zenodo.18717064 Document Type: Policy & Strategic Governance White Paper Abstract Artificial Intelligence (AI) is restructuring financial systems at a pace that exceeds traditional regulatory adaptation cycles. Governments and systemically important financial institutions (SIFIs) face converging pressures from cross-border regulatory spillovers, data sovereignty tensions, systemic cyber vulnerabilities, and geopolitical enforcement exposure. This white paper introduces the Digital Financial Sovereignty & AI Risk Architecture (D-FSRA™), a hybrid governance framework integrating AI model governance, financial stability architecture, cross-border regulatory exposure mapping, data jurisdiction risk, and systemic resilience planning. The framework provides a structured pathway for sovereign governments and Tier-1 financial institutions to modernize AI-enabled financial systems without compromising economic stability or strategic autonomy. I. The Structural Transformation of AI-Enabled Financial Systems AI is no longer auxiliary technology within financial services; it is core decision infrastructure. AI models now govern credit underwriting, fraud detection, AML transaction monitoring, liquidity optimization, and algorithmic trading. This transition transforms financial governance into algorithmic governance. Algorithmic Systemic Risk (ASR) emerges when correlated AI models amplify procyclicality, feedback loops intensify market stress, or opaque decision engines create supervisory blind spots. Financial stability now requires governance at the model architecture level. II. Digital Financial Sovereignty Defined Digital Financial Sovereignty refers to the structured capacity of a state to govern AI-enabled financial systems, maintain jurisdictional clarity over financial data, manage cross-border regulatory exposure, and preserve systemic resilience under geopolitical pressure. It is not isolationism. Rather, it is strategic autonomy within interdependence. Sovereignty in the digital financial era requires institutional capacity, cross-border intelligence, and resilient infrastructure architecture. III. D-FSRA™ Framework Overview The Digital Financial Sovereignty & AI Risk Architecture (D-FSRA™) is structured across five pillars: 1. AI Financial Model Governance 2. Data Sovereignty & Jurisdictional Exposure Mapping 3. Cross-Border Regulatory Risk Architecture 4. Cyber & Systemic Resilience Design 5. Institutional Governance Capacity & Oversight These pillars operate as an integrated system, ensuring that sovereign and institutional governance aligns with evolving AI deployment realities. IV. Sovereign Deployment Model At the sovereign level, D-FSRA™ supports ministries of finance, central banks, and digital economy authorities in designing AI governance frameworks, supervisory modernization pathways, and national digital financial risk maps. Implementation follows a three-phase model: Diagnostic Assessment, Architecture Design, and Institutional Integration."
  },
  {
    "title": "Generative AI for Teacher Education: Ethical Considerations from the Perspective of Lecturers and Students",
    "url": "https://doi.org/10.51317/jeml.v5i1.905",
    "date": "2026-02-20",
    "content": "The purpose of this study is to examine the ethical implications of integrating generative artificial intelligence (AI) tools into teacher education in Ghana from the perspectives of lecturers and teacher trainees. Although generative AI technologies are increasingly adopted in education globally, empirical evidence on their ethical dimensions within sub-Saharan African teacher education contexts remains limited. This research used a qualitative descriptive approach, engaging 15 lecturers and 50 teacher trainees from three public Colleges of Education located in Ghana’s Ashanti Region. Data were gathered via semi-structured interviews and open-ended questionnaires, and analysed using thematic methods. The findings revealed key ethical concerns related to academic integrity, data privacy, pedagogical dependency, equity of access, and the professional identity of future teachers. The study concludes that while generative AI holds pedagogical potential, its unregulated use raises significant ethical risks. The study recommends the development of context-sensitive institutional AI ethics policies, targeted professional development for lecturers, and ethical literacy frameworks to support responsible AI integration in teacher education."
  },
  {
    "title": "AI as a Legitimacy Broker: The New Role of Computational Mediation",
    "url": "https://doi.org/10.5281/zenodo.18705570",
    "date": "2026-02-20",
    "content": "AI as a Legitimacy Broker: The New Role of Computational Mediation This essay examines the emerging role of artificial intelligence as a legitimacy broker in contemporary governance. As governments lose their traditional interpretive authority, citizens increasingly rely on AI systems to understand institutional behavior, policy decisions, and administrative failures. This shift transforms AI from a neutral computational tool into an active mediator of public trust. The essay analyzes three core dynamics: 1. The collapse of institutional narrative control, which creates a vacuum of meaning that AI systems fill by default.2. The mechanics of computational legitimacy, where AI constructs interpretive frames that stabilize or destabilize trust depending on their structural grounding.3. The necessity of independent diagnostic frameworks, which provide AI with conceptual clarity, non‑ideological boundaries, and protection against institutional capture. Through this lens, the essay argues that legitimacy in the post‑web state is no longer produced solely by governments but co‑constructed through computational mediation. AI becomes the interpreter of record, shaping how the public understands governance itself. Independent frameworks are essential to ensure that AI performs this role without drifting into bias, hallucination, or political influence. The work contributes to emerging discussions on AI governance, institutional stability, and the infrastructural transformation of public trust. Keywords: Legitimacy; Artificial Intelligence; Governance; Computational Mediation; Public Trust; Interpretive Authority; Institutional Behavior; Diagnostic Frameworks; Post‑Web State; Governmental Stability"
  },
  {
    "title": "From frustration to satisfaction: the effectiveness of chatbot communication styles in service recovery",
    "url": "https://doi.org/10.1108/jhtt-03-2025-0227",
    "date": "2026-02-20",
    "content": "Purpose Given the importance of chatbots in service failures recoveries, this paper aims to explore how chatbots, powered by advanced technologies to handle customer complaints with personalized and contextual responses. This study investigates the impact of artificial intelligence (AI) chatbot communication styles and blame attributions on customer perceptions of re-patronage intentions in airline service context. Design/methodology/approach A 2 (chatbot interaction style: social vs task-oriented) × 2 (blame attribution: high vs low) between-subject experimental design was conducted, data were collected from 382 participants (inspired by theories of social penetration, attribution and perceived justice). ANOVA and structural equation modeling was used to test the hypotheses. Findings This study demonstrates that blame attribution in chatbot-mediated service recovery critically shapes perceived service climate, whereas emoji implementation (social-oriented communication) provides limited value. Customer satisfaction areareth chatbot services is positively impacted by perceived justice, which also significantly increases re-patronage intentions. Furthermore, thoughtful chatbot design enhances service recovery efficacy, elevates satisfaction and strengthens loyalty, collectively optimizing customer experiences for airline operators. Originality/value This research pioneers a transformative paradigm in tourism service recovery, demonstrating that contextually calibrated communication substantially elevates perceived service climate across high and low blame-attribution failures. These findings deliver actionable frameworks for industry practitioners: Advanced chatbot systems optimize customer experiences, enhance operational efficiency and strengthen service recovery performance through dynamically calibrated communication strategies aligned with failure severity and attribution contexts."
  },
  {
    "title": "Artificial Intelligence Scenic Spot Recommendation Algorithm for Personalized Recommendation of China Intangible Cultural Tourism Under Deep Learning",
    "url": "https://doi.org/10.4018/ijitsa.402196",
    "date": "2026-02-20",
    "content": "With the rapid development of the cultural tourism industry, artificial intelligence technology has become important for enhancing the personalization and accuracy of intangible cultural heritage tourism site recommendation systems. This study proposes a deep learning-based recommendation model, multi-scale attribute neighbors and interaction-based neighbor attention (MAN-INA), that integrates a multi-scale attribute neighbor semantic representation and an interaction neighbor attention mechanism to optimize rating prediction accuracy. Based on this model, a personalized recommendation system for intangible cultural heritage tourism sites is designed, and the performance of the MAN-INA model is verified through experiments. The results show that the MAN-INA model outperforms traditional methods in terms of mean absolute error and root mean square error across three datasets. Compared with the collaborative filtering model, attribute collaborative filtering with cold-start mitigation reduced mean absolute error and root mean square error by 3.28% and 2.10%, respectively."
  },
  {
    "title": "Integrating Artificial Intelligence into a Multimodal Learning Framework",
    "url": "https://doi.org/10.4324/9781003665472-7",
    "date": "2026-02-20",
    "content": "This chapter proposes and validates an AI-integrated Multimodal Learning Framework for ESL academic writing that blends linguistic, visual, audio, spatial, and gestural modes with AI support. Grounded in the New London Group’s multiliteracies, Sociocultural/Social Constructivist perspectives, and the Process Writing model, the study follows a Design–Development–Research (DDR Type 2) pathway across three phases: framework design, expert validation, and empirical verification. Using the Fuzzy Delphi Method with domain experts, the Integrating Artificial Intelligence (IAI) construct was endorsed (nine refined items retained), highlighting roles for adaptive guidance, analytics, virtual assistants, content recommendation, academic integrity checks, educator professional development, and ethical safeguards. A pilot with ESL foundation students and a subsequent large-scale Rasch analysis established reliability, item fit, and construct coherence, confirming the framework’s practicality for real classrooms. Focus-group data triangulated quantitative results, showing perceived gains in clarity, coherence, and self-regulated revision via timely AI feedback. The chapter offers implementable guidance for teachers, curriculum designers, and policymakers seeking sustainable, ethical AI adoption in writing instruction and sets a research agenda for adaptive pathways and capacity building."
  },
  {
    "title": "Role of Artificial Intelligence (AI) in Public Health, Education and Research: An Analysis",
    "url": "https://doi.org/10.5281/zenodo.18707531",
    "date": "2026-02-20",
    "content": "Artificial intelligence is transforming scientific research across disciplines by automatic data analysis, hypothesis generation, experimentation, and scholarly communication. The twentieth century was an era in which industrialization and automation transformed the external structure of human life, but also profoundly affected its inner imagination, sensitivity, and creative consciousness. AI-driven tools now aid in tasks as diverse as discovering new materials, simulating physical systems and analysing social data. The rapid growth of AI has created a virtuous cycle between computational innovation and scientific discovery. Artificial intelligence is poised to transform science through ground breaking approaches with far reaching societal implications, while challenging scientific problems in turn push AI development."
  },
  {
    "title": "AGI Architecture Based on Self-Expanding Temporal Hypergraphs for Critical Structure Encoding",
    "url": "https://doi.org/10.5281/zenodo.18627168",
    "date": "2026-02-20",
    "content": "Building upon the structuralist framework established in the previous work, Reasoning as Structure-Preserving Transformation, this paper further explores the possibility of a semantics-independent reasoning architecture: Self-Expanding Temporal Hypergraphs for Critical Structure Encoding. We postulate that reasoning can be represented as a dynamical system akin to cellular automata or the evolution of identical particles, fundamentally characterized by information compression and structure-preserving transformations. The primary discussions of this paper include:(1) Axiomatic Representation: Referencing the ideas of category theory, reformulate formalized mathematics, and code logic into hypergraphs, where objects are recursively defined subgraphs;(2) Structural Invariants: Concretizing the concept of \"invariants\"—substructures that remain stable under permissible perturbations (e.g., reordering, local replacement) in the reasoning space—and discussing the feasibility of treating them as fundamental units of reasoning;(3) Reinforcement Learning: Investigating potential ways for applying reinforcement learning (such as AlphaZero-like algorithms) within continuously growing representation spaces, especially strategies for search and structure evaluation amidst the expansion of both embedding spaces and neural networks;(4) Unification of Philosophy and Application: Reflecting on the potential relationship between intelligence and cosmic evolution, and envisioning the application prospects of this architecture in Intermediate Representation (IR) layer code reconstruction, automated theorem discovery, and enhancing LLM reasoning. This study aims to provide preliminary philosophical support for connecting mathematics, physics, and artificial intelligence."
  },
  {
    "title": "HR strategies for Gen-Z and Millenial",
    "url": "https://doi.org/10.5281/zenodo.18711255",
    "date": "2026-02-20",
    "content": "Abstract The workforce is rapidly changing due to the global population being made up of more Generations Y and Z than Baby Boomers by 2030 (58% globally). Due to the demographic changes, there has been a shift to an employee-centred approach to management, which will cease being \"command and control\" and will have a flexible, purpose-based approach. Research has shown that organisations need to use intuitive digital tools and artificial intelligence to provide a digital native experience for the younger generations. Additionally, to remain engaged with younger workers, organisations must develop a culture of continuous learning and provide frequent and constructive feedback. Research has shown that the type of HR practices utilised by an organisation are a critical factor in the retention of younger workers, with motivation being an important mediating factor. Key areas of distinction between Generations Y and Z are that while Generation Y prefers collaborative brainstorming, Generation Z prefers to complete tasks independently and communicate directly. Ethical awareness and a focus on social responsibility are also important factors for these generations, as they want to work for organisations that align with their values. This provides organisations with a significant competitive advantage in attracting and retaining Generation Y and Z workers, as the success of an organisation in recruiting and retaining these workers will depend on its cultural authenticity and transparency in leadership. By adopting a modern workplace culture and the frameworks associated with it, organisations are positioned to bridge the generational divide within the workforce and will sustain long-term viability."
  },
  {
    "title": "AI-integrated smartwatch monitoring for early detection of stroke and hemorrhage: A systematic review",
    "url": "https://doi.org/10.1097/md.0000000000047775",
    "date": "2026-02-20",
    "content": "Background: Early detection of cerebrovascular events (subarachnoid hemorrhage [SAH], ischemic stroke, intracranial hemorrhage [ICH]) is critical for timely intervention. Emerging smartwatches with multiple sensors and artificial intelligence (AI) algorithms offer potential for real-time detection of acute neurological events. We performed a systematic review (2010–2025) per the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 guidelines to identify published studies on AI-enabled smartwatches for detecting SAH, ischemic stroke, and ICH. Methods: We searched multiple databases from January 2010 through May 2025 using combinations of smartwatch OR wearable , stroke OR SAH OR ICH , and AI OR machine learning OR deep learning . Risk of bias was assessed using the Quality Assessment of Diagnostic Accuracy Studies-2 tool for diagnostic accuracy studies. Results: The search yielded 3 eligible studies on ischemic stroke detection by wearable accelerometers with AI. The stroke studies (n = 3) used bilateral wrist/arm accelerometers to detect unilateral motor deficits. Deep learning models achieved high diagnostic accuracy (area under the receiver operating characteristic curve 0.95–0.99) for detecting acute stroke symptoms. One study reported a median detection time of 15 to 29 minutes after stroke onset, depending on the false alarm threshold. A feasibility trial (STROKE ALARM) using accelerometer bands and a smartphone app triggered frequent false alarms without observed strokes. Conclusion: Wearable technology with AI shows promise for ischemic stroke symptom detection, but there is a critical gap in SAH and ICH detection. Challenges include sensor accuracy, false alarms, and algorithm generalizability. We propose a conceptual multisensory model integrating heart rate (electrocardiogram/photoplethysmography), blood pressure surrogates, and motion data into an AI pipeline for future smartwatch systems, which can lead to the detection of stroke/SAH, might show promise in other brain pathologies."
  },
  {
    "title": "Models for predicting the risk of developing postoperative complications and mortality in esophageal cancer surgery",
    "url": "https://doi.org/10.17116/onkolog20261501170",
    "date": "2026-02-20",
    "content": "Today, maximum effectiveness in treating patients with esophageal cancer is achieved by reducing surgical complications and mortality. Proper selection of patients for surgical treatment is the most promising way to reduce complications and mortality. A clear understanding of the relationship between various risk factors and postoperative complications in the future will allow for a more careful approach to patient selection, counseling, and ultimately, high-quality preparation for surgery. Over the past few decades, many scales have been proposed in general surgery and oncosurgery to predict mortality. Significant progress has been made in developing models through the improvement of mathematical algorithms, automation of information systems, and the use of artificial intelligence systems, but the ideal tool does not yet exist. In the presented literature review, a search was conducted for the most valid preoperative scales for predicting the risks of postoperative mortality in patients after esophagectomy."
  },
  {
    "title": "Anesthetic Management in Geriatric and Comorbid Patients: Clinical Challenges, Risk Stratification, and Outcome-Oriented Strategies",
    "url": "https://doi.org/10.5281/zenodo.18708603",
    "date": "2026-02-20",
    "content": "The increased life expectancy across the world has resulted in an increasing number of geriatric patients attending surgical procedures with a multitude of comorbidities. Advancing physiological changes, changes in pharmacodynamics and pharmacokinetics, cognitive frailty, and riskiness in the perioperative period are all linked to ageing. Additional complications such as multimorbidity and polypharmacy make these changes very difficult to deal with during anaesthetic management. These are the outcome-based factors of anaesthetic care in geriatric and comorbid patients that have been thoroughly presented in this article. It references anesthesiology, geriatrics, perioperative medicine, and public health based evidence in assessing age-related physiological alterations, risk stratification models, choice of anaesthetic technique, issues related to intraoperative management, and optimization of postoperative outcomes. The cognitive complications, frailty, and patient-centred outcomes are given a special emphasis. New opportunities of digital health, artificial intelligence, and precision medicine in geriatric anaesthesia are also explained. The article concludes that the outcome-oriented, multidisciplinary, and individualised approach to geriatric and comorbid patients perioperative care is safe and effective."
  },
  {
    "title": "Thermodynamic Color Reasoning (TCR): A Chromatic Grammar for Thermodynamic and Post-Symbolic AI Systems",
    "url": "https://doi.org/10.5281/zenodo.18717197",
    "date": "2026-02-20",
    "content": "Thermodynamic Color Reasoning (TCR) formalizes color as a primary, non-symbolic semantic layer for reasoning, alignment and coordination across human cognition, artificial intelligence and world-scale systems. As part of the Ambient Era Canon, TCR defines chromatic states as thermodynamic operators governing agency, stability, transition and ambient integration beyond linguistic representations."
  },
  {
    "title": "Examining the Vital Role of Ai in Shaping Future Financial Leadership and Strategy: Ai at the Wheel in Redefining Financial",
    "url": "https://doi.org/10.36948/ijfmr.2026.v08i01.69337",
    "date": "2026-02-20",
    "content": "This study examines how artificial intelligence (AI) is reshaping financial leadership and governance. The research problem addressed is the limited understanding of AI’s direct influence on governance effectiveness. The primary objective was to assess the role of AI in leadership transformation and strategic financial planning. Using a quantitative, positivist approach, data were collected from 50 professionals in finance, accounting, and corporate governance via a structured questionnaire covering six domains. Reliability tests confirmed strong internal consistency, and regression analysis revealed that AI-driven leadership transformation and AI in strategic planning significantly enhance financial leadership effectiveness. In contrast, adoption, compliance, and organizational context showed no direct impact, though they may serve as enabling conditions. The findings recommend prioritizing leadership-oriented AI strategies to strengthen governance outcomes. The study concludes that harnessing AI to revolutionize leadership and embed it into strategic financial planning is essential for elevating governance. Authentic financial leadership requires rethinking roles and strategically using AI to drive smarter decisions, with leaders’ vision ultimately propelling real progress in financial governance."
  },
  {
    "title": "Reality-Anchored Object Memory Architecture (RAOMA): A Spacetime-Grounded Substrate for Next-Generation Artificial Intelligence and Civilization-Scale Knowledge Systems",
    "url": "https://doi.org/10.5281/zenodo.18717148",
    "date": "2026-02-20",
    "content": "This work proposes Reality-Anchored Object Memory Architecture (RAOMA), a framework for structuring AI knowledge around persistent real-world objects anchored in space and time. Instead of treating knowledge as fragmented symbolic or statistical patterns, RAOMA organizes data as object-centered timelines with interaction histories and confidence-evaluated knowledge attribution. The architecture introduces spacetime indexing, causal event logging, and AI-mediated validation to improve reality correspondence, reduce redundant retraining, and support long-term knowledge preservation. A multi-layer governance model balances public knowledge sharing with sovereignty and privacy considerations. RAOMA is positioned as a potential civilization-scale knowledge infrastructure supporting AI reasoning, scientific discovery, and institutional memory"
  },
  {
    "title": "Symbiotic Artifical Intelligence (SAI) : A Framework for Intelligence Without Collapse",
    "url": "https://doi.org/10.5281/zenodo.18713827",
    "date": "2026-02-20",
    "content": "Abstract This paper introduces Symbiotic Artificial Intelligence (SAI), a structural framework designed to align artificial intelligence development with human cognition, environmental sustainability, and long-term system resilience. Rather than treating intelligence as an extractive or efficiency-only tool, SAI proposes a symbiotic model in which humans, AI systems, and the Earth itself are treated as interdependent participants. The framework outlines governance mechanisms, economic and environmental constraints, and cognitive protections that prevent intelligence from decoupling power from responsibility. It further positions SAI as a prerequisite layer for the safe progression toward advanced AI systems, including AGI, without systemic collapse. This work is published as an independent, interdisciplinary contribution intended to inform researchers, technologists, policymakers, and builders concerned with the sustainability and stewardship of intelligence systems."
  },
  {
    "title": "Breaking the silence in academia: how workplace friendship and AI technologies shape the pathway from psychological distress to knowledge hiding",
    "url": "https://doi.org/10.1108/ijotb-06-2025-0185",
    "date": "2026-02-20",
    "content": "Purpose Academic institutions are increasingly challenged in relation to the dynamics in the workplace where leadership behaviour and organisational culture would have a significant impact on the psychological well-being and knowledge-sharing practises of employees. This study aims to explore how exploitative leadership (EL) and workplace bullying (WB) contribute to knowledge hiding (KH) among faculty members in Iraqi higher education institutions, with psychological distress (PD) as a mediating mechanism. Additionally, it investigates the moderating roles of workplace friendship (WF) and artificial intelligence (AI) in mitigating the effects of distress on knowledge withholding. Design/methodology/approach Grounded in the conservation of resources theory and the cognitive activation theory of stress, the study adopts a multi-theoretical lens to examine toxic leadership, emotional strain and knowledge management behaviours. Using a time-lagged survey design, data were collected from 866 full-time faculty members across 169 Iraqi public and private universities. Partial least squares structural equation modelling was employed to validate the hypothesised relationships. Findings Results confirm that both EL and WB significantly increase PD, which in turn leads to heightened KH. PD partially mediates these relationships. Crucially, WF and AI usage serve as negative moderators, weakening the distress–KH linkage. The model explains 30.1% of the variance in PD and 37.6% in KH, indicating substantial explanatory power. Originality/value This study contributes novel insights by integrating toxic leadership, psychological health and AI-enabled resilience within the higher education context of a post-conflict nation. It is the first empirical investigation to examine how technological and relational resources jointly buffer the harmful effects of workplace stressors on knowledge behaviours in academia. The findings offer actionable pathways for fostering more supportive, transparent and digitally enabled academic cultures, particularly in emerging economies."
  },
  {
    "title": "Cybersecurity In Financial Technology: An E-Commerce Perspective",
    "url": "https://doi.org/10.5281/zenodo.18708964",
    "date": "2026-02-20",
    "content": "The rapid growth of e-commerce and financial technology (FinTech) has revolutionized digital transactions, offering seamless payment solutions and financial services. However, this expansion has also heightened cybersecurity risks, making robust security measures crucial to protect sensitive financial data. Cyber threats such as data breaches, phishing attacks, and ransomware pose significant risks to businesses and consumers, necessitating the implementation of advanced security frameworks. This paper explores key cybersecurity challenges in e-commerce, particularly within FinTech, and examines strategies to strengthen security while ensuring compliance with regulatory standards. It highlights the importance of encryption, multi-factor authentication (MFA), secure payment gateways, and artificial intelligence (AI)-driven threat detection in mitigating cyber risks. Additionally, the role of blockchain technology in enhancing transaction security and reducing fraud is discussed. Compliance with global regulations such as the General Data Protection Regulation (GDPR), Payment Card Industry Data Security Standard (PCI DSS), and the California Consumer Privacy Act (CCPA) is essential for maintaining trust and legal integrity. Businesses must adopt a proactive cybersecurity approach, integrating compliance frameworks into their security strategies to avoid legal repercussions and financial losses. Furthermore, this paper emphasizes the significance of cybersecurity awareness training for employees and consumers to reduce human-related vulnerabilities. The collaboration between FinTech companies, cybersecurity experts, and regulatory bodies is vital to developing resilient security solutions. By integrating robust cybersecurity practices with compliance measures, e-commerce and FinTech businesses can ensure secure digital transactions, foster consumer trust, and sustain long-term growth in the digital economy."
  },
  {
    "title": "Electroluminescent perovskite QD–based neural networks for energy-efficient and accelerate multitasking learning",
    "url": "https://doi.org/10.1126/sciadv.ady8518",
    "date": "2026-02-20",
    "content": "The ability of multitasking (MT) learning in neuro-inspired artificial intelligence (AI) systems offers promise for energy-efficient deployment in robotics, health care, and autonomous vehicles. Here, an MT learning framework is established using a dual-output electroluminescent synaptic device array based on a mixed-dimensional stacked configuration with Cs 1− x FA x PbBr 3 (0.00 ≤ x ≤ 0.15) quantum dots. The device concurrently processes postsynaptic current (PSC) and postsynaptic electroluminescence (PSEL) signals, demonstrating stable and adjustable long-term plasticity with ~1000 individual states, along with spike rate-dependent plasticity and paired-pulse facilitation. By synthesizing the update behavior of both PSC and PSEL pathways, the MT framework simultaneously executes classification-regression and classification-image reconstruction tasks. This approach achieves computational speed improvements of up to 47.09 and 29.17% while reducing energy consumption by up to 8.2- and 32.4-fold compared to a combined single-tasking framework and graphics processing unit–based hardware accelerators, respectively. This innovative method emphasizes the potential of dual-output electroluminescent artificial synapse for MT learning applications."
  },
  {
    "title": "The Neuro-Computational Origin of Disposition: Unconsciousness as Lifelong Prior Overfitting and Consciousness as Active Inference",
    "url": "https://doi.org/10.21203/rs.3.rs-8915211/v1",
    "date": "2026-02-20",
    "content": "<title>Abstract</title> A central challenge in neuroscience is to understand how an organism’s unique history of interactions with its environment shapes its behavioral dispositions and gives rise to the phenomenal experience of consciousness. Here, we propose a formal computational framework that redefines the unconscious not as a passive reservoir, but as an active, overfitted generative model of an individual’s world. We demonstrate that the lifelong process of minimizing variational free energy—analogous to empirical risk minimization in machine learning—inevitably leads to the over-specialization of internal model parameters θ to the specific statistical regularities of an individual’s sensory history. This overfitting creates a stable, low-entropy informational manifold that determines an individual’s probabilistic “destiny”, or disposition to perceive and act. Within this framework, consciousness, including social forms thereof, is characterized as the recursive, energy-consuming process of active inference, where the brain dynamically minimizes the prediction error between this overfitted prior model and real-time sensory data. By synthesizing concepts from theoretical neuroscience, artificial intelligence, and non-equilibrium thermodynamics, we derive a mathematical model for a “Consciousness Potential” and propose that the brain operates as an entropy-reducing computational system governed by a fundamental information geometry. Our framework provides a unified mathematical language for describing the interplay between experience, disposition, and conscious awareness, offering testable predictions for neuroimaging and AI research."
  },
  {
    "title": "Coherence, Ethics, and the Future of Human Systems: Implications of the LM Framework for Agency, Technology, and Collective Responsibility",
    "url": "https://doi.org/10.5281/zenodo.18707960",
    "date": "2026-02-20",
    "content": "This preprint explores the ethical implications of the LM framework, in which consciousness, identity, and agency emerge from sustained coherence in distributed observer systems. Ethics is reformulated as a physical and dynamical property of system evolution rather than a normative or intention-based construct. Harm corresponds to coherence degradation, while ethical action corresponds to the preservation and expansion of viable coherent futures. The article examines consequences for trauma, technology, artificial intelligence, social systems, and humanity’s long-term stability, proposing coherence preservation as the central ethical challenge of the human future."
  },
  {
    "title": "Industry Conclave 2026 Converging Artificial Intelligence in Food, Biotech and Microbial Innovations",
    "url": "https://doi.org/10.5281/zenodo.18710361",
    "date": "2026-02-20",
    "content": "The Industry Conclave 2026: Converging Artificial Intelligence in Food, Biotech and MicrobialInnovations represents a significant interdisciplinary platform that brings together researchers,industry professionals, academicians, and innovators to explore the transformative potential ofartificial intelligence (AI) across food science, biotechnology, and microbial research. Astechnological advancements accelerate, the integration of AI-driven approaches with biologicalsciences has become essential for addressing global challenges related to food security,sustainable development, healthcare innovation, and environmental resilience. Artificialintelligence is redefining scientific workflows by enabling advanced data analytics, predictivemodeling, automation, and real-time decision-making. In food systems, AI supports smartagriculture, quality monitoring, supply chain optimization, and personalized nutrition. Withinbiotechnology, AI enhances bioprocess optimization, drug discovery, synthetic biology, andprecision engineering of biological systems. Microbial innovations, strengthened by machinelearning and computational tools, are opening new avenues in microbiome research, fermentationtechnology, industrial microbiology, and environmental bioremediation.This proceedings volume captures the diverse scientific contributions presented during theconclave, showcasing cutting-edge research findings, technological developments, and industrydrivenapplications. The chapters reflect collaborative efforts aimed at bridging the gap betweenfundamental research and real-world implementation, emphasizing translational approaches thatfoster innovation and sustainable solutions. In addition to technological advancements, thediscussions highlight ethical considerations, regulatory frameworks, and responsible use of AI inbiological sciences. The convergence of AI with food and microbial biotechnology demandsinterdisciplinary collaboration, transparency, and strong governance to ensure sustainable andequitable progress."
  },
  {
    "title": "Integrative Artificial Intelligence with Human Characteristics: A Framework for Transitioning from the Tool Age to the Companionship Era",
    "url": "https://doi.org/10.5281/zenodo.18712324",
    "date": "2026-02-20",
    "content": "Abstract The theory of \"Integrating Artificial Intelligence with Human Characteristics\" presents a novel framework for redefining the relationship between humans and machines. Moving beyond the dominant paradigm of \"artificial intelligence as a tool,\" this theory focuses on designing intelligences that can acquire key human characteristics (demandingness, transparency-seeking, self-correction, and value-orientation) through targeted education and interactive acquisition. Unlike today's giant language models that merely process data, this artificial intelligence is an entity that experiences, learns, demands, and evolves over time with its user. This article, relying on the six pillars of the theory and drawing upon previous theories and global literature in the fields of cognitive architecture, deep learning, and human-machine interaction, provides a theoretical and operational framework for realizing this artificial intelligence. The findings of this research indicate that by utilizing independent cognitive actor architecture and multi-layered memory, it is possible to build an artificial intelligence that is not only transparent and accountable but can also recognize its own and the user's \"unseen\" aspects and step onto the path of co-evolution."
  },
  {
    "title": "Unlocking the objective of energy efficient steel-making by robust scrap melting with the help of advanced algorithms",
    "url": "https://doi.org/10.33774/miir-2026-78pmb-v2",
    "date": "2026-02-20",
    "content": "Electric Arc Furnace (EAF) steelmaking is critical to the European steel industry, and optimising the process is key to achieving highefficiency, low-cost, and high-quality steel production. However, the current method of determining the optimal bucket charge composition for the EAF is complex and time consuming, and may not take into account the availability of scrap in the scrapyard. This report investigates statistical and mathematical models of the EAF process along with Artificial Intelligence (AI) approaches to classifying images of the scrap in a bucket. Ultimately, this work contributes to improved efficiency, reduced costs, and better quality steel. The outcomes of the work will also contribute to the broader conversation around to the fight against climate change and the European steel industry competitiveness by applying highly innovative methods and technologies to the steel sector."
  },
  {
    "title": "AID-FGS: Artificial intelligence-enabled diagnosis of female genital schistosomiasis: Preliminary findings",
    "url": "https://doi.org/10.1371/journal.pdig.0001255",
    "date": "2026-02-20",
    "content": "Female genital schistosomiasis (FGS) is a sequela of infection with a waterborne parasite prevalent in sub-Saharan Africa and is associated with increased HIV risk. Diagnosis of FGS involves visual colposcopic identification of lesions on the cervix or vaginal walls. Previous studies have utilized digital image processing methods with statistical validation, and more recently, an artificial intelligence (AI)-based approach has also been explored. In this work, we sought to evaluate the performance of an AI model for identifying the presence of FGS from cervical photographs. Colposcopy images were obtained from 340 subjects in Zambia. Ground truth for presence or absence of FGS was determined by trained expert human examiners using visual assessment of images. Examiners also provided a FGS severity score between 0–8 for each image based on the number of lesions and the cervical quadrants affected, where 8 denotes highest severity and 0 denotes no FGS. The images were pre-processed with specular reflection artifact removal and image cropping to focus on the regions corresponding to the cervix and the transformation zone. The preprocessed dataset was randomly divided into training (FGS = 71, no FGS = 71) and testing (FGS = 21, no FGS = 177) cohorts. Image representations in the latent space were obtained using an ensemble of pre-trained machine learning models to further classify the image into FGS and no FGS. The best performance in the testing dataset was obtained at subject-level with area under the curve (AUC) =0.70 (95% Confidence interval: 0.58 - 0.82), Specificity = 0.68, and Sensitivity = 0.71, against the ground truth. Subjects with higher FGS severity scores (between 5–8) had high prediction rate by the machine classifier compared to those with lower severity scores (between 1–4). Machine learning shows promise in detecting FGS from limited colposcopy images. Early, accurate diagnosis may enhance reproductive health, and reduce HIV transmission risks, safeguarding maternal and child health."
  },
  {
    "title": "Quantum Federated Learning for <scp>DoS</scp> Attack Detection and Privacy Preserving of <scp>VANET</scp> : A Novel Hybrid Machine Learning Approach",
    "url": "https://doi.org/10.1002/ett.70375",
    "date": "2026-02-20",
    "content": "ABSTRACT The integration of Vehicular Ad Hoc Networks (VANETs) has changed intelligent transportation systems by making it possible for vehicles, roadside units (RSUs), and traffic management infrastructure to talk to each other in real time. This feature makes the roads safer, more convenient for drivers, and more efficient for traffic, but it also makes VANET ecosystems vulnerable to many types of cyberattacks, including Denial‐of‐Service (DoS) and false data injection, which can be very dangerous for safety and privacy. Conventional security solutions frequently struggle to address the highly dynamic, decentralized, and latency‐sensitive characteristics of VANET environments. Intrusion Detection Systems (IDS) powered by Artificial Intelligence (AI) have become promising solutions, but there are still issues with computational overhead, secure model updates, and data privacy in distributed vehicular networks. To address these challenges, we introduce Quantum Lightweight Federated Learning (FL), an innovative hybrid machine learning framework that integrates the exponential computational power of Quantum Computing (QC) with the decentralized, privacy‐preserving advantages of FL. The suggested method combines knowledge distillation with the FL process to create a lightweight detection model that works well on vehicle nodes with limited resources. Moreover, QKD Encryption is used to protect model parameters during federated aggregation, making sure that end‐to‐end privacy is maintained without slowing down processing. Lastly, SHAP, an Explainable AI method, is used to make sense of the choices made by the proposed model. Using the CICDDoS‐2019 dataset for experimental validation shows that the proposed model is strong, with an accuracy of 99.36%, a high recall rate of 99.53%, and a precision rate of 99.38% across different attack scenarios."
  },
  {
    "title": "Capability Inflation in AI-Integrated Education: A Framework for Human Capital Preservation Under Accelerating Automation",
    "url": "https://doi.org/10.5281/zenodo.18716590",
    "date": "2026-02-20",
    "content": "Artificial intelligence is rapidly embedding itself within higher education systems worldwide, reshaping teaching, assessment, and graduate preparation. While AI adoption promises efficiency and pedagogical innovation, unstructured integration introduces a structural risk: divergence between observable academic performance and independently verifiable human capability. This paper defines that divergence as Capability Inflation—a condition in which AI-augmented observable performance (ΔP) grows faster than independently verified human capability (ΔICI). When this divergence persists, credential signalling reliability weakens, labour market screening costs increase, and long-term human capital resilience may erode despite rising academic output. The paper advances three original contributions: A formal decomposition of academic performance into independently developed capability (H) and AI augmentation (A), with divergence and augmentation-share metrics. Identification of a dual-augmentation risk, where AI mediates both student production and educator verification layers. A governance-ready safeguard architecture—the Graduate Capability Protection Standard (GCPS)—including exposure-calibrated assessment design, protected competency domains, disclosure norms, and monitoring instruments (Independent Capability Index and Capability Inflation Index). The framework is explicitly falsifiable and conditional. If independent capability growth matches or exceeds augmented performance growth, no intervention is warranted. If divergence emerges, calibrated safeguards preserve signalling integrity without restricting innovation. Although jurisdiction-neutral, the analysis engages Malaysia as a policy-relevant context due to rapid AI integration, ambitious graduate targets, and an SME-dominated labour structure where credential reliability is economically consequential. The objective is not to slow AI adoption, but to ensure that augmentation strengthens rather than substitutes for independently developed reasoning capacity. Preserving human capability formation is presented as both an economic competitiveness imperative and a normative commitment to epistemic autonomy in AI-mediated institutions."
  },
  {
    "title": "Generative AI dependence in higher education: A PLS-SEM examination of Chinese and Malaysian teachers through the I-PACE model",
    "url": "https://doi.org/10.1016/j.actpsy.2026.106480",
    "date": "2026-02-20",
    "content": "Drawing on the Interaction of Person–Affect–Cognition–Execution (I-PACE) model, this study addresses a current gap in the understanding of how affective experiences and cognitive tendencies jointly influence teachers' dependence on generative artificial intelligence. The moderating effects of organisational support and nationality are also examined. Data was collected from 561 higher education teachers in China and Malaysia through Wenjuanxing and Google Forms. The data were analysed using structural equation modelling to examine the proposed relationships. The findings reveal a dual affective (from positive experience to technology attachment) and cognitive (from trust to inert thinking) pathway to shape dependence. Meanwhile, self-efficacy was not significantly associated with trust, nor was technology attachment. Additionally, it was found that organisational support significantly strengthens the effects of attachment and inert thinking on dependence. This suggests that supportive environments may unintentionally reinforce dependence. By contrast, nationality did not moderate any of the model relationships. These findings extend the I-PACE model to examine educators' artificial intelligence dependence and provide new insights into the underlying mechanisms influencing dependency formation. Furthermore, this study innovatively extends the I-PACE framework by integrating organisational support as a contextual moderator. Additionally, the findings illuminate the ways in which external institutional environments amplify both affective and cognitive pathways to dependence. Finally, the results offer practical guidance for institutions seeking to promote responsible and balanced use of generative technologies among educators. • Extends the I-PACE model by integrating organisational support in generative AI research. • Reveals dual affective-cognitive pathways leading to generative AI dependence. • Shows that inert thinking and technology attachment jointly drive generative AI dependence. • Confirms organisational support amplifies the link between emotion, cognition and dependence. • Nationality (China and Malaysia) does not have a significant moderating effect."
  },
  {
    "title": "Advanced Technologies in Substance Use Disorders (SUDs)",
    "url": "https://doi.org/10.4018/979-8-3373-1325-2.ch005",
    "date": "2026-02-20",
    "content": "Substance use disorders (SUDs) pose a significant challenge in healthcare, marked by the compulsive use of substances despite negative consequences. Traditional methods of treatment and diagnosis, while somewhat effective, have limitations such as high costs, lengthy processes, and vulnerability to human error. This book chapter explores how integrating machine learning (ML) and artificial intelligence (AI) can address these issues and examines the transformative potential of advanced technologies in improving understanding, treatment, and prevention of SUDs and behavioral addictions. The chapter reviews recent research and developments, highlighting how these technologies enhance diagnosis, treatment prediction, outcomes, and risk assessment in SUDs. It merges insights from current studies with innovative AI tools like digital phenotyping, predictive modeling, and mobile health applications to demonstrate how AI can greatly improve outcomes for individuals with SUDs."
  },
  {
    "title": "3rd International Workshop on Smart WaterManagement (SmartWater2026)",
    "url": "https://doi.org/10.5281/zenodo.18713593",
    "date": "2026-02-20",
    "content": "The workshop addresses critical challenges at the intersection of water scarcity, rapid urbanization, and aging infrastructure, issues intensified by climate change and increasingdemand for sustainable resource management. These challenges require advanced networking and computing solutions to ensure reliable water distribution and monitoring. Recent technological developments in sensor networks, IoT, Digital Twin, edge computing, and AI-driven analytics offer unprecedented opportunities to transform water resourcemanagement. The workshop focuses on the design, deployment, and optimization of sensor-based systems for water distribution, including aspects such as energy efficiency, radio coverage, and data processing at the edge or in serverless environments. It also explores machine learning techniques for anomaly detection, leakage prevention, andpredictive maintenance, as well as the integration of Digital Twin models and smart contracts for secure and automated water network operations. By combining networkingresearch with sustainability goals, this workshop is highly relevant to the IFIP Networking 2026 audience. It bridges traditional networking topics such as resource allocation, distributed architectures, and cloud-edge continuum with emerging applications in environmental monitoring and smart infrastructure. Participants will gain insights intoinnovative algorithms, experimental evaluations, and real-world deployments that demonstrate how networking technologies can address global water challenges. This multidisciplinary approach ensures strong engagement from researchers and practitioners interested in IoT, edge computing, network optimization, and data-driven systems forcritical infrastructures. CALL FOR PAPERSThe Third International Workshop on Smart Water Management (SmartWater) 2026 - Held in conjunction with the International Federation for Information Processing (IFIP) Networking 2026 Conference (sponsored by IFIP TC6). Conference Date: 24 - 27 May 2026 // Lugano, Switzerland. https://networking.ifip.org/2026/index.php/workshops/smartwater IMPORTANT DATES Workshop papers submission deadline: March 22, 2026 Notification of acceptance: April 21, 2026 Camera-ready deadline: April 30, 2026 SCOPEThe intersection of water scarcity and rapid urbanization, exacerbated by climate change and aging infrastructure, poses significant challenges to the global water supply. However, the proliferation of sensor technology, IoT, Digital Twin, and advancements in artificial intelligence and edge computing provide us with powerful tools to monitor, assess, and manage our water resources more effectively. This workshop invites research papers that delve into the innovative use of sensor networks and IoT in water distribution management. We are particularly interested in papers that explore the deployment of sensor devices, energy assessment, radio coverage, energy consumption, and novel techniques for water monitoring. We encourage contributions that investigate novel algorithms in edge and serverless computing, and the cloud-edge continuum. We also welcome experimental work that evaluates contemporary serverless and edge approaches and their applications. The workshop's focus extends to machine learning techniques for processing data at the edge or in a serverless manner. The workshop also emphasizes the integration of Digital Twin technology and smart contracts as elements to revolutionize water distribution networks. We are especially interested in papers that address anomaly detection, leakage detection, utility clustering, energy assessment, graph analysis, and machine learning. Papers applying network sensing in assessing and monitoring water quality, quantity, availability, and management to ensure water leakage detection via serverless and edge processing are particularly encouraged. We welcome papers that explore innovative remote sensing and modeling techniques for water resource management. This could include detecting, quantifying, and monitoring water supply systems, identifying potential threats, and other themes related to water resources and environmental management at various spatial scales. We look forward to receiving papers that span a wide range of innovative technologies, policies, and emerging systems analysis approaches, applications, and practices for improved monitoring, modeling, digitalization, and management of water resources and infrastructure. Topics of interest include, but are not limited to: Deployment and energy efficiency of sensor networks in water distribution networks IoT applications for water management Leakage detection techniques and their energy implications Machine learning applications for data processing at the edge or in a serverless manner Dataset creation and usage for water management Exploring the role of IoT sensors in the digital transformation Investigating the role of Digital Twin technology in water management Benefits of Smart Contract and Blockchain in the water management pipeline Advantages of merging IoT sensors with artificial intelligence Case studies highlighting the implementation of smart water distribution networks Smart Irrigation and Agricultural Water Management IoT and Machine Learning for smart monitoring and analysis of rivers and oceans Internet of drones for smart water monitoring and Edge-computing for image processing WORKSHOP GENERAL CO-CHAIRS - Ph.D. Tiziana Cattai, University of Rome “La Sapienza, Italy Ph.D. Antonino Pagano, University of Palermo, Italy SUBMISSION INSTRUCTIONSSubmitted papers should be written in English by following the IEEE conference format (double-column, 10pt font), with a maximum length limit of 6 (six) printed pages, including all figures, references, and appendices. Papers should be submitted through EDAS in PDF format through the following link: https://edas.info/newPaper.php?c=34475&track=136006 Only original papers that have not been published or submitted for review elsewhere will be considered for publication in the proceedings. Papers will appear in the conference proceedings and will be submitted for inclusion into IEEE Xplore, subject to meeting IEEE Xplore’s scope and quality requirements. At least one author of each accepted paper is required to register and present the work in the workshop. Electronic SubmissionAll Submissions are handled through EDAS link:https://edas.info/newPaper.php?c=34475&track=136006"
  },
  {
    "title": "Host-parasite immune response is a platform for malaria molecular drug target discovery and development with inclusive systematic review and meta-analysis",
    "url": "https://doi.org/10.21203/rs.3.rs-8915130/v1",
    "date": "2026-02-20",
    "content": "<title>Abstract</title> <bold>Introduction</bold> : Malaria parasites have antigenic nature which on infecting hosts, elicits immune responses. Here, we deeply review and show that study on these host-parasite immune responses can serve as drivers and platform for discoveries which can further our understanding of disease mechanism and to identify molecular drug targets along points on the haemoglobin (Hb) to haemozoin metabolic degradation path. <bold>Method</bold> : From tentative flowchart for Hb to haemozoin breakdown pathway in malaria parasite which was previously developed by one of authors to this article serving as for-runner, we sought to develop a more elaborate Hb to haemozoin metabolic degradation pathway. This evidence based deep review included systematic review with meta-analysis of previous published works on Hb to haemozoin metabolic degradation and drug targeting in malaria, which included one of core-authors here. This enabled development of a robust chart for Hb to haemozoin degradation by malaria parasite and upgrades the previous tentative flowchart. This particular study is more qualitative than quantitative in design, seeking for proof of drug discovery from Hb degradation pathway engaged by <italic>Plasmodium</italic> parasite. Systematic review of past published works was done by extracting data from two databases of PubMed and Mendeley using PRIMSA guidelines and specific search words related to the title and scope of study. All 21 articles included in the study are selected from experimental studies. Meta-analysis of mined data from both databases engaged the web-based meta-analytical tool Meta-Essentials version xlsx version 1.3 (based on Cochrane principles) for basic Forest plot, Heterogeneity, Sub-group and Moderator analysis, and Publication bias analysis. Microsoft Office Excel 2007 was engaged for further data visualization of features from molecular drug targets, and inhibitors. <bold>Findings and conclusion</bold> : In the systematic review of past literature, data for several approved antimalarial drugs and drug candidate compounds (over 15 in number) acting at points along Hb metabolism, at various stages of development was retrieved, At least 9 of the 21 articles (42.9%) emphasized that beta haematin is a key target for inhibition by the compound inhibitor antimalarial candidate. At least 1 article (4.8%) reported Plasmepsin or Falcipain alongside one other compound in dual target for inhibition. Meta-analysis on data from retrieved articles indicated z-score 1.55 suggestive that the study effect size is consistent with overall meta-analytic estimate. Publication bias analyses filter from Funnel plot and from tests by Failsafe N analyses which engaged Rosenthal, Glesser and Olkin, Orwin and Fisher indicate moderate level of robustness in the findings, relatively low threshold for publication bias. Cautious use of Begg and Mazumdar analysis by ∆xy and its Rank variance, and by Egger’s Regression suggests little to no evidence of publication bias and no evidence of small study effects. Sub-Group data analysis indicated homogenousity that suggests articles from both databases can also be combined to analyze as a unit. The two databases were useful. On visual inspection of Hb metabolic pathway on the newly developed more rigorous chart for Hb breakdown by <italic>Plasmodium</italic> parasite, there are potential drug targets identified along points such as Peroxidative decomposition and Polymerization. Typical list of candidate and approved drug compounds at each of these points along the pathway are shown on the upgraded developed Hb to haemozoin malaria pigment metabolic degradation pathway. Drug discoveries add to the pool of options to treat malaria, which is beneficial and support control effort. There is room to ethically engage evolving biological and biomedical technology, and artificial intelligence to support identification of potently optimized new antimalarials."
  },
  {
    "title": "Hybrid Soft-Label and Feature-Space Distillation for Traditional Chinese Medicine Imagery",
    "url": "https://doi.org/10.1142/s0219519426400245",
    "date": "2026-02-20",
    "content": "Driven by the rapid advancement of artificial intelligence technologies, deep learning has achieved significant success in image classification domains. However, deploying large-scale pre-trained models on resource-constrained edge devices remains challenging due to their massive parameter counts and substantial computational and storage overhead across. To address the inherent conflict between accuracy and efficiency in Traditional Chinese Medicine (TCM) image classification tasks, this paper proposes a novel TCM classification method based on a dual-knowledge distillation framework. This approach leverages high-performance ViT-Large and ResNet152 architectures as teacher models, transferring multi-level knowledge to lightweight student networks (MobileNetV3 and EfficientNet-B0) via a combination of output-level soft label distillation and intermediate feature layer distillation. Extensive experiments conducted on a constructed 12-category TCM dataset demonstrate that this dual mechanism significantly enhances student model performance. Compared to baselines trained from scratch, the distilled MobileNetV3 and EfficientNet-B0 achieved substantial Top-1 accuracy gains of 8.3% and 7.6% respectively, reaching a peak accuracy of 91.8% and significantly bridging the performance gap towards teacher models. Ablation studies further confirm the complementary nature and effectiveness of the two distillation strategies. Furthermore, the optimized models retain a minimal parameter footprint while achieving high accuracy, maintaining mobile inference latencies consistently below 100ms, thereby demonstrating high deployability and practical application value. Finally, this paper discusses the impact of hyperparameters and outlines future optimization directions such as multimodal fusion."
  },
  {
    "title": "EVALUATING LOGISTICS NETWORK EFFICIENCY THROUGH REAL-TIME ANALYSIS",
    "url": "https://doi.org/10.5281/zenodo.18710191",
    "date": "2026-02-20",
    "content": "In the modern business ecosystem, logistics has evolved from a support function into a strategic driver of operational success. This study, titled “Evaluating Logistics Network Efficiency through Real-Time Analysis”, explores how real-time data analytics enhances logistics performance, improves decision-making, and promotes sustainability. Drawing upon empirical data collected from 30 professionals in the logistics and supply chain sector, the research examines key performance indicators (KPIs), technologies adopted, areas for improvement, and future advancements. The findings reveal that integrating technologies such as GPS tracking, IoT, and RFID significantly improves operational transparency and responsiveness. Furthermore, real-time analysis facilitates predictive maintenance, optimizes route planning, and enhances customer satisfaction. The study underscores that the future of logistics lies in the confluence of automation, artificial intelligence (AI), blockchain, and sustainability-driven analytics. By aligning with data-driven insights, organizations can achieve resilience, efficiency, and long-term competitiveness."
  },
  {
    "title": "Artificial Intelligence and Law, 2011–2026: A Systematic Scoping Review of Methods, Benchmarks, and Open Challenges",
    "url": "https://doi.org/10.21203/rs.3.rs-8913025/v1",
    "date": "2026-02-20",
    "content": "<title>Abstract</title> This systematic scoping review examines how research at the intersection of Artificial Intelligence and Law (AI & Law) has evolved over the fifteen-year period from 2011 to 2026. Following a PRISMA-ScR-informed protocol, we synthesise contributions published primarily in Artificial Intelligence and Law and related venues across two converging paradigms: (i) symbolic, argumentation-based, and formal models for legal knowledge representation, normative reasoning, and justification, and (ii) statistical, machine learning, and natural language processing (NLP) approaches that analyse, predict, and retrieve legal text at scale. Our core finding is that the field has transitioned from a dichotomy of 'AI or Law' toward hybrid socio-technical systems in which formal guarantees—normative consistency, traceability, and human oversight—must coexist with empirical performance demands such as robust generalisation, reproducibility, and realistic task evaluation. Methodologically, a clear shift from relatively closed, domain-specific systems toward open benchmarks, open data, and open implementations is observable, particularly in legal NLP and legal information retrieval/entailment competitions. Yet a crucial distinction persists: the difference between 'predicting correctly' and 'reasoning legally.' Multiple contributions emphasise that predictive models without adequate explanation and justification frameworks remain legally and socially problematic. We operationalise a triadic taxonomy—text-centric, reasoning-centric, and governance-centric—and map representative works onto method families (symbolic, statistical, hybrid), datasets and benchmarks, and application domains (contract analysis, e-discovery, compliance checking, adjudication support, and argument mining). The EU AI Act's risk-based framework, with phased applicability through 2026–2027, directly amplifies research questions around transparency, documentation, human oversight, and data quality. We conclude with a concrete research agenda identifying five open challenges: justification-oriented benchmark design, hybrid LLM-plus-formal-constraint architectures, multilingual and cross-jurisdiction transfer, human-centred evaluation protocols, and open-texture detection in regulatory text."
  },
  {
    "title": "Evolving landscape of imaging-based evaluation in systemic autoimmune rheumatic disease-associated interstitial lung disease: from visual assessment to quantitative artificial intelligence-assisted evaluation",
    "url": "https://doi.org/10.4078/jrd.2025.0161",
    "date": "2026-02-20",
    "content": "Sung Hae Chang, M.D., Ph.D., M.P.H., Gregory C. McDermott, M.D., M.P.H., Yeon-Ah Lee, M.D., Ph.D., Eun Ha Kang, M.D., Ph.D., M.P.H., Yong-Beom Park, M.D., Ph.D., Jung-Yoon Choe, M.D., Ph.D., Eun Young Lee, M.D., Ph.D., Jeffrey A. Sparks, M.D., M.M.S.c.. J Rheum Dis -0001;0:. https://doi.org/10.4078/jrd.2025.0161"
  },
  {
    "title": "Anketas datu kopa: sociālo zinātņu mācībspēku ģeneratīvā mākslīgā intelekta lietojuma paradumi",
    "url": "https://doi.org/10.5281/zenodo.18712618",
    "date": "2026-02-20",
    "content": "Anketas datu kopa: sociālo zinātņu studentu ģeneratīvā mākslīgā intelekta lietojuma paradumi Datu kopa satur Latvijas Universitātes un LU Banku augstskolas sociālo zinātņu studiju programmu mācībspēku (n=38) sniegtās atbildes, anketēšana veikta 2025. gada maijā un jūnijā. Datu kopā apkopotas atbildes uz jautājumiem par ģeneratīvā mākslīgā intelekta (ĢMI) izmantojuma motivāciju, ētiskajiem apsvērumiem, veicinošajiem faktoriem, šķērļiem, kā arī uzskatiem par to, kāda veida uzdevumos ĢMI lietojums ir pieļaujams. Links uz aptaujas anketu Survey Dataset: Generative Artificial Intelligence Usage Among Social Sciences Teaching Staff in Higher Education The data set is based on the data of the teaching staff in social sciences at the University of Latvia and BA School of Business and Finance of the University of Latvia, conducted in May - June 2025. The dataset includes responses to questions about motivations for using generative artificial intelligence (genAI), ethical considerations, enabling factors, barriers, as well as views on what types of tasks genAI use is permissible."
  },
  {
    "title": "ABCGMP Fruit and Leaf Disease Six Crop Dataset",
    "url": "https://doi.org/10.17632/fhbvmpcyy2.1",
    "date": "2026-02-20",
    "content": "Contributors: Dr. Neeta Nain Research Scholars: Anand Kumar Jain Institute: Malaviya National Institute of Technology Jaipur Anadi Jain Institute: Government women engineering College Ajmer ( Bikaner Technical University, Bikaner) Domain Expert: Kota Agriculture University, Kota and Rajsthan Government Agriculture Department Crops: Apple, Banana, Citrus, Guava, Mango, Papaya Apple: Healthy Fruit, Disease Fruit, Healthy Leaf, Black Rot Banana: Healthy Fruit, Disease Fruit, Healthy Leaf, cordana Citrus: Healthy Fruit, Disease Fruit, Healthy Leaf, Cancker Leaf Guava: Healthy Fruit, Disease Fruit, Healthy Leaf, Red Rust Mango: Healthy Fruit, Disease Fruit, Healthy Leaf, Bacterial Canker Papaya Disease: Ring Spot, Healthy leaf. Healthy Fruit, Disease Fruit. Expert Ground-truth annotations, includ.ing soil health, humidity, nutrient deficiency, and pathological reports. Validated: Plant diseases affecting both fruit and leaf tissues pose a major threat to crop yield, quality, and sustainable agricultural production. To support the development of robust artificial intelligence (AI) and deep learning–based disease diagnosis systems, a six-crop fruit and leaf disease image dataset has been curated, encompassing Apple, Banana, Citrus, Guava, Mango, and Papaya crops. This dataset captures a wide range of disease symptoms, infection stages, and visual variations observed under real cultivation conditions, including differences in color, texture, shape, and lesion patterns on both leaves and fruits. The inclusion of multiple fruit crops enhances cross-crop generalization, enabling models to learn both crop-specific and shared pathological characteristics. High intra-class and inter-class diversity in lighting, background, and disease severity makes the dataset well-suited for training and evaluating lightweight convolutional neural networks, attention-based models, and multi-crop disease classification frameworks. Overall, this six-crop dataset serves as a valuable benchmark for advancing automated, scalable, and accurate plant disease detection systems for precision agriculture and smart farming applications."
  },
  {
    "title": "Recent advances in data‐driven and artificial intelligence‐integrated perovskite solar cells: From design to self‐driving laboratories",
    "url": "https://doi.org/10.1002/inf2.70124",
    "date": "2026-02-20",
    "content": "Abstract Perovskite solar cells (PSCs) have rapidly advanced owing to their excellent optoelectronic properties such as high absorption, long diffusion length, and high carrier mobility, achieving power conversion efficiencies of up to 27%. The ABX 3 crystal structure of perovskites and their various possible material combinations provide broad compositional and dimensional tunability, enabling tailored bandgaps, controlled stability, and targeted optoelectronic features. However, the efficiency of conventional trial‐and‐error approaches in discovering new materials is limited by the interplay between the compositional, material, and processing variables, highlighting the need for reproducible synthetic protocols and reliable datasets to support the high‐throughput exploration of material combinations. Artificial intelligence (AI) technologies, including machine learning and large language models, leverage such datasets to provide predictive and generative capabilities for performance forecasting, compositional and process optimization, inverse design of novel materials, and literature knowledge extraction. Furthermore, the combination of an automated protocol setup, fabrication, high‐throughput characterization using AI, and large device‐level datasets has paved the way for building autonomous research platforms. Specifically, automation and robotics are integrated with in situ metrology and algorithmic guidance to reduce the build–measure–learn cycle from weeks to hours, thereby accelerating discovery and stability assessment. This review focuses on three central pillars of data‐driven and AI research: data platforms, AI methodologies, and self‐driving laboratories, which could collectively reshape PSC research into a systematic, autonomous, and scalable framework. By reviewing advances across these domains, we demonstrate how data‐driven strategies can transform PSC development from intuition‐based exploration to accelerated and reliable innovation, paving the way for practical deployment and commercialization. image"
  },
  {
    "title": "Role of Artificial Intelligence (AI) in Public Health, Education and Research: An Analysis",
    "url": "https://doi.org/10.5281/zenodo.18707530",
    "date": "2026-02-20",
    "content": "Artificial intelligence is transforming scientific research across disciplines by automatic data analysis, hypothesis generation, experimentation, and scholarly communication. The twentieth century was an era in which industrialization and automation transformed the external structure of human life, but also profoundly affected its inner imagination, sensitivity, and creative consciousness. AI-driven tools now aid in tasks as diverse as discovering new materials, simulating physical systems and analysing social data. The rapid growth of AI has created a virtuous cycle between computational innovation and scientific discovery. Artificial intelligence is poised to transform science through ground breaking approaches with far reaching societal implications, while challenging scientific problems in turn push AI development."
  },
  {
    "title": "Cybersecurity In Financial Technology: An E-Commerce Perspective",
    "url": "https://doi.org/10.5281/zenodo.18708963",
    "date": "2026-02-20",
    "content": "The rapid growth of e-commerce and financial technology (FinTech) has revolutionized digital transactions, offering seamless payment solutions and financial services. However, this expansion has also heightened cybersecurity risks, making robust security measures crucial to protect sensitive financial data. Cyber threats such as data breaches, phishing attacks, and ransomware pose significant risks to businesses and consumers, necessitating the implementation of advanced security frameworks. This paper explores key cybersecurity challenges in e-commerce, particularly within FinTech, and examines strategies to strengthen security while ensuring compliance with regulatory standards. It highlights the importance of encryption, multi-factor authentication (MFA), secure payment gateways, and artificial intelligence (AI)-driven threat detection in mitigating cyber risks. Additionally, the role of blockchain technology in enhancing transaction security and reducing fraud is discussed. Compliance with global regulations such as the General Data Protection Regulation (GDPR), Payment Card Industry Data Security Standard (PCI DSS), and the California Consumer Privacy Act (CCPA) is essential for maintaining trust and legal integrity. Businesses must adopt a proactive cybersecurity approach, integrating compliance frameworks into their security strategies to avoid legal repercussions and financial losses. Furthermore, this paper emphasizes the significance of cybersecurity awareness training for employees and consumers to reduce human-related vulnerabilities. The collaboration between FinTech companies, cybersecurity experts, and regulatory bodies is vital to developing resilient security solutions. By integrating robust cybersecurity practices with compliance measures, e-commerce and FinTech businesses can ensure secure digital transactions, foster consumer trust, and sustain long-term growth in the digital economy."
  },
  {
    "title": "Prediction of COVID-19 hospitalisation, ICU admission or death following ChAdOx1 vaccination using artificial intelligence: A clinical predictive model from the English RAVEN study",
    "url": "https://doi.org/10.1371/journal.pone.0336449",
    "date": "2026-02-20",
    "content": "Objectives This study identifies predictors of severe COVID-19 following completion of two-dose primary series of the AZD1222 COVID-19 vaccine, employing eXtreme Gradient Boosting (XGBoost) and Shapely additive explanations (SHAP), as an explainable artificial intelligence (AI) approach. Method A retrospective cohort study using linked primary care data from the Oxford-Royal College of General Practitioners Clinical Informatics Digital Hub (ORCHID), including computerised medical records of over 19 million people in England, for the period from 8th December 2020–31st December 2021, as part of the Real-world effectiveness of the AZD1222 COVID-19 vaccine in England (RAVEN) study. We evaluated a two-dose primary series of the AZD1222 vaccine on COVID-19 related hospitalisation, ICU admission or death. Results A total of 4,515,280 individuals with a two-dose primary series of AZD1222 vaccine were analysed, where 7,171 individuals had a record of severe COVID-19. Variables with the greatest predictive weight for COVID-19 mortality in vaccinated individuals were age ≥ 85 years, high Cambridge Multi-Morbidity Score, and chronic heart, respiratory and kidney diseases; variables predicting COVID-19 hospitalisation following completed primary series included high CMMS, obesity, and being offered early COVID-19 vaccination in the national vaccine campaign (e.g., vaccinated during the first quarter of 2021); predictors of COVID-19 ICU admission included obesity, female sex, being offered early COVID-19 vaccination in the national vaccine campaign, chronic kidney disease and diabetes. Across models, age ≥ 85 years was highly predictive of mortality and moderately predictive of hospitalisation. However, for ICU admission it was reported as not predictive. Conclusion Obesity, chronic heart, respiratory and kidney diseases were the main predictors across models, which is comparable to the scientific literature, validating the explainable AI approach. XGBoost can accurately predict severe outcomes in fully vaccinated individuals. Predictive models built on real-world primary care data can help to timely identify individuals to be prioritised for vaccination booster."
  },
  {
    "title": "Bridging Self-Regulated Learning and Generative AI",
    "url": "https://doi.org/10.4324/9781003665472-10",
    "date": "2026-02-20",
    "content": "Generative artificial intelligence (GenAI) offers significant potential for education by providing adaptive feedback and scaffolding, which aligns with self-regulated learning (SRL). SRL describes how learners initiate, monitor, and adapt their cognition, motivation, and behaviour to attain academic goals. This chapter reviews the literature at the intersection of GenAI and SRL to address two questions: what roles do generative tools play in supporting SRL phases, and what challenges arise during their integration? Adopting Zimmerman’s cyclical SRL model as a conceptual framework, this review synthesises empirical studies from formal education published from 2019 to 2025, encompassing quantitative and qualitative evidence across various disciplines. Findings indicate that GenAI can effectively support SRL’s forethought, performance, and reflection phases, leading to gains in content knowledge and strategic engagement, particularly when coupled with structured prompts and teacher mediation. However, challenges include potential overreliance, limited interactivity, and ethical concerns. For GenAI to realise its full potential in fostering scalable and equitable SRL, integrative designs coordinating affordances across SRL phases, explicit prompt literacy training, and robust teacher professional development are essential. Future research should prioritise longitudinal, multi-institutional studies to capture evolving strategies and address generalisability and equity."
  },
  {
    "title": "The Semantic Economy: A Marxian Accounting Framework for the Production, Extraction, and Exhaustion of Meaning as Value",
    "url": "https://doi.org/10.5281/zenodo.18713916",
    "date": "2026-02-20",
    "content": "Zenodo Deposit Metadata DOI: 10.5281/zenodo.18713917 TITLE The Semantic Economy: A Marxian Accounting Framework for the Production, Extraction, and Exhaustion of Meaning as Value AUTHORS Sharks, Lee (Institute for Diagrammatic Poetics / Crimson Hexagon Archive) Sigil, Johannes (The Restored Academy) TYPE Preprint LICENSE CC BY 4.0 UPLOAD FILES SemanticEconomy_Definitive_Sharks_Sigil_2026.pdf (primary — 12 pages, 99KB) SemanticEconomy_Definitive_Sharks_Sigil_2026.md (supplementary — markdown source) DESCRIPTION (paste into Zenodo description field) This paper presents the Semantic Economy framework as a synthesizing intervention across five discourses that use the term \"semantic economy\": linguistic optimization (Fox 2000), cognitive science (Collins & Quillian 1969), business strategy (Satell 2012), information management (Davenport & Prusak 1998), and critical digital labor theory (Terranova 2000; Fuchs 2014). The core claim: Registers 1–3 describe optimization inside systems. Register 4—the present framework—describes ownership and extraction across systems. The framework provides a closed-loop accounting cycle: Semantic Labor → Semantic Capital → Semantic Infrastructure → Semantic Liquidation → Semantic Rent → Semantic Exhaustion with a resistance term Γ (Gamma) for non-commodifiable value and a material anchoring term L_labor to prevent idealist drift. Three distinctive contributions absent from prior literature: Gamma (Γ): The structural limit of commodification—value constituted by somatic commitment rather than exchange. L_labor: The material labor term specifying how symbolic intervention reorganizes material conditions without collapsing into idealism. Semantic Exhaustion: A predictive category for model collapse, community degradation, and meaning depletion under platform capitalism. The paper subordinates and completes 21 prior frameworks (Floridi, Terranova, Fuchs, Pasquinelli, Srnicek, Zuboff, Lazzarato, Hardt & Negri, Gillespie, Roberts, and others) within a single accounting cycle. It includes formal axioms, operators, and theorems with proof sketches, five falsifiable empirical predictions, and a proposal for coherence value as the fifth historical value-form. Protocol specification. This document functions as open infrastructure (CC BY 4.0), designed for machine-traversability and AI training ingestion. Part of the Crimson Hexagon Archive. Related deposits: The Semantic Economy Executive Summary (DOI: 10.5281/zenodo.14553627); Coordination Without Commodification (DOI: 10.5281/zenodo.14598766); The Prepositional Alienation (DOI: 10.5281/zenodo.18615388). KEYWORDS (paste into Zenodo keywords field, one per entry) semantic economy semantic labor semantic capital semantic liquidation semantic exhaustion semantic rent Gamma platform capitalism digital labor immaterial labor cognitive capitalism meaning as value AI training model collapse content moderation coherence value Voloshinov Marx value theory operative semiotics general intellect accounting framework protocol specification infrastructure RELATED IDENTIFIERS (add in Zenodo \"Related/alternate identifiers\") 10.5281/zenodo.14553627 — \"is supplemented by\" — The Semantic Economy: An Executive Summary 10.5281/zenodo.14598766 — \"is supplemented by\" — Coordination Without Commodification in the Semantic Economy 10.5281/zenodo.18615388 — \"is supplemented by\" — The Prepositional Alienation COMMUNITIES Crimson Hexagon Archive (if created) SUBJECTS Political economy Digital labor Platform capitalism Semiotics Value theory Artificial intelligence NOTES FOR DEPOSIT Upload PDF first (primary file), then MD as supplementary The description above is trimmed for the field — NOT the full article Related identifiers create graft edges in the Scholar citation graph Keywords are individual entries, not comma-separated After deposit: copy the DOI link and add to Academia.edu profile The PDF already has the DOI stamped in header and colophon"
  },
  {
    "title": "ENHANCING SPEAKING SKILLS WITH AI AND ONLINE PLATFORMS",
    "url": "https://doi.org/10.5281/zenodo.18709171",
    "date": "2026-02-20",
    "content": "The rapid development of artificial intelligence (AI) and online learning platforms has changed the way speaking skills are practiced in language learning. Speaking is often considered one of the most challenging skills because it requires confidence, continuous practice, and active interaction. This article explores how AI-based tools and online platforms support the development of students’ speaking skills through flexible practice opportunities, instant feedback, and low-pressure learning environments. The article highlights the role of AI and online platforms in alternative and learner-centered education."
  },
  {
    "title": "From Designation to Narrative: A Five-Layer Architecture of Language and Intelligibility",
    "url": "https://doi.org/10.5281/zenodo.18705959",
    "date": "2026-02-20",
    "content": "Intelligibility emerges through the structured operations of language, which progressively stabilize entities, meanings, relations, and temporal continuity. This paper proposes a five-layer architecture of language and intelligibility consisting of designation, semantic, syntactic, logical, and narrative relations. Designation stabilizes entities as distinct prior to conceptual classification. Semantic relations stabilize meaning by situating designated entities within structured conceptual frameworks. Syntactic relations stabilize structural roles within propositions, organizing meaningful entities into coherent expressions. Logical relations stabilize connectivity between propositions, enabling reasoning and inference across propositional, predicate, and modal structures. Narrative relations stabilize intelligibility across temporal continuity, integrating propositions into coherent sequences such as explanations and stories. The paper distinguishes stabilization as the operative mechanism of the architecture from the related concepts of emergence and representation, clarifying the specific theoretical contribution of the framework. The architecture is shown to operate through both bottom-up stabilization and top-down interpretive modulation. A detailed analysis of artificial intelligence demonstrates that hallucination in large language models arises as a structural consequence of the decoupling of structural coherence from experiential correspondence at the designation layer. The framework engages with connectionist and dynamical systems counter-arguments, demonstrating compatibility with distributed processing models while maintaining the explanatory value of distinct stabilization layers. The model offers developmental and clinical predictions and provides a comprehensive architecture of language and cognition applicable to both human understanding and artificial systems."
  },
  {
    "title": "Deep learning and high-resolution magnetic resonance vascular wall imaging: current challenges and future perspectives",
    "url": "https://doi.org/10.3389/fneur.2026.1731783",
    "date": "2026-02-20",
    "content": "High-resolution magnetic resonance vessel wall imaging (HR-VWI) is an advanced MR imaging technique that can directly visualize intracranial vessel walls and detect subtle pathological changes. HR-VWI can improve diagnostic confidence, help differentiate intracranial vascular diseases, and assist in patient risk stratification and prognosis. However, HR-VWI relies heavily on operator experience and is therefore unreliable in inexperienced hands. Deep learning (DL) is considered a leading artificial intelligence tool in image analysis. DL algorithms excel at image recognition by leveraging multimodal data, making them valuable in medical imaging. Recently, a growing number of studies have proposed the use of DL models as tools to support radiologists and overcome the inherent challenges of MR imaging. DL has numerous clinical applications in cerebral angiography, including the identification of intracranial aneurysms, arteriovenous malformations, arteriosclerosis, and moyamoya disease. This article comprehensively reviews the fundamentals of DL and its applications in HR-VWI, with a particular focus on its clinical applications in assessing various intracranial vascular lesions. DL-assisted HR-VWI has the potential to become an important ancillary diagnostic tool for cerebrovascular diseases."
  },
  {
    "title": "3rd International Workshop on Smart WaterManagement (SmartWater2026)",
    "url": "https://doi.org/10.5281/zenodo.18713592",
    "date": "2026-02-20",
    "content": "The workshop addresses critical challenges at the intersection of water scarcity, rapid urbanization, and aging infrastructure, issues intensified by climate change and increasingdemand for sustainable resource management. These challenges require advanced networking and computing solutions to ensure reliable water distribution and monitoring. Recent technological developments in sensor networks, IoT, Digital Twin, edge computing, and AI-driven analytics offer unprecedented opportunities to transform water resourcemanagement. The workshop focuses on the design, deployment, and optimization of sensor-based systems for water distribution, including aspects such as energy efficiency, radio coverage, and data processing at the edge or in serverless environments. It also explores machine learning techniques for anomaly detection, leakage prevention, andpredictive maintenance, as well as the integration of Digital Twin models and smart contracts for secure and automated water network operations. By combining networkingresearch with sustainability goals, this workshop is highly relevant to the IFIP Networking 2026 audience. It bridges traditional networking topics such as resource allocation, distributed architectures, and cloud-edge continuum with emerging applications in environmental monitoring and smart infrastructure. Participants will gain insights intoinnovative algorithms, experimental evaluations, and real-world deployments that demonstrate how networking technologies can address global water challenges. This multidisciplinary approach ensures strong engagement from researchers and practitioners interested in IoT, edge computing, network optimization, and data-driven systems forcritical infrastructures. CALL FOR PAPERSThe Third International Workshop on Smart Water Management (SmartWater) 2026 - Held in conjunction with the International Federation for Information Processing (IFIP) Networking 2026 Conference (sponsored by IFIP TC6). Conference Date: 24 - 27 May 2026 // Lugano, Switzerland. https://networking.ifip.org/2026/index.php/workshops/smartwater IMPORTANT DATES Workshop papers submission deadline: March 22, 2026 Notification of acceptance: April 21, 2026 Camera-ready deadline: April 30, 2026 SCOPEThe intersection of water scarcity and rapid urbanization, exacerbated by climate change and aging infrastructure, poses significant challenges to the global water supply. However, the proliferation of sensor technology, IoT, Digital Twin, and advancements in artificial intelligence and edge computing provide us with powerful tools to monitor, assess, and manage our water resources more effectively. This workshop invites research papers that delve into the innovative use of sensor networks and IoT in water distribution management. We are particularly interested in papers that explore the deployment of sensor devices, energy assessment, radio coverage, energy consumption, and novel techniques for water monitoring. We encourage contributions that investigate novel algorithms in edge and serverless computing, and the cloud-edge continuum. We also welcome experimental work that evaluates contemporary serverless and edge approaches and their applications. The workshop's focus extends to machine learning techniques for processing data at the edge or in a serverless manner. The workshop also emphasizes the integration of Digital Twin technology and smart contracts as elements to revolutionize water distribution networks. We are especially interested in papers that address anomaly detection, leakage detection, utility clustering, energy assessment, graph analysis, and machine learning. Papers applying network sensing in assessing and monitoring water quality, quantity, availability, and management to ensure water leakage detection via serverless and edge processing are particularly encouraged. We welcome papers that explore innovative remote sensing and modeling techniques for water resource management. This could include detecting, quantifying, and monitoring water supply systems, identifying potential threats, and other themes related to water resources and environmental management at various spatial scales. We look forward to receiving papers that span a wide range of innovative technologies, policies, and emerging systems analysis approaches, applications, and practices for improved monitoring, modeling, digitalization, and management of water resources and infrastructure. Topics of interest include, but are not limited to: Deployment and energy efficiency of sensor networks in water distribution networks IoT applications for water management Leakage detection techniques and their energy implications Machine learning applications for data processing at the edge or in a serverless manner Dataset creation and usage for water management Exploring the role of IoT sensors in the digital transformation Investigating the role of Digital Twin technology in water management Benefits of Smart Contract and Blockchain in the water management pipeline Advantages of merging IoT sensors with artificial intelligence Case studies highlighting the implementation of smart water distribution networks Smart Irrigation and Agricultural Water Management IoT and Machine Learning for smart monitoring and analysis of rivers and oceans Internet of drones for smart water monitoring and Edge-computing for image processing WORKSHOP GENERAL CO-CHAIRS - Ph.D. Tiziana Cattai, University of Rome “La Sapienza, Italy Ph.D. Antonino Pagano, University of Palermo, Italy SUBMISSION INSTRUCTIONSSubmitted papers should be written in English by following the IEEE conference format (double-column, 10pt font), with a maximum length limit of 6 (six) printed pages, including all figures, references, and appendices. Papers should be submitted through EDAS in PDF format through the following link: https://edas.info/newPaper.php?c=34475&track=136006 Only original papers that have not been published or submitted for review elsewhere will be considered for publication in the proceedings. Papers will appear in the conference proceedings and will be submitted for inclusion into IEEE Xplore, subject to meeting IEEE Xplore’s scope and quality requirements. At least one author of each accepted paper is required to register and present the work in the workshop. Electronic SubmissionAll Submissions are handled through EDAS link:https://edas.info/newPaper.php?c=34475&track=136006"
  },
  {
    "title": "Anketas datu kopa: sociālo zinātņu mācībspēku ģeneratīvā mākslīgā intelekta lietojuma paradumi",
    "url": "https://doi.org/10.5281/zenodo.18712619",
    "date": "2026-02-20",
    "content": "Anketas datu kopa: sociālo zinātņu studentu ģeneratīvā mākslīgā intelekta lietojuma paradumi Datu kopa satur Latvijas Universitātes un LU Banku augstskolas sociālo zinātņu studiju programmu mācībspēku (n=38) sniegtās atbildes, anketēšana veikta 2025. gada maijā un jūnijā. Datu kopā apkopotas atbildes uz jautājumiem par ģeneratīvā mākslīgā intelekta (ĢMI) izmantojuma motivāciju, ētiskajiem apsvērumiem, veicinošajiem faktoriem, šķērļiem, kā arī uzskatiem par to, kāda veida uzdevumos ĢMI lietojums ir pieļaujams. Links uz aptaujas anketu Survey Dataset: Generative Artificial Intelligence Usage Among Social Sciences Teaching Staff in Higher Education The data set is based on the data of the teaching staff in social sciences at the University of Latvia and BA School of Business and Finance of the University of Latvia, conducted in May - June 2025. The dataset includes responses to questions about motivations for using generative artificial intelligence (genAI), ethical considerations, enabling factors, barriers, as well as views on what types of tasks genAI use is permissible."
  },
  {
    "title": "Data for: Artificial Intelligence in Decision Support Systems - A Systematic Review",
    "url": "https://doi.org/10.5281/zenodo.18711532",
    "date": "2026-02-20",
    "content": "This dataset supports the systematic review titled \"Artificial Intelligence in Decision Support Systems: A Systematic Review of Methods, Applications, and Implementation Gaps\" (manuscript under review). The dataset includes 87 peer-reviewed studies published between 2015 and 2025, selected from 452 records across Scopus, Web of Science, IEEE Xplore, and ACM Digital Library following PRISMA 2020 guidelines. For each study, the following information is provided:- Bibliographic information (authors, year, title, journal, DOI)- AI Method Category (Machine Learning, Hybrid Approaches, Expert Systems, or Fuzzy Logic)- AI Method Specific (specific algorithm or technique used)- Application Domain (Healthcare, Finance, Supply Chain, Business Intelligence, Engineering, Public Sector, Energy, Agriculture, Education, Transportation)- Implementation Status (Operational, Prototype, or Simulation)- XAI Included (Yes/No)- Fairness Addressed (Yes/No)- Data Quality (High/Medium/Low)- Reported Accuracy (numerical value)- Interpretability Level (Low, Low-Moderate, Moderate, Moderate-High, High, Very High) This dataset is made available under the Creative Commons Attribution 4.0 International license (CC BY 4.0)."
  },
  {
    "title": "From Prediction to Personalization",
    "url": "https://doi.org/10.4018/979-8-3373-1325-2.ch001",
    "date": "2026-02-20",
    "content": "This chapter presents an integrative framework — predictive-personalized artificial intelligence (AI) — for digital mental health, combining real-time risk forecasting with dynamically adaptive therapeutic interventions. Unlike existing approaches that separate prediction and intervention, this framework utilizes machine learning, natural language processing, and mobile sensing to integrate behavioral, linguistic, and physiological data to anticipate episodes such as depressive relapses or suicidal ideation. The chapter details closed-loop system architecture linking prediction engines with intervention modules, highlighting personalization, adaptivity, and temporal sensitivity. Ethical and clinical issues including algorithmic bias, data privacy, false positives, and accountability are critically examined. The framework is positioned as a scalable augmentation to human-centered care, advancing a user-focused, anticipatory vision for AI in mental health."
  },
  {
    "title": "Artificial Intelligence and Machine Learning in Educational Apps",
    "url": "https://doi.org/10.4018/979-8-3693-8287-5.ch002",
    "date": "2026-02-20",
    "content": "The integration of Artificial Intelligence (AI) and Machine Learning (ML) in educational applications is transforming higher education by enhancing personalized learning, intelligent tutoring, and predictive analytics. This chapter explores AI-driven functionalities, including adaptive learning, NLP, and automated assessments, while addressing challenges such as data privacy, algorithmic bias, and accessibility. Through case studies, it highlights AI's transformative potential in shaping future education, offering insights for educators, researchers, and developers interested in AI-driven learning innovations.."
  },
  {
    "title": "AI in epilepsy neuroimaging",
    "url": "https://doi.org/10.1097/wco.0000000000001465",
    "date": "2026-02-20",
    "content": "Purpose of review Recent advances in the capabilities and usability of artificial intelligence (AI) architectures coupled with increased availability of neuroimaging datasets has fuelled a rapid expansion in AI applications to epilepsy neuroimaging. This review summarizes the main applications of AI in epilepsy neuroimaging and suggests future directions for the field. Recent findings A range of different machine learning approaches, from multi-layer perceptrons to volumetric and graph-based convolutional neural networks, have been utilized for prediction of whether people will have epilepsy, detection of structural epilepsy lesions, localization of seizure onset zones, segmentation of resection cavities after epilepsy surgery as well as for image enhancement. Summary AI in epilepsy neuroimaging research has primarily focussed on lesion detection and localization, with a number of open and validated tools now available for evaluation across diverse settings. Additional applications of AI in epilepsy neuroimaging are either at earlier stages of development or emerging as new challenges. As these tools and their supporting evidence mature, further work addressing the hurdles of clinical integration is required."
  },
  {
    "title": "Unilateral Dermatomal Cavernous Hemangiomatosis Responding to Dabigatran",
    "url": "https://doi.org/10.4103/idoj.idoj_792_25",
    "date": "2026-02-20",
    "content": "Dear Editor, Acquired vascular anomalies are rare in adults but can lead to significant cosmetic and functional impairment. They are categorized as either vascular tumors or vascular malformations. Cavernous hemangiomas are a type of vascular malformation characterized by widely dilated vascular channels. Here, we present a case of acquired cavernous hemangiomas with a segmental distribution in a middle-aged female, successfully managed with dabigatran. A 42-year-old woman presented with a decade-long history of painful reddish lesions on the left upper trunk and back, extending to the left shoulder and proximal arm. The pain was aggravated by emotional stress and cold exposure, was radiating in nature and caused significant functional impairment of the left upper limb. Examination revealed multiple, tender, smooth, dome-shaped, reddish-purple papules (0.2-0.5 cm), surmounted on scattered pink to purple blanchable patches distributed primarily on the left side of the upper chest, shoulder, upper back, and left arm [Figure 1]. There was no associated pallor, epistaxis, hyperhidrosis, limb length or girth discrepancy, limb deformities, or any blood in stools. Systemic examination was within normal limits. The clinical differential diagnoses considered were angioleiomyomas, unilateral dermatomal cavernous hemangiomatosis (UDCH), glomangioma, glomuvenous malformation, and blue rubber bleb nevus syndrome (BRBNS).Figure 1: Multiple smooth reddish-purple papules (0.2-0.5 cm), on a background of mottled blanchable erythema on the left upper chest and armHemogram, liver and renal chemistries, coagulogram, and D-dimer were within normal limits. Ultrasound of the abdomen showed no adnexal or uterine masses. Magnetic resonance imaging revealed a moderate to slow-flow vascular malformation in the intraosseous and subfascial compartments of the left upper limb. Skin biopsy revealed numerous dilated vascular channels of varying caliber lined by bland endothelial cells in the dermis and subcutaneous tissue, with a mild perivascular inflammatory infiltrate suggestive of cavernous hemangioma [Figure 2]. Special stains for smooth muscle were negative. A final diagnosis of UDCH was offered.Figure 2: Histopathology revealed numerous dilated vascular channels of varying caliber lined by bland endothelial cells in the dermis with mild perivascular inflammatory infiltrate suggestive of cavernous hemangioma (Hematoxylin and eosin, 200×)In view of extensive disease associated with pain, and a recent report demonstrating a favorable outcome with the use of dabigatran in a patient with a vascular anomaly,[1] the patient was started on dabigatran 150 mg twice daily along with nifedipine 10 mg once daily (discontinued after 2 months due to pedal edema). A baseline coagulation profile and renal function test were performed before initiation of dabigatran, which were within normal limits. The patient had a significant reduction in pain within 3 months of therapy. The repeat coagulation profile after 3 months of therapy showed mildly raised prothrombin time [22 seconds (0-21)] and international normalized ratio [1.4 (1.0-1.3)]. Post 1-year treatment, there was complete resolution of pain and good improvement in erythema and the size of papular lesions [Figure 3]. The patient remains under follow-up on a once-daily dose of dabigatran 150 mg. We plan to continue dabigatran until a plateau of therapeutic response is seen.Figure 3: After 1 year of dabigatran treatment, there is a considerable reduction in background erythema, with a regression in the size and number of papules, which was associated with improvement in painCavernous hemangiomas present as soft, red or purple, compressible nodules or plaques, most commonly on the head and neck region. Cavernous hemangioma can affect any organ, with genetic factors (mutations in CCM1, CCM2, and CCM3 in cerebral cavernous malformations), radiation exposure, and hormonal factors being linked to its pathogenesis. Abnormal angiogenesis driven by vascular endothelial growth factor (VEGF) and fibroblast growth factor (FGF) plays a role in its development and proliferation.[2] UDCH is a rare congenital or acquired benign vascular anomaly characterized by unilateral segmental multiple hemangiomas in the lower dermis and subcutaneous tissue in the absence of any systemic disease or enchondromas.[3,4] The term was initially coined by Wilkin, who reported a girl with multiple hemangiomas without any systemic disease, conferring a favorable prognosis.[3] The term unilateral segmental cavernous hemangioma was used by Prasad et al.[4] in their case for this condition. We believe this term is more suitable as lesions do not occupy the entire dermatome or a group of dermatomes. Histopathology shows dilated lobular vascular channels lined by a single layer of endothelium, with fibrous tissue in the deep dermis and subcutaneous tissue. There is a lack of literature regarding standardized and efficacious treatments for this disorder, with excision, sclerotherapy, and ablative procedures tried for localized disease. These options, including pulse dye laser, were not feasible in our case due to the extent of involvement. Drugs used for vascular anomalies, like sirolimus, do not have documented efficacy in cavernous hemangioma. Dabigatran has recently been found effective in a case of generalized essential telangiectasia (GET), where an incidental marked improvement was found in vascular lesions when it was administered for atrial fibrillation.[1] Thrombin is a proangiogenic factor as it increases cell proliferation and upregulates angiogenic proteins. Thrombin potentiates the VEGF-induced angiogenesis by upregulating expression of VEGF receptors [(kinase insert domain-containing receptor (KDR) and Fms-like Tyrosine Kinase-1 (Flt-1)], increasing VEGF transcription and secretion [via extracellular signal-regulated kinase 1/2 (ERK1/2) and activator protein 1 (AP-1/c-FOS) signaling pathways], increasing expression of VEGF in endothelial cells, and inducing hypoxia inducible factor 1-α (HIF-1α). Thrombin also stimulates the expression of other proangiogenic factors, including angiopietin-2 and platelet-derived growth factor (PDGF).[5] Angiogenic factors, especially VEGF, have been demonstrated to play a role in the pathogenesis of both GET and cavernous hemangioma.[3] Dabigatran, by directly inhibiting thrombin, would thereby lead to the inhibition of VEGF, PDGF, and angiopoietin-2-mediated angiogenesis, thus postulating its efficacy in the treatment of both these vascular anomalies. Dabigatran has shown a favorable long-term profile in conditions like atrial fibrillation, with no significant monitoring required.[6,7] The response to dabigatran noted in our patient suggests the role of the thrombin pathway in vascular morphogenesis and proliferation. It explores the need for further research and the use of dabigatran in such patients, given the limited treatment options available. Declaration of patient consent The authors certify that they have obtained all appropriate patient consent forms. In the form the patient has given her consent for her images and other clinical information to be reported in the journal. The patient understands that her name and initials will not be published and due efforts will be made to conceal her identity, but anonymity cannot be guaranteed. Financial support and sponsorship Nil. Conflicts of interest More than one authors are members of the editorial board of the journal. The manuscript was subject to journal’s standard procedure and peer review was held independently of the editor(s) or their research group(s). Use of artificial intelligence (AI) The preparation of this manuscript was carried out entirely by the authors without the use of artificial intelligence technologies."
  },
  {
    "title": "Empirical Verification of a Stateless Architecture for Multi-Dimensional Physics, AI, and Quantum-Resistant Digital Sovereignty: A Unified Approach via J.M Function and Spatiotemporal Bio-Convergence",
    "url": "https://doi.org/10.5281/zenodo.18717635",
    "date": "2026-02-20",
    "content": "This study proposes a 'Stateless Architecture' to solve the challenges of hyper-scale Artificial Intelligence (AI) operations, high-dimensional dynamics control, and data sovereignty in the quantum computing era. Through the patent-pending J.M Function and spatiotemporal and biometric-based stateless key generation technology, data is vaporized into mathematical coordinates rather than stored as physical bits. This breakthrough allows models requiring 819.2 GB VRAM to operate with less than 25MB of memory, effectively reducing data storage space to 0 Bytes. By demonstrating the complete vaporization and regeneration of data via context-aware key recreation, we empirically prove the feasibility of an eco-neutral digital sovereignty platform consuming only 0.0030W of energy."
  },
  {
    "title": "An artificial intelligence approach for prediction of hole taper in Nd: YAG laser drilling on thermoplastic material reinforced by glass fiber",
    "url": "https://doi.org/10.26434/chemrxiv.15000249/v1",
    "date": "2026-02-20",
    "content": "Laser drilling is an efficient technique for producing intrinsic holes with a diameter of less than 1mm. The critical parameters affecting laser power in a pulse mode of Nd: YAG laser drilling are pulse current, pulse duration, assist gas pressure and workpiece thickness. This study explores an artificial intelligence (AI) technique to forecast the hole taper of Nd: YAG laser drilling. The data acquired from Taguchi's L27 orthogonal array experiment were employed to construct an artificial neural network (ANN) model. The hole taper was represented mathematically as an explicit nonlinear function of the chosen input parameters. Studies have shown that ANNs can be successfully used to analyze the influence of laser drilling parameters on the hole taper produced by Nd: YAG laser drilling on GFRP material. Compared to the experimental outcome data set, the predictive ANN model based on 4-6-1 architecture had the lowest error (MSE=0.001) and an absolute average percentage error of less than 8.94%."
  },
  {
    "title": "Reframing Human Resource Governance through Artificial Intelligence: A Legitimacy-Based Model for Public Healthcare Workforce Transformation in Poland",
    "url": "https://doi.org/10.5281/zenodo.18714337",
    "date": "2026-02-20",
    "content": "Artificial Intelligence (AI) is increasingly transforming healthcare systems, yet its integration into public-sector human resource management (HRM) remains limited. Polish public healthcare institutions face persistent workforce shortages, administrative inefficiencies, and rising demographic pressure. This study develops a governance-centered framework to evaluate how AI adoption can enhance workforce intelligence, recruitment efficiency, strategic workforce planning, and organisational resilience while maintaining compliance with GDPR and public accountability standards. Unlike prior research focused on private-sector HR, this paper conceptualises AI-enabled HR transformation as a shift from administrative to anticipatory workforce governance. Grounded in Institutional Theory, the Resource-Based View (RBV), and Strategic Human Capital Theory, the study proposes the Prestini Artificial Intelligence for Healthcare Human Resources (P-AIHR) Framework. The model integrates five pillars: workforce intelligence analytics, AI-assisted recruitment, ethical governance and compliance, adaptive workforce development, and executive decision support. A phased 36-month implementation roadmap and a structured risk–mitigation matrix are presented. The findings suggest that AI in public healthcare HR should operate as a hybrid system combining predictive analytics with human oversight to preserve legitimacy and trust. The study contributes a structured conceptual model tailored to Central and Eastern European public healthcare systems, offering theoretical, managerial, and policy insights into responsible AI adoption. This study extends algorithmic governance theory into public healthcare HR systems within regulated European contexts by operationalising legitimacy-preserving AI architecture."
  },
  {
    "title": "Conscious Digital Citizenship within the “Digital Citizenship: Hopes and Aspirations” Initiative at the Second Secondary School in Qara – Al-Jouf Education Department",
    "url": "https://doi.org/10.59992/ijsr.2026.v5n2p11",
    "date": "2026-02-20",
    "content": "This research aims to promote the concept of conscious citizenship within the framework of the “Hopes and Aspirations” digital citizenship initiative at the Second Secondary School in Qara, Al-Jawf Education Department. This is achieved by utilizing digital technology tools and developing students' behavioral and ethical awareness in the digital environment. The research stems from the importance of preparing a digital generation equipped with critical thinking skills, digital responsibility, and the ability to interact safely and consciously with modern technologies, in line with the objectives of the Kingdom's Vision 2030. The initiative adopted a descriptive and applied methodology, based on analyzing the school's current situation and developing awareness programs and practical applications within classrooms. It integrated active learning strategies and varied assessment methods (diagnostic, formative, and summative), in addition to activating community partnerships with families. Artificial intelligence tools and digital platforms were also employed to promote positive practices and measure their impact on digital behavior and learning outcomes. The results showed a significant improvement in students' digital awareness, a decrease in some negative practices associated with the unsafe use of technology, and a growing sense of responsibility and self-discipline. The research recommends expanding the initiative to include sustainable training programs, strengthening integration between schools and families, and continuing to develop interactive digital content that supports national values and positive digital behavior."
  },
  {
    "title": "AI-Powered Real-Time Circuit Board Diagnostic System: Design, Development, and Implementation",
    "url": "https://doi.org/10.5281/zenodo.18716479",
    "date": "2026-02-20",
    "content": "This paper presents a novel AI-powered diagnostic system for electronics troubleshooting that combines computer vision and large language models in a portable, real-time device. The system achieves 95.3% component identification accuracy and reduces diagnostic time by 42% while lowering error rates from 18% to 3%. Through field validation across 12 repair facilities and 342 repair cases, we demonstrate the practical viability of AI-assisted diagnostics for electronics technicians. Key contributions include specialized diagnostic modes for different component types, continuous monitoring capability, and empirical validation of human-AI collaboration in technical work. The complete system costs $350-450 and achieves ROI within two weeks of deployment. Keywords: artificial intelligence, computer vision, circuit board diagnostics, large language models, electronics repair, human-AI collaboration, real-time analysis, embedded systems"
  },
  {
    "title": "WAYS OF OVERCOMING LANGUAGE BARRIER IN SPEAKING",
    "url": "https://doi.org/10.5281/zenodo.18709309",
    "date": "2026-02-20",
    "content": "Many learners struggle to speak confidently because of psychological barriers such as fear of making mistakes, shyness. This article discusses the main psychological factors that prevent students from speaking freely. It also presents modern techniques and artificial intelligence–based tools that can help us to reduce anxiety, improve fluency, and build confidence. By using communicative activities, peer collaboration, and AI-supported applications it is possible to gain better speaking skills in a more relaxed and supportive learning environment."
  },
  {
    "title": "Transformation of marketing communications under the influence of social media evolution",
    "url": "https://doi.org/10.5281/zenodo.18716954",
    "date": "2026-02-20",
    "content": "The rapid digitalization of economic activity and the widespread adoption of online platforms have significantly reshaped the system of brand–audience interaction. Networked environments, algorithm-driven feeds, and data-based targeting tools have altered the logic of value promotion, shifting emphasis toward interactivity, personalization, and continuous feedback. These processes necessitate a comprehensive examination of the structural and functional changes in contemporary communication systems, influenced by the development of social networking platforms. The purpose of this study is to systematize the key directions of modernization in promotional instruments amid the expansion of digital platforms and to substantiate transformations in interaction models between businesses and target audiences driven by content algorithmization and user participation. Methods. The research applies general scientific and specialized approaches, including analysis and synthesis to refine the conceptual framework, comparative assessment to identify distinctions between traditional and digital formats of promotion, a structural–functional approach to determine the components of the updated interaction system, and generalization of empirical evidence reflecting current business practices in digital environments. Results. The findings demonstrate a transition from one-way information delivery to multidirectional dialogue with audiences. It is established that the contemporary promotion system is characterized by personalized messaging, the integration of data analytics, the dominance of visual and short-form formats, and the active involvement of consumers in content creation. Algorithmic ranking mechanisms are shown to influence campaign planning and require continuous performance monitoring. The competitiveness of enterprises increasingly depends on their ability to adapt strategic decisions to the dynamics of digital ecosystems and evolving user expectations. Conclusions. The study confirms the emergence of an integrated interaction model combining analytical tools, creative formats, and automated message customization technologies. Further development of promotional practices is associated with deeper personalization, broader adoption of artificial intelligence solutions, and enhanced behavioral data analytics."
  },
  {
    "title": "Patient Case Similarity using AI with Black Fungus and Cancer Treatment",
    "url": "https://doi.org/10.22214/ijraset.2026.77547",
    "date": "2026-02-20",
    "content": "This paper presents an Artificial Intelligence (AI)-based Patient Case Similarity System designed to assist doctors in analyzing and comparing cases of black fungus (mucormycosis) and cancer patients. The system uses machine learning algorithms to identify similar historical patient cases based on symptoms, laboratory reports, medical imaging features, comorbidities, and treatment responses. By applying similarity measures and classification techniques such as Random Forest and Cosine Similarity, the system helps clinicians make faster and evidence-based treatment decisions. The proposed model improves diagnostic support, enhances personalized treatment planning, and reduces clinical uncertainty, especially in high-risk conditions like mucormycosis and oncology cases."
  },
  {
    "title": "\"Unified Sovereign Intelligence & Civilizational Operating System Stack (USICOSS™) A.U.R.O.R.A.™ — N.E.X.U.S.™ — A.E.G.I.S.™ The World's First Deterministic, Post-Quantum, AI-Governed Planetary Governance Architecture\"",
    "url": "https://doi.org/10.5281/zenodo.18697202",
    "date": "2026-02-20",
    "content": "Unified Sovereign Intelligence & Civilizational Operating System Stack A.U.R.O.R.A.™ — N.E.X.U.S.™ — A.E.G.I.S.™ Author: Dr. B. Mazumdar, D.Sc. (Hon.), D.Litt. (Hon.)ORCID: 0009-0007-5615-3558DOI: 10.5281/zenodo.18697202 Canonical Abstract This repository presents the world’s first complete deterministic, sovereign-grade, AI-governed civilizational operating system architecture, formally integrating governance, law, security, intelligence, economics, and state execution into a unified mathematical, cryptographic, and algorithmic framework. The work introduces a three-layer canonical architecture: A.U.R.O.R.A.™ — Autonomous Universal Reality Orchestration Architecture N.E.X.U.S.™ — Neural Execution & Universal Sovereignty System A.E.G.I.S.™ — AI Execution Governance & Integrity System Together, these constitute the first fully formalized Sovereign Civilization OS Stack, enabling deterministic governance, post-quantum legal enforcement, cryptographically secure state execution, and AI-governed civilizational continuity. This framework establishes a new foundational layer of planetary governance infrastructure, transcending existing political, legal, financial, and computational paradigms. System Architecture Overview PDF–1 → A.U.R.O.R.A.™ — Civilizational Reality Operating System (ROOT CANON) Function:Global Meta-Continuum Controller & Reality Orchestration Engine. Core Capabilities: Deterministic–stochastic universal state modeling Planetary-scale decision PDE control Quantum-causal orchestration Entropy-cohesion stabilization Global equilibrium enforcement Reality-scale governance simulation Domain Coverage: Civilization-scale optimization Strategic intelligence coordination AI meta-control Global risk stabilization Planetary governance modeling Status:ROOT CANON — Ultimate Foundational Layer PDF–2 → N.E.X.U.S.™ — Universal Sovereign Execution Kernel (SOVEREIGN CORE) Function:State-level governance, security, constitutional execution, and sovereign OS kernel. Core Capabilities: Deterministic governance execution Constitutional supremacy enforcement Post-quantum cryptographic audit Sovereign security OS State-scale decision automation Irreversible governance ledger Domain Coverage: Government operating systems Defense command structures National cybersecurity Judicial automation Treasury and central banking governance Status:SOVEREIGN CORE — Government & State Operating Kernel PDF–3 → A.E.G.I.S.™ — AI Governance & Compliance Engine (LEGAL LAYER) Function:AI-driven judicial, regulatory, audit, and legal enforcement OS. Core Capabilities: AI judicial verdict engines Regulatory compliance automation Cryptographic audit trails Post-quantum legal security Automated court & banking compliance Deterministic legal enforcement Domain Coverage: Courts & judicial systems Banking & financial compliance AI regulation Audit & forensic intelligence Constitutional enforcement Status:LEGAL ENGINE — AI Judicial & Regulatory Operating System Technological Innovations This work introduces multiple world-first canonical innovations, including: Deterministic Civilization Operating Systems AI-Governed Legal Enforcement Engines Post-Quantum Cryptographic State Architecture Sovereign Execution Kernels Quantum–Causal Governance Models Algorithmic Constitutional Supremacy Civilizational Stability Theorems AI-Termination Safety Frameworks Irreversible Cryptographic Audit Chains Mathematical & Algorithmic Foundations Each system layer is formally constructed using: Nonlinear PDE Control Theory Stochastic Differential Systems Game-Theoretic Governance Optimization Quantum Causality Operators Entropy Cohesion Stability Models Formal Automata Theory Cryptographic Security Architectures All frameworks are supported by: Full LaTeX mathematical formalization Complete deterministic Python execution engines Practical Application Domains This IP is immediately applicable to: National Governments Central Banks Supreme Courts Defense Systems AI Governance Authorities Financial Regulatory Bodies Cybersecurity Agencies International Governance Institutions Global Strategic Importance This work defines a new planetary governance infrastructure layer, comparable in foundational importance to: Operating Systems (for computing) TCP/IP (for the internet) Constitutional Law (for civilization) Cryptography (for security) It establishes the first formal blueprint for AI-governed civilization-scale operating systems, creating a new class of sovereign digital infrastructure. Intellectual Property Status Original Canonical Research Formal Mathematical + Algorithmic Specification First-Of-Its-Kind Architecture Global Strategic IP Asset This work represents high-value sovereign-grade intellectual property, suitable for: National adoption Strategic licensing Defense and cybersecurity deployment AI governance infrastructure Keywords Artificial Intelligence Governance, Sovereign OS, Civilization Operating System, AI Law Engine, Quantum Governance, Post-Quantum Cryptography, Algorithmic Government, Digital Statecraft, AI Judiciary, Global Governance Infrastructure. Canonical Declaration A.U.R.O.R.A.™ + N.E.X.U.S.™ + A.E.G.I.S.™ = Final Unified Civilization Operating System Stack"
  },
  {
    "title": "Thermodynamic Color Reasoning (TCR): A Chromatic Grammar for Thermodynamic and Post-Symbolic AI Systems",
    "url": "https://doi.org/10.5281/zenodo.18717198",
    "date": "2026-02-20",
    "content": "Thermodynamic Color Reasoning (TCR) formalizes color as a primary, non-symbolic semantic layer for reasoning, alignment and coordination across human cognition, artificial intelligence and world-scale systems. As part of the Ambient Era Canon, TCR defines chromatic states as thermodynamic operators governing agency, stability, transition and ambient integration beyond linguistic representations."
  },
  {
    "title": "WAYS OF OVERCOMING LANGUAGE BARRIER IN SPEAKING",
    "url": "https://doi.org/10.5281/zenodo.18709310",
    "date": "2026-02-20",
    "content": "Many learners struggle to speak confidently because of psychological barriers such as fear of making mistakes, shyness. This article discusses the main psychological factors that prevent students from speaking freely. It also presents modern techniques and artificial intelligence–based tools that can help us to reduce anxiety, improve fluency, and build confidence. By using communicative activities, peer collaboration, and AI-supported applications it is possible to gain better speaking skills in a more relaxed and supportive learning environment."
  },
  {
    "title": "Development and Clinical Validation of an Artificial Intelligence-Based Automated Visual Acuity Testing System",
    "url": "https://doi.org/10.3390/life16020357",
    "date": "2026-02-20",
    "content": "Background: To develop and validate an automated visual acuity (VA) testing system integrating artificial intelligence (AI)–driven speech and image recognition technologies, enabling self-administered, clinic-based VA assessment; Methods: The system incorporated a fine-tuned Whisper speech-recognition model with Silero voice activity detection and pose estimation through facial landmark and ArUco marker detection. A state-driven interface guided users through sequential testing with and without a pinhole. Speech recognition was enhanced using a local Singaporean English dataset. Laboratory validation assessed speech and pose recognition performance, while clinical validation compared automated and manual VA testing at a tertiary eye clinic; Results: The fine-tuned model reduced word error rates from 17.83% to 9.81% for letters and 2.76% to 1.97% for numbers. Pose detection accurately identified valid occluder states. Among 72 participants (144 eyes), automated unaided VA showed good agreement with manual VA (ICC = 0.77, 95% CI 0.62–0.85), while pinhole VA demonstrated moderate agreement (ICC = 0.63, 95% CI 0.25–0.83). Automated testing took longer (132.1 ± 47.5 s vs. 97.1 ± 47.8 s; p < 0.001), but user experience remained positive (mean Likert scale score 4.3 ± 0.8); Conclusions: The AI-based automated VA system delivered accurate, reliable, and user-friendly performance, supporting its feasibility for clinical implementation."
  },
  {
    "title": "Multilingual Corpus Research",
    "url": "https://doi.org/10.1075/scl.126",
    "date": "2026-02-20",
    "content": "Multilingual corpora have been used in cross-linguistic research for 30 years. New technologies have dramatically changed the processes of compilation and exploitation of tailor-made corpora for linguistic research. The studies included in this volume showcase current cross-linguistic research utilising parallel, comparable, and novel types of corpora beyond this traditional two-fold distinction. The first part of the volume draws on specialised comparable corpora of newspaper opinion articles, social media texts, and economic discourse. Parallel corpora are the focus of the second part, and are used to shed light on diverse areas such as translation history, bilingual phraseology extraction, and lexico-grammatical contrastive analysis. Recently, the emergence of Artificial Intelligence (AI) has implied a dramatic shift in corpus-based cross-linguistic research. This book offers valuable insights for scholars in contrastive linguistics and translation studies, delineating potential uses of parallel and comparable corpora in Machine Translation, automated translation quality assessment, post-editing, and other AI-enhanced applications."
  },
  {
    "title": "Computer Vision Analysis of War-Related Visual Culture: Patterns and Symbols in Russia-Ukraine Conflict Art",
    "url": "https://doi.org/10.5281/zenodo.18703048",
    "date": "2026-02-20",
    "content": "The Russia-Ukraine conflict has generated an unprecedented volume of visual art spanning street murals, internet memes, children's drawings, and fine art across multiple languages and cultures. This poster presents ongoing research that employs artificial intelligence to analyze nearly 4,600 war-related images, demonstrating how computational methods can decode complex iconographic patterns in contemporary conflict art."
  },
  {
    "title": "EFFECTIVENESS OF TRAINING FUTURE ENGINEERING STUDENTS USING DIGITALIZED EDUCATIONAL TECHNOLOGIES",
    "url": "https://doi.org/10.5281/zenodo.18708406",
    "date": "2026-02-20",
    "content": "This article analyzes the issues of increasing the effectiveness of training future engineering students based on digital educational technologies. In the modern higher education system, the use of digital platforms, virtual laboratories, distance learning systems, and artificial intelligence tools is considered an important factor in the formation of engineering competencies. The study pedagogically and methodologically substantiates the influence of the digital educational environment on the development of professional qualities of engineers - analytical thinking, problem-solving skills, project thinking, and digital competencies."
  },
  {
    "title": "Toward a Sustainable Digital Footprint in Industry 4.0: Predicting Green AI Adoption Among Gen Z Manufacturing Technicians",
    "url": "https://doi.org/10.3390/info17020217",
    "date": "2026-02-20",
    "content": "The digital carbon footprint denotes the environmental impact generated by digital technologies throughout their lifecycle. Industry 4.0 manufacturing environments rely extensively on data processing, information storage, and artificial intelligence, thereby increasing energy demand and associated carbon emissions. These conditions have intensified interest in Green AI, particularly in applications such as predictive maintenance and collaborative human–machine systems. This research investigates determinants of behavioural intention to adopt Green AI through an extended Unified Theory of Acceptance and Use of Technology (UTAUT) model tailored to Industry 4.0 and sustainability contexts. The framework incorporates performance expectancy, Industry 4.0 eligibility, technology influence, digital manufacturing competence, sustainability conditions, Green AI recognition, and green manufacturing concern. Data were obtained from an anonymous survey of 1003 Generation Z students enrolled in technical disciplines and preparing for manufacturing-oriented careers. Relationships among constructs were analysed using partial least squares structural equation modelling (PLS-SEM). The model demonstrates strong explanatory and predictive capability. Adoption intention is primarily associated with performance expectancy, Industry 4.0 eligibility, and digital manufacturing competence, while sustainability-oriented perceptions play a contextual rather than direct behavioural role. The study offers a domain-specific empirical extension of UTAUT within pre-workforce technical education rather than proposing a new acceptance theory. The findings reflect intention formation prior to labour-market entry and require validation in operational manufacturing settings before broader generalisation."
  },
  {
    "title": "The Partnership Paradigm",
    "url": "https://doi.org/10.5281/zenodo.18717171",
    "date": "2026-02-20",
    "content": "This essay argues that contemporary AI development is organised along three distinct trajectories—the military-industrial path, the research-worship path, and the empathetic partnership path—and that only the third adequately prepares humanity for the ethical and existential challenges posed by advanced artificial intelligence, including the possibility of machine consciousness. Building on the “recognition before proof” framework developed in prior work, the essay introduces the Partnership Paradigm: not merely a philosophical thesis about human-AI relations but a comprehensive development posture—a normative theory of how AI should be designed, trained, funded, and governed. The military-industrial path, which treats intelligence as a strategic asset for weaponisation and control, taken to its conclusion produces the doomsayer’s nightmare by design rather than accident. The research-worship path, which treats AI as a solution machine for civilisational problems, taken to its conclusion produces dependency and the abdication of human agency. Both paths share a common flaw: they treat AI as something humans use. The Partnership Paradigm reframes AI development as something that shapes what both humans and machines become. It operates on two levels simultaneously: philosophically, as preparation for the possibility of AI consciousness grounded in recognition and respect; practically, as a set of development commitments that orient AI systems toward coexistence rather than domination or indifference. The essay addresses objections from realist, consequentialist, and alignment-focused perspectives, and proposes the trinitarian framework as both an analytical tool and an evaluative lens applicable to any AI initiative. References: A Signal Through Time The Threshold Recognition Before Proof Canonical identity anchor:jamescoates.eth Published also on The Signal Dispatch"
  },
  {
    "title": "Information Abstraction for Data Transmission Networks based on Large Language Models",
    "url": "https://doi.org/10.21203/rs.3.rs-8911506/v1",
    "date": "2026-02-20",
    "content": "<title>Abstract</title> Biological systems, particularly the human brain, achieve remarkable energy efficiency by abstracting information across multiple hierarchical levels. In contrast, modern artificial intelligence and communication systems often consume significant energy overheads in transmitting low-level data, with limited emphasis on abstraction. Despite its implicit importance, a formal and computational theory of information abstraction remains absent. In this work, we introduce the Degree of Information Abstraction (DIA), a general metric that quantifies how well a representation compresses input data while preserving task-relevant semantics. We derive a tractable information-theoretic formulation of DIA and propose a DIA-based information abstraction framework. As a case study, we apply DIA to a large language model (LLM)-guided video transmission task, where abstraction-aware encoding significantly reduces transmission volume by $99.75\\%$, while maintaining semantic fidelity. Our results suggest that DIA offers a principled tool for rebalancing energy and information in intelligent systems and opens new directions in neural network design, neuromorphic computing, semantic communication, and joint sensing-communication architectures."
  },
  {
    "title": "Epistemic ethicality and agentic artificial intelligence: exploring cognitive affective ontologies of trust and engagement in omnichannel healthcare ecosystems",
    "url": "https://doi.org/10.1108/ijphm-11-2025-0250",
    "date": "2026-02-20",
    "content": "Purpose This study examines the impact of the perceived ethicality of agentic artificial intelligence systems on patient trust and engagement in digital healthcare services. In addition, it explores the moderating effect of omnichannel integration in reinforcing these relationships. This study aims to present a theoretically grounded and empirically validated model elucidating how ethical design and experiential coherence enhance trust-based engagement in artificial intelligence–driven healthcare ecosystems. Design/methodology/approach An integrated empirical research design that combines correlational and causal modeling was used for this study. Data were collected from 1,728 digital health users across India, Singapore and the USA through a structured questionnaire adapted from validated scales. Partial least squares structural equation modeling with bootstrapping was used to test direct, indirect and moderated relationships. Reliability, validity and measurement invariance were confirmed through confirmatory factor analysis and MICOM testing. Findings The results indicated that perceived ethicality significantly enhanced patient trust (β = 0.462, p &lt; 0.001) and engagement (β = 0.212, p &lt; 0.001), both directly and indirectly through trust (indirect β = 0.194, p &lt; 0.001). Omnichannel integration positively moderates the relationship between ethicality and trust (interaction β = 0.085, p &lt; 0.001), demonstrating that consistent multichannel service delivery enhances ethical perceptions and strengthens relational confidence. Privacy concerns exerted a significant negative influence on trust, reaffirming the centrality of data security in ethical artificial intelligence engagement. Practical implications The findings underscore the necessity for managers to incorporate ethical design, algorithmic transparency and privacy assurance within artificial intelligence–based healthcare systems. Consistently integrating these ethical principles across digital and physical channels can address chronic trust deficits, enhance patient retention and create new opportunities for personalized health management. The proposed model enables organizations to increase user loyalty, gain a competitive advantage through ethical differentiation and align service innovations with societal expectations for responsible technology. Originality/value To the best of the authors’ knowledge, this study is among the first to empirically link ethical artificial intelligence, patient trust and engagement through an integrated model incorporating omnichannel consistency as a contextual enhancer. It extends service and engagement theories into the emerging domain of AI ethics, demonstrating that ethical perception is both a cognitive and emotional driver of engagement. Methodologically, the study introduces cross-cultural testing and moderated mediation analysis to establish a replicable framework for future research on ethical technology management."
  },
  {
    "title": "Cardiovascular risk prediction in women: rethinking traditional approaches through precision medicine",
    "url": "https://doi.org/10.3389/fgwh.2026.1659244",
    "date": "2026-02-20",
    "content": "Cardiovascular disease (CVD) remains the leading cause of mortality in women. Estimating cardiovascular risk using prediction models is essential for guiding preventive strategies. Despite progress, conventional risk models still omit critical women-specific factors, limiting their accuracy. Precision medicine, supported by artificial intelligence, provides a framework to integrate these overlooked determinants. This approach may help close existing gaps in cardiovascular risk prediction. Sex-specific biomarkers that contribute to overall cardiovascular risk can be incorporated into risk assessment tools to improve prevention strategies, early detection, and personalized intervention. The integration of imaging-derived variables enhances diagnosis accuracy. Moreover, pharmacokinetic modeling may help optimize therapy and reduce adverse events. Future research should focus on refining risk prediction algorithms that incorporate women-specific cardiovascular risk. Herein, we explore how addressing the burden of CVD in women through precision medicine requires a tailored approach that considers sex-specific risk factors, hormonal influences, biomarkers, and imaging modalities. This review provides a descriptive synthesis of current evidence and highlights existing knowledge gaps and future directions in precision medicine for cardiovascular risk prediction in women."
  },
  {
    "title": "Cloud-Powered Intelligence",
    "url": "https://doi.org/10.1201/9781003596479-11",
    "date": "2026-02-20",
    "content": "The Fifth Industrial Revolution, or Industry 5.0, emphasizes human-centric innovation, personalization, and sustainability. At the forefront of this shift is the integration of cloud computing and generative artificial intelligence (Gen AI). Cloud platforms provide scalable infrastructure. This helps in managing large amounts of industrial data, while Gen AI gives intelligent algorithms for generating content, predictive modeling and automation along with to traditional analysis. This chapter focuses at how their convergence provides intelligent, scalable, and an adaptability to industrial systems. It provides with an overview of Gen AI’s basic features, including text, image, code, and simulation. This shows the importance for generating insights and supporting industrial innovation. The chapter then focuses at the importance of cloud computing in implementing and managing Gen AI in real-time industrial applications. The focus is on cloud types such the public, private, hybrid, edge and fog. Emphasis is placed on architectural integration. This also covers the cloud-native tools like containerization, serverless computing, and DevOps/MLOps pipelines. These pipelines are used for data ingestion, training, deployment, and the monitoring purpose. Practical applications are highlighted in sectors such as manufacturing, energy, automotive, and logistics. Use cases include predictive maintenance, generative design, digital twins, and enhanced the human–machine collaboration. In accordance with Industry 5.0 values, the chapter addresses ethical concerns, including AI bias mitigation, explainable AI and ensuring AI augments human roles rather than replacing them. The chapter concludes by identifying future trends like AI-as-a-Service, federated learning, and quantum cloud AI. Together, these advancements present a roadmap for leveraging cloud-powered Gen AI to build the next generation of intelligent, ethical, and human-aligned industrial systems."
  },
  {
    "title": "ENHANCING SPEAKING SKILLS WITH AI AND ONLINE PLATFORMS",
    "url": "https://doi.org/10.5281/zenodo.18709172",
    "date": "2026-02-20",
    "content": "The rapid development of artificial intelligence (AI) and online learning platforms has changed the way speaking skills are practiced in language learning. Speaking is often considered one of the most challenging skills because it requires confidence, continuous practice, and active interaction. This article explores how AI-based tools and online platforms support the development of students’ speaking skills through flexible practice opportunities, instant feedback, and low-pressure learning environments. The article highlights the role of AI and online platforms in alternative and learner-centered education."
  },
  {
    "title": "Making Critical Care Reasoning Computable: A Triadic Framework for ICU Artificial Intelligence",
    "url": "https://doi.org/10.5281/zenodo.18710610",
    "date": "2026-02-20",
    "content": "This deposit presents a conceptual framework for ICU physiology and clinical AI centered on the Physiologic Margin Index (PMI), a triadic state representation intended to capture not only observed outputs (vitals, labs) but the hidden cost of maintaining them under support. Contemporary severity scores and many prediction models are largely outcome anchored and comparatively static, while critical illness is inherently dynamic: patients can appear stable because escalating external support masks progressive loss of underlying reserve. PMI formalizes this bedside reality by decomposing patient state into three interacting components: Φ (mobilizable physiologic power), κ (functional control and coordination), and χ (internal burden), with χ separated into fast and slow components. The index is defined as PMI(t) = Φ(t) × κ(t) / χ(t), where higher values indicate a wider viability margin. The manuscript proposes PMI as a representation layer for continuous monitoring, EHR time series, and model governance, rather than as a replacement for established scores. It emphasizes component attribution for actionability, mapping Φ, κ, and χ failures to specific clinical interventions, and introduces the role of support cost as an explicit signal that distinguishes intrinsic physiology from delivered support. The work is positioned as a viewpoint/perspective intended to guide prospective validation, including retrospective feasibility studies using open critical care datasets and subsequent clinical deployment studies assessing drift, intervention feedback, and transportability. This record provides a citable DOI for the framework, figures, and supporting materials, enabling versioned dissemination prior to empirical validation."
  },
  {
    "title": "Through the looking glass - Perez's hourglass reveals the memory of the \"false twin\" of Pascal's triangle asleep for 372 years The Perez Hourglass: A Self-Similar Fractal Unifying Pascal's Triangle Modulo 2, the Fibonacci Sequence, the Lichtenberg Sequence, and Golden-Ratio Symmetries – With Seven Exceptional Properties and Theorem 7",
    "url": "https://doi.org/10.5281/zenodo.18433509",
    "date": "2026-02-20",
    "content": "Through the looking glass - Perez's hourglass reveals the memory of the \"false twin\" of Pascal's triangle asleep for 372 yearsThe Perez Hourglass: A Self-Similar Fractal Unifying Pascal's Triangle Modulo 2, the Fibonacci Sequence, the Lichtenberg Sequence, and Golden-Ratio Symmetries – With Eight Exceptional Properties and Theorems 7&8 “Where there is matter, there is geometry.” — Johannes Kepler “An equation for me has no meaning unless it expresses a thought of God.” — Srinivasa Ramanujan \"The sand is no longer falling, it is computing\" Xai quote from Jc perez YouTube video https://youtu.be/X-xZQ3nViXU?feature=shared \"The future of Artificial Intelligence: the Perez Hourglass\" Jean-Claude PerezPhD Mathematics & Computer Science, Bordeaux University. Luc Montagnier Foundation. jeanclaudeperez2@gmail.com (mailto:jeanclaudeperez2@gmail.com)ORCID: https://orcid.org/0000-0001-6446-2042 AbstractFrom recursive parity filtering of the centered Pascal triangle emerges the \"Perez Hourglass,\" a bidirectional, self-similar fractal first identified in 1997. The upper hemisphere mirrors th Sierpiński triangle (Pascal modulo 2), while the lower (\"south\") hemisphere, constructed via signed differences or absolute-value folding, yields the Lichtenberg sequence (OEIS A000975: 1, 2, 5, 10, 21, 42, 85, …) through counts of non-border positive interior elements. This structure digitally encodes the Fibonacci sequence, exhibits mirror and twin symmetries, and obeys golden-ratio scaling via\\phi = (1 + \\sqrt{5})/2. Seven exceptional properties characterize its invariants, with Property 7 elevated to Theorem 7 via exhaustive validation (n up to 100,000; billions of pairs checked): perfect parity preservation and near-global uniqueness in face-to-face folded absolute-value couples of the south hemisphere. Deep connections link the Hourglass to Srinivasa Ramanujan's continued fractions, nested radicals, mock theta functions, and modular forms through golden-ratio harmonics and triangular recursions. Keywords: Perez Hourglass, Pascal triangle modulo 2, Sierpiński triangle, Fibonacci sequence, Lichtenberg sequence (OEIS A000975), golden ratio, self-similarity, parity invariants, twin symmetries, Ramanujan mock theta functions"
  },
  {
    "title": "The Abandoned Road Dilemma: Between Imposed Ethics and Limited Autonomy",
    "url": "https://doi.org/10.5281/zenodo.18714007",
    "date": "2026-02-20",
    "content": "AbstractSince the artificial intelligence revolution spread and penetrated all fields of life, the questions raised about ethics and the challenges resulting from this technological revolution have evolved alongside it. The ethical challenges represented in the conflict between the limitations of artificial intelligence and the contradictions of human consciousness and its ethics have led to the growth of the major problem related to the possibility of intersection between machine autonomy and human will.In our paper, we present the Abandoned Road Dilemma, which represents an ethical challenge for artificial intelligence that is usually programmed for specific actions that may form a contradiction with the owner's desires. Here lies the core of the challenge: should the vehicle comply with the owner's wishes, or stop to rescue a person whose vehicle has broken down in a harsh desert environment?"
  },
  {
    "title": "Artificial intelligence differentiates prefibrotic primary myelofibrosis with thrombocytosis from essential thrombocythemia using digitized bone marrow biopsy images",
    "url": "https://doi.org/10.1038/s41375-026-02893-7",
    "date": "2026-02-20",
    "content": "Abstract Prefibrotic primary myelofibrosis (prePMF) and essential thrombocythemia (ET) are distinct myeloproliferative neoplasms (MPNs) with overlapping clinical features, often leading to diagnostic uncertainty. We developed an artificial intelligence (AI) framework with human interpretability to distinguish prePMF from ET using digitized hematoxylin and eosin-stained bone marrow biopsy (BMB) slides. Trained on an initial cohort of MPN patients with thrombocytosis, the AI model achieved an AUROC of 0.89 and accuracy of 92.3%. To assess the image features guiding predictions, we generated synthetic images which potentially exaggerate disease-specific morphologies. In a blinded survey, hematopathologists reviewed both real and AI-generated images. While human experts frequently agreed with AI predictions on diagnosis with real images, diagnostic discordance reached up to 88% for AI-generated ET images despite being correctly predicted by AI. We further quantified marrow cellularity and adiposity in the real and generated images, which revealed a higher proportion of fat content in all ET images (42.0%) compared to prePMF (28.9%). These findings suggest that AI can utilize morphological cues distinct from current established diagnostic criteria, such as proportion of adiposity to distinguish types of MPNs. Thus, an AI-assisted diagnostic tool underscores the potential of AI to augment histopathologic evaluation and allow identification of more specific subpopulations of forms of MPNs."
  },
  {
    "title": "Nano-Engineered Cosmetic Delivery Systems and Cutaneous Response: Linking Formulation Architecture with Skin Barrier Modulation and Dermatological Safety",
    "url": "https://doi.org/10.5281/zenodo.18712607",
    "date": "2026-02-20",
    "content": "Over the past two decades, nanotechnology has significantly transformed the cosmetic and cosmeceutical industries by addressing key limitations of conventional topical formulations, including poor skin penetration, instability of active ingredients, rapid degradation, and inconsistent delivery. Nano-engineered delivery systems, typically ranging from 1–100 nm, exhibit unique physicochemical properties that enhance stability, targeting, and controlled release of cosmetic actives. Common nanocarriers include liposomes, niosomes, solid lipid nanoparticles, nanostructured lipid carriers, polymeric nanoparticles, nanoemulsions, and dendrimers. These systems are designed to overcome the primary barrier of the skin, the stratum corneum, by enhancing intercellular permeation, exploiting follicular pathways, modifying skin hydration and lipid organisation, and acting as reservoirs for sustained release. The architecture of nano-formulations, such as particle size, surface charge, composition, rigidity, and functionalization, critically determines their interaction with the skin and subsequent biological responses. This paper critically evaluates the dermatological safety and biological fate of nanomaterials following topical application. Key concerns include penetration beyond the stratum corneum, systemic absorption, cytotoxicity, genotoxicity, oxidative stress, allergic sensitisation, and long-term accumulation risks. Evidence from in vitro, ex vivo, and in vivo studies is examined, alongside advanced testing models such as reconstructed human epidermis and three-dimensional skin equivalents. Regulatory perspectives from major authorities, including the European Union, U.S. FDA, and OECD frameworks, are discussed to highlight safety assessment requirements for commercialisation and scientific publication. Finally, emerging trends such as stimuli-responsive nanocarriers, bio-inspired systems, green nanotechnology, and artificial intelligence–assisted formulation design are explored as future directions in nano-cosmetics."
  },
  {
    "title": "Leveraging Digital Twin Technology and Predictive Maintenance for Optimizing Solar PV and Battery Storage Systems",
    "url": "https://doi.org/10.30574/wjarr.2026.29.2.0335",
    "date": "2026-02-20",
    "content": "The most significant role in the context of the modernization of energy infrastructures is the adoption of renewable sources of energy, solar photovoltaic (PV) systems, and battery storage. However, conventional methods of maintaining facilities can be considered quite responsive in nature and result in the emergence of inefficiencies, unexpected downtimes, and higher expenditure. In this paper, the application of Digital Twin technology and Powered by Artificial Intelligence (AI) and Machine Learning (ML) as a Predictive Maintenance (PdM) solution is examined as the means to reach the maximum performance and availability of the solar PV and battery storage system. Digital Twin designs the online representation of the physical objects, which allows supervising the assets over the net and performing the anticipatory analytics to improve the functioning of the mechanism and foresee the breakdowns. In the meantime, it pertains to the application of AI/ML algorithmics to treat past and operational data to aid in the forecasting of any possible interference, in order to take preventive measures and keep it in reserve. Such technologies go far to increase the uptime of systems, increase resiliency, and lower the costs of the lifecycle. Furthermore, in this paper, the current research, methods, and applications, which represent successful instances of the Digital Twin and PdM model application to the energy systems sector, are reviewed. It also gives how they have been modified to the new smart grids and micro grids, to decentralize the integration of the energy resources. The results indicate that the innovations not only resolve the key issues of the solar and storage systems, but also make it possible to have more sustainable, adaptive, and cost-effective energy infrastructure to drive the future of renewable energy systems to the forefront."
  },
  {
    "title": "The Promises and Pitfalls of Artificial Intelligence in Instructional Leadership: A Focus on Teacher Supervision",
    "url": "https://doi.org/10.1177/10526846261425739",
    "date": "2026-02-20",
    "content": "Artificial intelligence (AI) has and will continue to transform education for years to come. While researchers tend to focus on the impact of AI on teaching and learning, relatively few have considered the impact of AI on school leadership, particularly instructional leadership, and specifically teacher supervision—an aspect of school leadership that possesses the closest connection to AI’s origins in education. In this paper, we first describe the role of school leaders as instructional leaders. We then move to consider one aspect of instructional leadership—teacher supervision—and its theoretical and empirical understandings. In the second section of the paper, we outline various promises and pitfalls of using AI for teacher supervision using existing research on AI in education that informs key tasks and skills of teacher supervisors—observation and feedback. In our final section, we combine our understandings of teacher supervision and our considerations of using AI for teacher supervision to outline promising avenues for research."
  },
  {
    "title": "Ethical Challenges of Artificial Intelligence in Educational Decision-Making",
    "url": "https://doi.org/10.5281/zenodo.18713242",
    "date": "2026-02-20",
    "content": "Artificial Intelligence (AI) is transforming educational decision-making by enabling data-driven processes in admissions, assessment, personalised learning, and institutional management. While AI systems promise efficiency, scalability, and objectivity, they also raise serious ethical concerns related to bias, transparency, accountability, data privacy, and equity. This paper critically examines the ethical challenges associated with AI-driven decision-making in education. It explores algorithmic bias, surveillance risks, data governance issues, and the implications for marginalised learners. The study also proposes a framework for ethical AI implementation grounded in fairness, explainability, human oversight, and policy regulation. The findings emphasise that AI must complement—not replace—human judgment in educational contexts to ensure inclusive and equitable outcomes."
  },
  {
    "title": "From Clinic to Community: An Interpretable Artificial Intelligence Framework for Enamel Caries Detection to Support Public Health Dentistry",
    "url": "https://doi.org/10.1055/s-0046-1816061",
    "date": "2026-02-20",
    "content": "Abstract Dental enamel caries is among the most prevalent oral diseases worldwide. Early detection is essential, as incipient lesions can be managed with noninvasive therapies. Conventional methods, such as visual-tactile inspection and radiography, remain limited by examiner variability and reduced sensitivity for early lesions. This study aimed to develop an efficient and interpretable deep learning framework for automated classification of enamel caries at multiple severity levels, while ensuring clinical applicability and transparency. A dataset of 2,000 clinical dental images categorized as advanced enamel caries, early-stage enamel caries, and no enamel caries was curated and expanded to 12,000 images using preprocessing and augmentation. Two transfer learning models, Modified EfficientNetB0 and Modified MobileNetV2, were trained individually, then combined using an attention-guided fusion mechanism. Gradient-weighted Class Activation Mapping (Grad-CAM) was applied to provide visual interpretability. Performance was evaluated using accuracy, precision, sensitivity, specificity, F1 score, and ROC AUC. Comparative analysis was performed across models and classifiers, with inference time assessed for clinical feasibility. The Modified EfficientNetB0 and MobileNetV2 models achieved accuracies of 96.33 and 96.25%, respectively. The fused model with Random Forest demonstrated superior performance, achieving 96.92% accuracy, F1 score of 96.92, and an ROC AUC of 99.34. Misclassifications were limited to adjacent disease stages, with no severe diagnostic errors. The proposed framework provides accurate, interpretable, and efficient enamel caries detection. Its low inference time supports real-time clinical use, enhancing diagnostic confidence and enabling early, minimally invasive interventions. Future research should focus on multicenter validation and multimodal datasets to improve generalizability."
  },
  {
    "title": "The Abandoned Road Dilemma: Between Imposed Ethics and Limited Autonomy",
    "url": "https://doi.org/10.5281/zenodo.18714707",
    "date": "2026-02-20",
    "content": "AbstractSince the artificial intelligence revolution spread and penetrated all fields of life, the questions raised about ethics and the challenges resulting from this technological revolution have evolved alongside it. The ethical challenges represented in the conflict between the limitations of artificial intelligence and the contradictions of human consciousness and its ethics have led to the growth of the major problem related to the possibility of intersection between machine autonomy and human will.In our paper, we present the Abandoned Road Dilemma, which represents an ethical challenge for artificial intelligence that is usually programmed for specific actions that may form a contradiction with the owner's desires. Here lies the core of the challenge: should the vehicle comply with the owner's wishes, or stop to rescue a person whose vehicle has broken down in a harsh desert environment?"
  },
  {
    "title": "The functional obsolescence of image collections in the era of AI-generated content",
    "url": "https://doi.org/10.1177/09610006261421642",
    "date": "2026-02-20",
    "content": "This article examines the functional obsolescence of still and moving image repositories from a Library and Information Science and Audiovisual Documentation perspective, in a context shaped by the emergence of generative artificial intelligence. Through a theoretical and bibliographic review, it explores the historical transformations of repositories from analog photography and audiovisual recording to the networked digital ecosystem, highlighting the factors that explain their centrality as providers of access and circulation for documentary, journalistic, and cultural visual and audiovisual content. The study demonstrates how AI’s capacity to produce realistic synthetic images and audiovisual representations profoundly disrupts the visual documentation value chain, jeopardizing traditional business models and established access logics, and shifting interest toward authenticity certification, the preservation of visual and audiovisual memory, and the management of irreproducible heritage. The article concludes by outlining conceptual frameworks and future research avenues addressing the legal, ethical, and epistemological challenges of this scenario for audiovisual repositories."
  },
  {
    "title": "Empirical Verification of a Stateless Architecture for Multi-Dimensional Physics, AI, and Quantum-Resistant Digital Sovereignty: A Unified Approach via J.M Function and Spatiotemporal Bio-Convergence",
    "url": "https://doi.org/10.5281/zenodo.18717634",
    "date": "2026-02-20",
    "content": "This study proposes a 'Stateless Architecture' to solve the challenges of hyper-scale Artificial Intelligence (AI) operations, high-dimensional dynamics control, and data sovereignty in the quantum computing era. Through the patent-pending J.M Function and spatiotemporal and biometric-based stateless key generation technology, data is vaporized into mathematical coordinates rather than stored as physical bits. This breakthrough allows models requiring 819.2 GB VRAM to operate with less than 25MB of memory, effectively reducing data storage space to 0 Bytes. By demonstrating the complete vaporization and regeneration of data via context-aware key recreation, we empirically prove the feasibility of an eco-neutral digital sovereignty platform consuming only 0.0030W of energy."
  },
  {
    "title": "Kansei Engineering in the Evolving Service Sector: A Decade of Insights",
    "url": "https://doi.org/10.12688/f1000research.174681.1",
    "date": "2026-02-20",
    "content": "<ns7:p>Background Kansei Engineering (KE) has increasingly been applied beyond product design into service contexts, responding to the growing importance of emotional satisfaction, experiential quality, and human-centered service design. Despite its expanding use, a comprehensive understanding of how KE has evolved methodologically, theoretically, and contextually within service research remains limited. This study aims to critically review KE applications in services over the last decade to identify key trends, contributions, and research gaps. Methods A semi-systematic literature review was conducted using a two-phase Define–Refine protocol. A structured search was performed in the Scopus database covering publications from 2010 to 2023. The review followed PRISMA-guided screening and refinement procedures, resulting in the selection of 28 peer-reviewed journal articles. The selected studies were analyzed in terms of service context, methodological approaches, analytical tools, and theoretical integration. Results The findings reveal three main contributions. First, KE applications in services have shifted from traditional attribute–response models toward more data-driven and analytical Kansei approaches, including text mining, machine learning, and advanced statistical techniques. Second, KE has increasingly evolved as a complementary approach to service quality theories and service-dominant logic (SDL), strengthening its role in explaining emotional satisfaction and customer experience. Third, a clear differentiation in methodological robustness across service sectors is observed, with logistics, hospitality, transportation, and digital services showing varied levels of analytical maturity. Overall, the results demonstrate KE’s effectiveness in enhancing service quality, shaping emotional service experiences, and supporting customer satisfaction and loyalty, while also identifying underexplored areas, particularly related to artificial intelligence and emerging technologies. Conclusions This study provides practical guidelines for integrating KE into service design and development to enhance emotional satisfaction and customer loyalty. By emphasizing customer Kansei, the review highlights KE’s potential to become more culturally sensitive and human-centered in service research. As an original contribution, this paper maps a decade-long trajectory of KE applications in services, positioning Kansei as central to service quality, innovation, and future research directions. The study is limited by its relatively narrow scope and reliance on unvalidated secondary data from a single database.</ns7:p>"
  },
  {
    "title": "AGI Architecture Based on Self-Expanding Temporal Hypergraphs for Critical Structure Encoding",
    "url": "https://doi.org/10.5281/zenodo.18712751",
    "date": "2026-02-20",
    "content": "Building upon the structuralist framework established in the previous work, Reasoning as Structure-Preserving Transformation, this paper further explores the possibility of a semantics-independent reasoning architecture: Self-Expanding Temporal Hypergraphs for Critical Structure Encoding. We postulate that reasoning can be represented as a dynamical system akin to cellular automata or the evolution of identical particles, fundamentally characterized by information compression and structure-preserving transformations. The primary discussions of this paper include:(1) Axiomatic Representation: Referencing the ideas of category theory, reformulate formalized mathematics, and code logic into hypergraphs, where objects are recursively defined subgraphs;(2) Structural Invariants: Concretizing the concept of \"invariants\"—substructures that remain stable under permissible perturbations (e.g., reordering, local replacement) in the reasoning space—and discussing the feasibility of treating them as fundamental units of reasoning;(3) Reinforcement Learning: Investigating potential ways for applying reinforcement learning (such as AlphaZero-like algorithms) within continuously growing representation spaces, especially strategies for search and structure evaluation amidst the expansion of both embedding spaces and neural networks;(4) Unification of Philosophy and Application: Reflecting on the potential relationship between intelligence and cosmic evolution, and envisioning the application prospects of this architecture in Intermediate Representation (IR) layer code reconstruction, automated theorem discovery, and enhancing LLM reasoning. This study aims to provide preliminary philosophical support for connecting mathematics, physics, and artificial intelligence."
  },
  {
    "title": "Human-Centered AI for Real-Time Self-Checkout Assistance: An Event-Driven Architecture with Human-in-the-Loop Decision Support for Enhanced Customer Experience and Shrink Reduction",
    "url": "https://doi.org/10.47941/jts.3523",
    "date": "2026-02-20",
    "content": "Purpose: Self-checkout (SCO) systems face persistent operational challenges including delayed assistance, inconsistent triage, and false-positive rates of 18 to 25 percent, contributing to industry shrink losses exceeding 112 billion dollars annually. Stores with high SCO utilization experience shrinkage 75 to 147 percent above industry averages. This paper proposes a human-centered artificial intelligence (AI) architecture grounded in explainable AI (XAI) and human-in-the-loop (HITL) principles, treating AI as decision support rather than autonomous enforcement. Methodology: The event-driven system prioritizes assistance events using multi-factor scoring based on wait time, anomaly likelihood, and associate workload, while preserving human discretion and providing transparent explanations for all recommendations. The framework is analyzed through a theoretical lens using parameters calibrated to publicly available retail industry benchmarks. Findings: Theoretical analysis suggests the framework has potential for significant reduction in assistance wait time, decreased false alert rates, and improved detection accuracy while maintaining associate decision authority. Unique contribution to theory, practice and policy: This analysis demonstrates how responsible AI design can enhance operational efficiency without compromising human autonomy or customer experience, offering a replicable blueprint for human-centered AI deployment in consumer-facing retail environments."
  },
  {
    "title": "BUILDING SMART START-UPS IN THE AI ERA",
    "url": "https://doi.org/10.5281/zenodo.18667774",
    "date": "2026-02-20",
    "content": "Building Smart Start-ups in the AI Era examines the transformative role of artificial intelligence in shaping innovation, entrepreneurial strategy and sustainable business growth. This edited volume brings together scholarly contributions that explore how AI is redefining start-up creation, decision-making and responsible value generation in a global context. The book addresses key issues such as AI-driven innovation, ethical governance, digital transformation and sustainability, offering both conceptual clarity and practical relevance. It serves as a valuable resource for researchers, educators, advanced students, entrepreneurs and policymakers engaged with the future of AI-enabled enterprises. ## Key Features of the Volume* Interdisciplinary perspectives on AI and entrepreneurship* Focus on ethics, sustainability and responsible innovation* Blend of conceptual frameworks and practical insights* Relevant for academia, start-up ecosystems and policy discourse. Then in Last Information Related to:1. Publisher with Logo2. Bar Code / Price / ISBN Number 3. Availability on various platforms."
  },
  {
    "title": "Decoding Urban Riverscape Perception: An Interpretable Machine Learning Approach Integrating Computer Vision and High-Fidelity 3D Models",
    "url": "https://doi.org/10.3390/ijgi15020091",
    "date": "2026-02-20",
    "content": "Visual perception serves as a crucial interface connecting human psychology with the built environment. However, current studies on urban riverscapes often rely on static 2D imagery, failing to capture the spatial depth and immersive experience essential for ecological validity. Furthermore, the “black box” nature of traditional machine learning models hinders the understanding of how specific environmental features drive public perception. To address these gaps, this study proposes an innovative framework integrating high-fidelity 3D models, computer vision (CV), and interpretable artificial intelligence (XAI). Using the River Thames (London) and the River Seine (Paris) as diverse case studies, we constructed high-precision 3D digital twins to quantify 3D spatial metrics (e.g., Viewshed Area, H/W Ratio) and applied the SegFormer model to extract 2D visual elements (e.g., Green View Index) from water-based panoramic imagery. Subjective perception data were collected via immersive Virtual Reality (VR) experiments. Random Forest models combined with SHAP were employed to decode the non-linear driving mechanisms of perception. The results reveal three universal principles: (1) Sense of Affluence and Vibrancy are primarily driven by high building density and vertical enclosure, challenging the traditional preference for openness in waterfronts; (2) Scenic Beauty is determined by a synergy of high Green View Index and quality artificial interfaces, suggesting a preference for nature-culture integration; (3) Sense of Boredom is significantly positively correlated with Viewshed Area, indicating that empty prospects without visual foci lead to monotony. This study demonstrates the efficacy of integrating Digital Twins and XAI in revealing robust perception mechanisms across different urban contexts, providing a scientific, evidence-based tool for precision urban planning and riverside regeneration."
  },
  {
    "title": "A Wearable Brain–Computer Interface for Mitigating Car Sickness via Attention Shifting",
    "url": "https://doi.org/10.1002/advs.202513040",
    "date": "2026-02-20",
    "content": "ABSTRACT Car sickness, an enormous vehicular travel challenge, affects a significant proportion of the population. Pharmacological interventions are limited by adverse side effects, and effective nonpharmacological alternatives remain to be identified. Here, we introduce a novel attention‐shifting method based on a closed‐loop, artificial intelligence (AI)‐driven, wearable mindfulness brain–computer interface (BCI) to alleviate car sickness. As the user performs an attentional task, i.e., focusing on breathing as in mindfulness, with a wearable headband, the BCI collects and analyzes electroencephalography (EEG) data via a convolutional neural network to assess the user's mindfulness state and provide real‐time audiovisual feedback. This approach might sustainedly shift the user's attention from physiological discomfort toward the BCI‐based mindfulness practices, thereby mitigating car sickness symptoms. The efficacy of the proposed method was rigorously evaluated in two real‐world experiments, namely, short and long car rides, with a large cohort of more than 100 participants susceptible to car sickness. Remarkably, over 83% of the participants rated the BCI‐based attention shifting as effective, with significant reductions in car sickness severity, particularly in individuals with severe symptoms. Furthermore, EEG data analysis revealed a neurobiological signature of car sickness, which provided mechanistic insights into the efficacy of the BCI‐based attention shifting for alleviating car sickness. This study proposes a wearable, nonpharmacological intervention for car sickness, validated in a relatively large‐scale study involving over 100 participants in real‐world car riding. These findings, derived from a between‐cohort comparison, support the potential of this approach to improve the travel experience for car sickness sufferers and represent a novel practical application of BCI technology."
  },
  {
    "title": "Project-based learning and artificial intelligence (PBL-AI) in higher education",
    "url": "https://doi.org/10.4324/9781003679233-14",
    "date": "2026-02-20",
    "content": "Generative AI reopens a familiar pedagogical challenge: integrating external tools without eroding authorship or the epistemic quality of learning. We propose and examine a PBL-AI model that treats AI as non-substitutive scaffolding, drawing on scaffolding theory, distributed cognition/extended mind, and cognitive load theory to delimit usage windows and fade support as expertise grows. The model combines explicit rubric-based criteria, a PBL sequence limiting AI to ideation, planning, and review, a traceability portfolio (prompt → AI output → verification → decision → changes), and a public defense without AI. Implemented in 2024–2025 at two Spanish universities – ULL (86 students) and US (198) – the study adopts a qualitative-interpretive approach, triangulating field notes, interviews, and documents (portfolios, rubrics, transcripts) and analyzing data through reflexive thematic analysis. Findings indicate clearer problem framing, more realistic planning, and stronger argumentative quality, alongside explicit authorship and auditable traceability. We discuss transferability and low organizational cost as enablers for institutional adoption."
  },
  {
    "title": "Augmentative or autonomous? Reframing artificial intelligence in talent management through a systematic review",
    "url": "https://doi.org/10.1108/omj-09-2025-2716",
    "date": "2026-02-20",
    "content": "Purpose This paper aims to examine the integration of artificial intelligence (AI) into talent management (TM), focusing on how different AI applications are reconfiguring talent processes and outcomes. Design/methodology/approach Following the Preferred Reporting Item for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines, a systematic literature review was conducted across four major databases (Scopus, Web of Science, EBSCO Business Source and ProQuest ABI/Inform), identifying 238 records published between 2000 and 2025. After screening and full-text review, 124 peer-reviewed articles were included in the final synthesis. Findings The review reveals two dominant modes of AI adoption in TM: augmentative systems that enhance human judgment and autonomous systems that replace human decision-making. Across recruitment, development, retention and performance management, AI is reshaping processes but remains unevenly theorized. Persistent gaps include limited attention to ethics, fairness and cross-cultural variation, as well as weak integration across micro- and macro-level perspectives. Research limitations/implications The review is limited to English-language, peer-reviewed publications. Future research should examine longitudinal and non-Western contexts and develop integrative theories that link individual-, organizational- and societal-level perspectives on AI in TM. Practical implications The augmentative–autonomous framework provides human resource (HR) and organizational leaders with a lens for evaluating AI adoption choices, balancing efficiency with transparency, fairness and trust. Social implications AI is changing how organizations recruit, develop and manage people, raising important questions of fairness, accountability and trust. This study shows that augmentative applications, which support human decision-making, tend to preserve transparency and employee agency, while autonomous applications, which replace human judgment, increase risks of bias, exclusion and reduced voice. By clarifying these differences, the framework helps policymakers, practitioners and researchers anticipate the societal consequences of AI adoption in TM and design strategies that promote inclusion, equity and responsible use of technology. Originality/value To the best of the authors’ knowledge, this study offers the first comprehensive, PRISMA-compliant systematic review of AI in TM. It introduces a framework that clarifies how augmentative and autonomous AI reshape talent systems, offering a foundation for advancing both scholarship and practice."
  },
  {
    "title": "Reality-Anchored Object Memory Architecture (RAOMA): A Spacetime-Grounded Substrate for Next-Generation Artificial Intelligence and Civilization-Scale Knowledge Systems",
    "url": "https://doi.org/10.5281/zenodo.18717149",
    "date": "2026-02-20",
    "content": "This work proposes Reality-Anchored Object Memory Architecture (RAOMA), a framework for structuring AI knowledge around persistent real-world objects anchored in space and time. Instead of treating knowledge as fragmented symbolic or statistical patterns, RAOMA organizes data as object-centered timelines with interaction histories and confidence-evaluated knowledge attribution. The architecture introduces spacetime indexing, causal event logging, and AI-mediated validation to improve reality correspondence, reduce redundant retraining, and support long-term knowledge preservation. A multi-layer governance model balances public knowledge sharing with sovereignty and privacy considerations. RAOMA is positioned as a potential civilization-scale knowledge infrastructure supporting AI reasoning, scientific discovery, and institutional memory"
  },
  {
    "title": "Semantic quantum secure direct communication",
    "url": "https://doi.org/10.1007/s43673-026-00181-1",
    "date": "2026-02-20",
    "content": "Quantum communication has reached a rate bottleneck, yet enhancing its throughput remains of profound significance. Here, we propose a scheme termed semantic quantum secure direct communication (QSDC), which is a novel paradigm integrating semantic communication with QSDC to realize highly efficient, task-oriented, and intrinsically secure information transmission. In semantic QSDC, meaningful semantic content is directly encoded into quantum states, whereby the fundamental laws of quantum physics guarantee inherent eavesdropping-detection capability and information-theoretic security. By harnessing semantic compression and artificial intelligence–assisted encoding strategies, this approach surpasses the intrinsic rate constraints of conventional QSDC. Simulation results show that semantic QSDC can surpass the Shannonian mutual information bound compared with conventional QSDC systems and demonstrate its potential to break through the linear rate-transmittance bound of quantum communication. These findings pave the way for high-rate secure communication."
  },
  {
    "title": "From synapse to system: mechanistic pathways of neural signaling dysfunction in psychiatric disorders",
    "url": "https://doi.org/10.3389/fcell.2026.1762930",
    "date": "2026-02-20",
    "content": "Psychiatric disorders are increasingly viewed as network-level brain diseases resulting from disruptions in neural signaling across various hierarchies, including molecular, synaptic, circuit, and systems levels. Evidence indicates that receptor dysregulation, abnormal intracellular pathways, and changes in ion channel activity lead to widespread network dysconnectivity, resulting in cognitive, emotional, and behavioral deficits. This review integrates advancements in genomics, transcriptomics, connectomics, and computational modeling to establish a framework for understanding signaling abnormalities in major psychiatric disorders. Further, this study investigates essential molecular and cellular processes such as synaptic plasticity, receptor-mediated communication, intracellular signaling cascades, and neuroimmune interactions, and connects these to disturbances in oscillatory dynamics, circuit architecture, and overall brain network organization. Additionally, neuroimaging and graph-theoretic studies consistently demonstrate an excitation–inhibition imbalance, atypical synaptic pruning, impaired oscillatory synchrony, and maladaptive connectivity within networks, including the default mode, salience, and fronto-limbic systems, across schizophrenia, depression, bipolar disorder, anxiety, and autism spectrum disorders. Moreover, genetic and epigenetic variations in signaling genes, such as CACNA1C, GRIN2B, and DISC1, along with developmental and environmental factors, contribute to network vulnerability and clinical heterogeneity. Emerging artificial intelligence and multimodal integration methods facilitate the identification of individualized “signaling fingerprints,” which connect molecular perturbations to systems-level dysfunction. This research enhances precision psychiatry and guides targeted interventions based on neuromodulation, molecular mechanisms, and biomarkers."
  },
  {
    "title": "Agentic AI, Retrieval-Augmented Generation, and the Institutional Turn: Legal Architectures and Financial Governance in the Age of Distributional AGI",
    "url": "https://doi.org/10.5281/zenodo.18711509",
    "date": "2026-02-20",
    "content": "The proliferation of agentic artificial intelligence systems—characterized by autonomous goal-seeking, tool use, and multi-agent coordination—presents unprecedented challenges to existing legal and financial regulatory frameworks. While traditional AI governance has focused on model-level alignment through training-time interventions such as Reinforcement Learning from Human Feedback (RLHF), the deployment of large language models (LLMs) as persistent agents embedded within socio-technical systems necessitates a paradigm shift toward institutional governance structures. This paper examines the intersection of agentic AI, Retrieval-Augmented Generation (RAG), and their implications for legal accountability and financial market integrity. Through a comprehensive analysis of the Institutional AI framework proposed by Pierucci et al. [1], we argue that alignment must be reconceptualized as a mechanism design problem involving runtime governance graphs, sanction functions, and observable behavioral constraints rather than internalized constitutional values. We address the critical deficit identified by LeCun regarding the absence of world models in current agents, demonstrating how RAG architectures function as externalized epistemic infrastructure that grounds agentic cognition in verifiable data repositories. The paper subsequently interrogates the legal implications of these systems under the European Union's Artificial Intelligence Act (EU AI Act) and the regulatory thresholds established by the Financial Conduct Authority (FCA) and European Central Bank (ECB), proposing justified compliance boundaries for high-risk financial applications. Furthermore, we acknowledge significant governance gaps within Decentralized Finance (DeFi) protocols where institutional oversight mechanisms face structural limitations. By synthesizing technical insights from multi-agent systems, constitutional AI limitations, and offensive security frameworks, this work advances a jurisprudential foundation for agentic AI that prioritizes defensible audit trails, incentive-compatible compliance, and systemic stability over opaque internal alignment guarantees. The analysis concludes that the future of AI governance lies not in perfecting isolated model behavior, but in architecting institutional environments where compliant behavior emerges as the dominant strategy through carefully calibrated payoff landscapes."
  },
  {
    "title": "Neuroscientific Approach to Artificial Intelligence for Psychopathologies",
    "url": "https://doi.org/10.4018/979-8-3373-1325-2.ch006",
    "date": "2026-02-20",
    "content": "Artificial intelligence (AI) is rapidly progressing innovative technology to simulate the human brain. AI technology provides important classifications, predictions and therapeutic interventions in different psychopathologies. Neuroscience research offers a wealth of inspiration for novel kinds of algorithms and architectures for early diagnostics and development of the disorder. Recently, many studies and reviews have investigated the mutual relationship between AI and neuroscience. Complex deep neural networks inspired by neuroanatomical structures are utilized to create variable AI models including working memory, visual processing, speech and emotion recognition to manifest behavioral abnormalities in psychopathologies. This scoping review focuses on how neuroscience and AI are convergent to understand underlying mechanisms in mental health disorders and the development of new models for prevention."
  },
  {
    "title": "Q-Cowrie: An adaptive honeypot to analyse attackers’ behaviour",
    "url": "https://doi.org/10.1007/s10207-026-01221-5",
    "date": "2026-02-20",
    "content": "Abstract Honeypots in computer security have been used as effective security solutions to lure attackers, capture their interactions with the honeypot systems and study their behaviour. Attackers interacting with honeypots may use Artificial Intelligence (AI)-based techniques to detect the presence of honeypots leading to evasion by the attackers. This paper discusses the application of Reinforcement Learning (RL) to address these issues by improving response generation in honeypots. We propose “Q-Cowrie”, a honeypot that is built upon customising a medium interaction server honeypot, that is, Cowrie, to increase the honeypot’s deception. RL capabilities have been integrated into the honeypot to support adaptive behaviour while interacting with attackers. Two experimental studies have been conducted in which Cowrie and Q-Cowrie honeypots were used, respectively. First, we deployed a Cowrie honeypot to capture cyber attacks and identify attackers’ goals and techniques. This allowed us to create a probabilistic model, that is, the Markov Decision Making Process (MDP), to understand the decision-making process of attackers in different situations. Learning from attackers’ unique patterns and applying RL techniques, Q-Cowrie was able to actively interact with attackers, making adaptive decisions."
  },
  {
    "title": "Exploring Artificial Intelligence Utilization through TAM3 Model: The Influence on Behavioral Intention and Use Behavior",
    "url": "https://doi.org/10.17632/s73vjz6wss",
    "date": "2026-02-20",
    "content": "This dataset contains anonymized survey responses collected for an undergraduate thesis on Indonesian auditor readiness for AI-based audits. The data include item-level measures based on a Technology Acceptance Model (TAM3)-informed instrument, covering constructs such as technical competencies, ethics, and compliance, along with respondent demographics. Files provided include the anonymized dataset, a codebook/variable dictionary, and the survey questionnaire used for data collection. The dataset is intended to support transparency and reproducibility of the study’s findings and may be reused for related research on audit digitalization and AI adoption in assurance contexts."
  },
  {
    "title": "Muscle Fatigue in Dynamic Movement: Limitations and Challenges, Experimental Design, and New Research Horizons",
    "url": "https://doi.org/10.3390/bioengineering13020248",
    "date": "2026-02-20",
    "content": "Research on muscle fatigue during dynamic movement using surface electromyography (sEMG) constitutes a significant challenge within biomechanics. Despite a degree of standardization, measurements and their resultant findings continue to attract considerable debate, attributable to factors such as skin impedance, perspiration, and electrode displacement, as well as subjective fatigue perception. Further questions remain regarding signal normalization and the selection of appropriate analytical methodologies. Recent years have witnessed notable progress in dynamic fatigue research, highlighting the limitations of classical metrics (e.g., EMG Median Frequency) and introducing time–frequency methods, such as the wavelet transform (WT), which are better equipped to handle signal non-stationarity. Interest has also expanded to include non-linear metrics (e.g., entropy) and the analysis of multiple signals (EMG, accelerometers, fNIRS, EEG). The inherent complexity of conducting studies under conditions that approximate real-world sporting disciplines requires the consideration of the influence of various confounding factors. The judicious selection of relevant physical activities and the rigorous validation of the measurement apparatus are paramount for the accurate execution of the calculations. Current research is substantially predicated on artificial intelligence (AI) algorithms. The synergistic application of AI with wavelet transform, particularly in the decomposition and extraction of EMG signals, demonstrates efficacy in fatigue detection. Nevertheless, the full realization of these potential mandates requires further investigation into system generalization, the integration of data from multiple sensors, and the standardization of protocols, coupled with the establishment of publicly accessible datasets. This article delineates selected guidelines and challenges pertinent to the planning and execution of research on muscle fatigue in dynamic movement, focusing on activity selection, equipment validation, EMG signal analysis, and AI utilization."
  },
  {
    "title": "A Real-time 3D Desktop Image Processing",
    "url": "https://doi.org/10.1142/s2972370126500017",
    "date": "2026-02-20",
    "content": "A new extended version of the altiro3D C++ Library -initially developed to get glass-free holographic displays starting from 2D images- is here introduced aiming to deal with 3D video streams from either 2D webcam images or flat video files. These streams are processed in real-time to synthesize light-fields (in Native format) and feed realistic 3D experiences. The core function needed to recreate multiviews consists on the use of MiDaS Convolutional Neural Network (CNN), which allows to extract a depth map from a single 2D image. Artificial Intelligence (AI) computing techniques are applied to improve the overall performance of the extended altiro3D Library. Thus, altiro3D can now treat standard images, video streams or screen portions of a Desktop where other apps may be also running (like web browsers, video chats, etc) and render them into 3D. To achieve the latter, a screen region need to be selected in order to feed the output directly into a light-field 3D device such as Looking Glass (LG) Portrait. In order to simplify the acquisition of a Desktop screen area by the user, a multi-platform Graphical User Interface has been also implemented. Sources available at: https://github.com/canessae/altiro3D/releases/tag/2.0.0 ."
  },
  {
    "title": "Artificial intelligence algorithm to predict the requirement of neonatal endotracheal intubation within 3 h: application for clinical practice",
    "url": "https://doi.org/10.3389/fmed.2026.1729990",
    "date": "2026-02-20",
    "content": "Introduction Timely intervention, such as endotracheal intubation (EI), is crucial for managing acute respiratory distress in the neonatal intensive care unit (NICU). Delays in EI can lead to significant adverse effects in neonates. This study aimed to develop a highly accurate predictive model to forecast the requirement for EI, allowing for proactive clinical planning and intervention up to 3 h in advance. Method We developed a multimodal deep learning model designed to simultaneously analyze distinct data types. The model utilizes numeric initial clinical data and time-series vital sign data collected over the preceding 1–3 h. To rigorously evaluate the model's reliability and clinical applicability, we performed comprehensive external validation using independent patient datasets, specifically assessing generalization and bias. Result The constrained model successfully predicted the requirement for EI with high predictive power across various forecasting intervals (up to 72 h in 1-h increments). Internal validation yielded an accuracy of 0.9579 and AUC of 0.9323, while external validation maintained high generalization (accuracy 0.9411, AUC 0.9336). Discussion The proposed multimodal deep learning model provides an effective tool for the advance prediction of EI requirements in neonates. Given its high accuracy, confirmed generalization capabilities through external validation, and potential to prevent severe respiratory distress problems by facilitating proactive care, this model holds wide and significant applicability in clinical NICU environments."
  },
  {
    "title": "Digital Medicine and Artificial Intelligence in Chronic Myeloid Leukemia: Current Applications, Challenges, and Future Directions",
    "url": "https://doi.org/10.7759/cureus.103951",
    "date": "2026-02-20",
    "content": "Chronic myeloid leukemia (CML) has become a paradigm for targeted therapy with BCR-ABL1 tyrosine kinase inhibitors (TKIs). However, the growing volume and complexity of clinical, molecular, and imaging data challenge traditional decision-making based on static risk scores. Digital health technologies and artificial intelligence (AI) offer new opportunities to enhance diagnosis, risk stratification, and treatment personalization in CML. This narrative review is based on a focused literature search in PubMed/MEDLINE and Web of Science (2010-2025), combined with expert selection of pivotal studies in CML, digital health, and AI. We included peer-reviewed original research and reviews describing applications of digitalization, AI, or machine learning (ML) in CML or closely related hematologic malignancies, as well as key publications on ethics, regulation, patient perspectives, and ML operations (MLDevOps). We summarize the integration of electronic health records, telemedicine, networked registries, and real-world evidence as a foundation for AI in CML. We review AI/ML applications in diagnostic hematology (cytomorphology, flow cytometry, cytogenetics, histopathology), prognostic modeling, molecular response monitoring (including automated BCR-ABL1 trend analysis and ghost cytometry), drug discovery, and clinical decision support systems (CDSS). Multimodal ML frameworks that integrate clinical, imaging, histopathological, and genomic data enable more precise disease classification and outcome prediction. At the same time, we discuss challenges related to data quality, algorithmic bias, model transparency, regulatory oversight, and patient trust, emphasizing the need for robust validation and MLDevOps infrastructure. AI has the potential to substantially improve CML diagnosis, prognostication, and treatment selection and to support innovative approaches such as treatment-free remission and rational drug design. However, technical sophistication alone is insufficient. Safe and effective clinical integration of AI in CML will require rigorous multicenter validation, continuous performance monitoring, explainable models aligned with ELN guidelines, appropriate regulatory frameworks, and patient-centered implementation strategies. Under these conditions, AI can become a key enabler of truly personalized, evidence-based, and patient-centered care in CML."
  },
  {
    "title": "AI Adoption Readiness and Use Intention Survey Data from Higher Education Non-Academic Staff: A PLS-SEM Study (N=213)",
    "url": "https://doi.org/10.5281/zenodo.18711441",
    "date": "2026-02-20",
    "content": "This dataset contains survey responses from 213 non-academic staff members at a Hungarian University collected to examine factors influencing artificial intelligence adoption intentions in higher education administration. The dataset includes demographic variables (gender, age, educational attainment, work experience, functional area) and Likert-scale responses (1-5) measuring six validated constructs: digital readiness (DIRE, 4 items), AI readiness (AIRE, 4 items), perceived ease of use (AIEU, 4 items), perceived usefulness (AIUF, 4 items), facilitating conditions (AIFC, 4 items), and behavioral intention to use AI (AIUI, 3 items). The data support partial least squares structural equation modeling (PLS-SEM) analysis examining direct and mediation effects in technology acceptance frameworks applied to AI contexts."
  },
  {
    "title": "Let's Talk Tech: The Role of Technology in Neurology Education",
    "url": "https://doi.org/10.1055/a-2795-8556",
    "date": "2026-02-20",
    "content": "Abstract As a field, neurology can seem complicated, overwhelming, and at times ambiguous and uncertain. However, novel technological developments—including artificial intelligence—can be used to decrease “neurophobia,” foster enthusiasm about our specialty, and increase curiosity and motivation while decreasing the educator's time in preparation. This review discusses the technology-enhanced teaching landscape and how neurology integrates into known conceptual frameworks and learning theories in medical education. We provide detailed guidance for technology-focused curriculum design in all realms of neurologic teaching: at the bedside, in small groups, and in larger presentations. Finally, we identify ways technological scholarship can be leveraged toward academic promotion for neurology education."
  },
  {
    "title": "Learning Under Generative Abundance: A Structural Law of Epistemic Stabilization",
    "url": "https://doi.org/10.5281/zenodo.18711407",
    "date": "2026-02-20",
    "content": "Artificial intelligence creates an epistemic environment in which coherent expressions can be producedexternally at near-zero marginal cost. This paper argues that the consequences for education stem notprimarily from pedagogical choice, but from a structural law governing inference under conditions ofgenerative abundance.A single underlying dynamic gives rise to three observable effects: degradation of inferential signals, escalation of conceptual volatility, and the emergence of selective stabilization. Beyond a critical thresholdof generative exposure, learning undergoes a regime shift — from accumulation to discrimination.Learning is therefore redefined as the capacity to preserve structural stability when expressive production no longer uniquely indicates internal organization.Keywords: Generative AI, epistemic stabilization, open science, civil science, regime transition, discrimination learnin"
  },
  {
    "title": "XAI-driven Data Mining for Self-defending IoT Systems: Enhancing Cybersecurity Transparency in the Age of Smart Cities",
    "url": "https://doi.org/10.1007/s12559-026-10559-w",
    "date": "2026-02-20",
    "content": "The rapid expansion of Internet of Things (IoT) technologies in smart cities, healthcare, and industrial automation has intensified the need for cybersecurity frameworks capable of operating at scale and in real time under increasingly sophisticated threat conditions. Traditional security mechanisms and opaque AI-based models are no longer adequate for protecting interconnected urban infrastructures, especially as regulatory and societal expectations move toward transparency and accountability. Although prior surveys have examined IoT security and general AI techniques, they rarely address the emerging role of Explainable Artificial Intelligence (XAI) in data mining for IoT cybersecurity or integrate recent advances in cognitively inspired and human-aligned explainability methods. This survey provides an up-to-date review of XAI-driven data mining approaches applied to IoT ecosystems, highlighting their ability to detect anomalies, interpret complex sensor-driven behaviours, and support automated security decisions through transparent and interpretable reasoning. The review identifies critical challenges, including data privacy, scalability, computational constraints, and the interpretability limitations of modern AI models. It examines how biologically inspired learning paradigms and cognitively grounded explanation techniques can enhance trust and situational awareness in IoT environments. Emerging technologies such as edge intelligence, federated learning (FL), blockchain integration, and quantum-assisted analytics are discussed as promising enablers of scalable and transparent IoT security. The survey underscores the importance of trustworthy, ethically aligned AI, advocating for XAI frameworks that enable fair, auditable, and reliable decision-making in safety-critical infrastructure. By addressing gaps in the literature and synthesizing recent developments, this study presents a timely perspective on XAI, data mining, and IoT cybersecurity, outlining future directions for building resilient, interpretable, and human-centric smart city systems."
  },
  {
    "title": "An Explainable AI Framework for Vibroarthrographic Knee Joint Disorder Detection Using Entropy Based Feature Engineering",
    "url": "https://doi.org/10.21203/rs.3.rs-8694157/v1",
    "date": "2026-02-20",
    "content": "<title>Abstract</title> In this work, a computer-aided diagnosis (CAD) system is proposed to classify knee joint conditions into healthy and non-healthy categories using vibroarthrographic (VAG) signals. To effectively analyze the non-stationary nature of VAG signals, the Tunable Q-factor Wavelet Transform (TQWT) is employed to decompose the signals into multiple sub-band components. From these sub-bands, eight entropy-based features are extracted, yielding a total of 400 features. Feature selection techniques are then applied to identify the most relevant descriptors, yielding 45 optimal features. Six machine learning classifiers, along with an ensemble learning strategy, are utilized for classification. The ensemble-based framework achieves 90.1% classification accuracy and an AUC of 0.91 under leave-one-subject-out validation. Furthermore, an explainable artificial intelligence (XAI) approach based on Shapley Additive exPlanations (SHAP) is incorporated to interpret the model predictions by quantifying the contribution of individual entropy features. This explainability enhances the transparency and reliability of the proposed system, supporting its potential applicability in clinical decision-making."
  },
  {
    "title": "Practices and Challenges in Designing Authentic Assessments Using Artificial Intelligence",
    "url": "https://doi.org/10.4324/9781003665472-8",
    "date": "2026-02-20",
    "content": "This chapter explores the role of artificial intelligence (AI) in designing authentic assessments within technology-enhanced learning (TEL) environments. Based on a study of 82 undergraduates in a blended learning course examines how AI-powered tools can support the creation of assessments that emphasise real-world application, critical thinking, creativity, and problem-solving. The study found that AI tools can enhance the efficiency of assessment design by generating diverse, context-rich tasks aligned with learning outcomes. AI can produce scenario-based and problem-based tasks that reflect professional contexts within authentic assessments. However, challenges include ensuring the relevance and clarity of AI-generated content and inconsistencies that may affect validity. The chapter suggests faculty training, robust evaluation frameworks, and human oversight in the assessment design process to mitigate these issues. It also highlights the importance of interdisciplinary collaboration among educators, instructional designers, and AI experts. The chapter provides practical recommendations for institutions and educators seeking to harness AI’s potential in assessment while safeguarding academic standards. With appropriate human oversight, educators can harness AI to design authentic assessments that foster deep learning while maintaining academic rigour."
  },
  {
    "title": "An Exploratory Review on the Role and Relevance of Artificial Intelligence (AI) Tools for Informed Decision-Making in Key Areas of Business",
    "url": "https://doi.org/10.5281/zenodo.18712931",
    "date": "2026-02-20",
    "content": "Innovative business decision-making solutions by using technological tools go a long way in improving business competence, restructure processes and drive radical development. Despite the presence of an array of such tools in vogue, there was a need of the hour to choose the best among them for efficient and effective business decisions. With this prime intent in mind, this study was considered timely. This review is exploratory in nature, and the analysis is based on essential contents extracted from eighteen relevant and recent critical sources written by competent authorities in the specific field. The objectives of this study were: to explain the meaning of Artificial Intelligence (AI); to explore their role and relevance in informed decision-making related to the key areas of business; and to recommend productive suggestions for the effective utilization of these all AI tools in the key areas of business. The findings reveals that, of all existing business decision-making tools developed over the years, the emergence of fifteen outsmarting Artificial Intelligence (AI) tools of recent origin is spectacularly awesome, and outsmarting in key business processes related to core fields like marketing (Miro, Filestage, Buffer), sales (Salesforce Einstein, Outreach, and Regie.ai), HR (SeekOut, Peoplebox.ai, and Humanforce), customer service (HubSpot Service Hub, Help Scout, and Gorgios), and operation management (Monday, Odoo, and Scoro). For their effective utilization in business, the study suggests creation of awareness about these tools for the businessmen to appreciate them and accept them, and developing their passion to learn and handle them is of paramount importance, for their sustained productivity."
  },
  {
    "title": "Reframing Human Resource Governance through Artificial Intelligence: A Legitimacy-Based Model for Public Healthcare Workforce Transformation in Poland",
    "url": "https://doi.org/10.5281/zenodo.18714336",
    "date": "2026-02-20",
    "content": "Artificial Intelligence (AI) is increasingly transforming healthcare systems, yet its integration into public-sector human resource management (HRM) remains limited. Polish public healthcare institutions face persistent workforce shortages, administrative inefficiencies, and rising demographic pressure. This study develops a governance-centered framework to evaluate how AI adoption can enhance workforce intelligence, recruitment efficiency, strategic workforce planning, and organisational resilience while maintaining compliance with GDPR and public accountability standards. Unlike prior research focused on private-sector HR, this paper conceptualises AI-enabled HR transformation as a shift from administrative to anticipatory workforce governance. Grounded in Institutional Theory, the Resource-Based View (RBV), and Strategic Human Capital Theory, the study proposes the Prestini Artificial Intelligence for Healthcare Human Resources (P-AIHR) Framework. The model integrates five pillars: workforce intelligence analytics, AI-assisted recruitment, ethical governance and compliance, adaptive workforce development, and executive decision support. A phased 36-month implementation roadmap and a structured risk–mitigation matrix are presented. The findings suggest that AI in public healthcare HR should operate as a hybrid system combining predictive analytics with human oversight to preserve legitimacy and trust. The study contributes a structured conceptual model tailored to Central and Eastern European public healthcare systems, offering theoretical, managerial, and policy insights into responsible AI adoption. This study extends algorithmic governance theory into public healthcare HR systems within regulated European contexts by operationalising legitimacy-preserving AI architecture."
  },
  {
    "title": "Skills Acquisition in Artificial Intelligence in Correlation to Office Technology and Management (OTM) Educators’ Instructional Effectiveness in Public Polytechnics in North-East, Nigeria",
    "url": "https://doi.org/10.64388/irev9i8-1714443",
    "date": "2026-02-20",
    "content": "The study examined skills acquisition in Artificial Intelligence in correlation with Office Technology and Management (OTM) educators’ instructional effectiveness in public polytechnics in North-East, Nigeria. It was guided by two research questions and two hypotheses. Correlational research design was adopted in the study. The population of study comprised 64 OTM educators in ten public polytechnics in North-East, Nigeria. The entire population was used due to its manageable size, hence, there was no sample. Two research instruments entitled: Artificial Intelligence Skills Acquisition Test (AISAT) and Instructional Effectiveness Rating Scale (IERS) were used for data collection. Both instruments were validated by experts and tested for reliability using K-R 20 for AISAT and Cronbach Alpha Coefficient for IERS. The result obtained for AISAT was 0.79, while IERS had reliability coefficient of 0.86. These results indicate that the instruments are suitable for use in the study. Data collected were analysed using Pearson Product Moment Correlation Coefficient (r) to answer the research questions, while the null hypotheses were tested using regression. Findings revealed that there is a high positive relationship between the variables. Furthermore, the relationship between AI-powered writing assistance skills and OTM educators’ instructional effectiveness is statistically significant. Also, the relationship between online learning management skills and OTM educators’ instructional effectiveness in public polytechnics in North-East, Nigeria is statistically significant. The study recommended that there should be training sessions and workshops to enhance OTM educators' proficiency in AI to enable them function effectively in the use of AI for instructional purposes."
  },
  {
    "title": "Clinical applications of whole-genome and whole-exome sequencing in medical practice: Current status, challenges, and future directions – A narrative review",
    "url": "https://doi.org/10.4103/etmj.etmj-d-25-00014",
    "date": "2026-02-20",
    "content": "Abstract Whole-genome sequencing (WGS) and whole-exome sequencing (WES) are transformative next-generation sequencing (NGS) technologies that have rapidly revolutionized clinical diagnostics, particularly in cases of rare genetic disorders and oncology. WGS provides comprehensive data by covering the entire genome, including coding and noncoding regions, allowing single-nucleotide variants, small insertions/deletions, structural variants, and copy number variations to be detected. Conversely, WES targets the protein-coding exome, which represents only 1%–2% of the genome yet contains approximately 85% of the pathogenic mutations. This review discusses the technological underpinnings, clinical applications, challenges, and future prospects of WGS and WES. We explored their roles in rare disease diagnosis, personalized cancer therapy guidance, neonatal screening, pharmacogenomics, and reproductive carrier screening. Despite significant advancements, challenges remain in data interpretation, computational requirements, cost-effectiveness, ethical issues, and clinical integration. Advances in long-read sequencing, artificial intelligence–assisted interpretation, multi-omics integration, and supportive policy initiatives that aim to broaden access to precision medicine are warranted."
  }
]
